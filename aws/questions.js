Array.prototype.shuffle = function() {
	let i = this.length
	if (i == 0) return this
	while (--i) {
	   const j = ~~(Math.random() * ( i + 1 ));
	   [this[i], this[j]] = [this[j], this[i]]
	}
	return this
}

function randomInteger(min, max) {
	return Math.floor(Math.random() * (max - min + 1)) + min
}

let questionSet = randomInteger(0, 3)

let questions = [
	{
		"question": "<p>A startup develops Internet-Of-Things (IoT) devices that provide health monitoring for dogs and cats which is integrated into their collars. The startup has an engineering team to build a smart pet collar that collects biometric information of the pet every second and then sends it to a web portal through a POST API request. The Solutions Architect has been tasked to set up the API services and the web portal that will accept and process the biometric data as well as provide complete trends and health reports to pet owners around the globe. The portal should be highly durable, available, and scalable with an additional feature for showing real-time biometric data analytics and monitoring.</p><p>Which of the following is the best architecture that the Solutions Architect should implement to meet the above requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Launch an Amazon Elastic MapReduce instance to collect the incoming biometrics data.</p><p>2. Use Amazon Kinesis to analyze the data.</p><p>3. Save the results to an Amazon DynamoDB table.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Use Amazon Kinesis Data Streams to collect the incoming biometric data.</p><p>2. Analyze the data using Amazon Kinesis and show the results in a real-time dashboard.</p><p>3. Set up a simple data aggregation process and pass the results to Amazon S3.</p><p>4. Store the data to Amazon Redshift, configured with automated backups, to handle complex analytics.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Create an Amazon SQS queue to collect the incoming biometric data.</p><p>2. Analyze the data from SQS with Amazon Kinesis.</p><p>3. Store the results to an Amazon RDS for MySQL database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Create an Amazon S3 bucket to collect the incoming biometric data from the smart pet collar.</p><p>2. Use Amazon Data Pipeline to run a data analysis task in the S3 bucket every day.</p><p>3. Use Amazon Redshift as the online analytic processing (OLAP) database for the web portal.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Kinesis Data Streams</strong> enable you to build custom applications that process or analyze streaming data for specialized needs. Kinesis Data Streams can continuously capture and store terabytes of data per hour from hundreds of thousands of sources such as website clickstreams, financial transactions, social media feeds, IT logs, and location-tracking events. With the Kinesis Client Library (KCL), you can build Kinesis Applications and use streaming data to power real-time dashboards, generate alerts, implement dynamic pricing and advertising, and more. You can also emit data from Kinesis Data Streams to other AWS services such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon EMR, and AWS Lambda.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_arch.JPG\"></p><p><strong>Kinesis Data Streams</strong> can be used to collect log and event data from sources such as servers, desktops, and mobile devices. You can then build Kinesis Applications to continuously process the data, generate metrics, power live dashboards, and emit aggregated data into stores such as Amazon S3.</p><p>You can have your Kinesis Applications run real-time analytics on high frequency event data such as sensor data collected by Kinesis Data Streams, which enables you to gain insights from your data at a frequency of minutes instead of hours or days.</p><p>Hence, the following option is the correct answer as Amazon Kinesis is the one used here to collect the streaming data:</p><p><strong>1. Use Amazon Kinesis Data Streams to collect the incoming biometric data.</strong></p><p><strong>2. Analyze the data using Amazon Kinesis and show the results in a real-time dashboard.</strong></p><p><strong>3. Set up a simple data aggregation process and pass the results to Amazon S3.</strong></p><p><strong>4. Store the data to Amazon Redshift, configured with automated backups, to handle complex analytics.</strong></p><p>The following option is incorrect because S3, Data Pipeline, and Redshift do not provide real-time data analytics:</p><p><strong>1. Create an Amazon S3 bucket to collect the incoming biometric data from the smart pet collar.</strong></p><p><strong>2. Use Amazon Data Pipeline to run a data analysis task in the S3 bucket every day.</strong></p><p><strong>3. Use Amazon Redshift as the online analytic processing (OLAP) database for the web portal.</strong></p><p>The following option is incorrect because an SQS queue is not appropriate to use to accept all of the incoming biometric data. You should use Amazon Kinesis instead:</p><p><strong>1. Create an Amazon SQS queue to collect the incoming biometric data.</strong></p><p><strong>2. Analyze the data from SQS with Amazon Kinesis.</strong></p><p><strong>3. Store the results to an Amazon RDS for MySQL database<em>.</em></strong></p><p>The following option is incorrect because just like in the above, it does not use Amazon Kinesis to accept the incoming data:</p><p><strong>1. Launch an Amazon Elastic MapReduce instance to collect the incoming biometrics data.</strong></p><p><strong>2. Use Amazon Kinesis to analyze the data.</strong></p><p><strong>3. Save the results to an Amazon DynamoDB table.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/details/\">https://aws.amazon.com/kinesis/data-streams/details/</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/introduction.html\">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
	},
	{
		"question": "<p>A leading financial company is planning to launch its MERN (MongoDB, Express, React, Node.js) application with an Amazon RDS MariaDB database to serve its clients worldwide. The application will run on both on-premises servers as well as Reserved EC2 instances. To comply with the company's strict security policy, the database credentials must be encrypted both at rest and in transit. These credentials will be used by the application servers to connect to the database. The Solutions Architect is tasked to manage all of the aspects of the application architecture and production deployment. </p><p>How should the Architect automate the deployment process of the application in the MOST secure manner?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Upload the database credentials with key rotation in AWS Secrets Manager. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Attach this IAM policy to the instance profile for CodeDeploy-managed EC2 instances. Associate the same policy as well to the on-premises instances. Using AWS CodeDeploy, launch the application packages to the Amazon EC2 instances and on-premises servers.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role<strong> </strong>to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to the EC2 instances. Create an IAM Service Role that will be associated with the on-premises servers. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, and license codes as parameter values. You can store values as plain text or encrypted data. You can then reference values by using the unique name that you specified when you created the parameter. Highly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.</p><p>Servers and virtual machines (VMs) in a hybrid environment require an IAM role to communicate with the Systems Manager service. The role grants <code>AssumeRole</code> trust to the Systems Manager service. You only need to create the service role for a hybrid environment once for each AWS account.</p><p>Users in your company or organization who will use Systems Manager on your hybrid machines must be granted permission in IAM to call the SSM API.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_parameter_store.png\"></p><p><strong>Service role</strong>: A service role is an AWS Identity and Access Management (IAM) that grants permissions to an AWS service so that the service can access AWS resources. Only a few Systems Manager scenarios require a service role. When you create a service role for Systems Manager, you choose the permissions to grant in order for it to access or interact with other AWS resources.</p><p><strong>Service-linked role</strong>: A service-linked role is predefined by Systems Manager and includes all the permissions that the service requires to call other AWS services on your behalf.</p><p>If you plan to use Systems Manager to manage on-premises servers and virtual machines (VMs) in what is called a <strong>hybrid environment</strong>, you must create an IAM role for those resources to communicate with the Systems Manager service.</p><p>Hence, the correct answer is: <strong>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to the EC2 instances. Create an IAM Service Role that will be associated with the on-premises servers. Deploy the application packages to the EC2 instances and on-premises servers using AWS CodeDeploy.</strong></p><p>The option that says: <strong>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk</strong> is incorrect. You can't deploy an application to your on-premises servers using Elastic Beanstalk. This is only applicable to your Amazon EC2 instances.</p><p>The option that says: <strong>Upload the database credentials with a Secure String data type in AWS Systems Manager Parameter Store. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Attach this IAM policy to the instance profile for CodeDeploy-managed EC2 instances. Associate the same policy as well to the on-premises instances. Using AWS CodeDeploy, launch the application packages to the Amazon EC2 instances and on-premises servers</strong> is incorrect. You have to use an IAM Role and not an IAM Policy to grant access to AWS Systems Manager Parameter Store.</p><p>The option that says: <strong>Upload the database credentials with key rotation in AWS Secrets Manager. Install the AWS SSM agent on all servers. Set up a new IAM role that enables access and decryption of the database credentials from SSM Parameter Store. Associate this role to all on-premises servers and EC2 instances. Use Elastic Beanstalk to host and manage the application on both on-premises servers and EC2 instances. Deploy the succeeding application revisions to AWS and on-premises servers using Elastic Beanstalk</strong> is incorrect. Although you can store the database credentials to AWS Secrets Manager, you still can't deploy an application to your on-premises servers using Elastic Beanstalk.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-service-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-service-role.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-service-role.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A company is hosting its flagship product page on a three-tier web application in its on-premises data center. The popularity of the last product launch attracted a sudden surge of traffic to their site, which caused some downtime that resulted in a significant impact on the product’s sales volume. The management decided to move the application to AWS. The application uses a MySQL database and is written in .NET framework. The Solutions Architect must design a highly available and scalable infrastructure to handle the demand of 300,000 peak users.</p><p>Which of the following design options would satisfy the above requirements while being cost-effective?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an AWS Elastic Beanstalk application with an Auto Scaling group of EC2 instances as web servers that spans two separate regions. Put the EC2 instances behind an Application Load Balancer in each region. Launch a Multi-AZ Amazon Aurora MySQL database with cross-region read replica to the other region. Create zone entries in Route 53 with <code>geoproximity</code> routing policy to direct the traffic between the two regions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an AWS Elastic Beanstalk application that contains a web server tier and an Amazon RDS MySQL Multi-AZ database tier. The web server tier should launch a fleet of Amazon EC2 Auto Scaling Group spanning multiple Availability Zones and behind a Network Load Balancer. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the NLB.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Launch a CloudFormation stack that contains an Auto Scaling Group of Amazon EC2 instances spanning multiple Availability Zones that are behind an Application Load Balancer. Use the stack to launch an Amazon Aurora MySQL database cluster in a Multi-AZ configuration with a “retain” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch a CloudFormation stack that contains an Amazon ECS cluster that spans multiple Availability Zones using Spot Instances. Create an Application Load Balancer in front of the ECS cluster. Use the stack to launch an Amazon RDS MySQL database in Multi-AZ configuration with a “snapshot” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudFormation</strong> gives you an easy way to model a collection of related AWS resources, provision them quickly and consistently, and manage them throughout their lifecycles, by treating infrastructure as code. A CloudFormation template describes your desired resources and their dependencies so you can launch and configure them together as a stack.</p><p>In CloudFormation, the <code>AWS::AutoScaling::AutoScalingGroup</code> resource defines an Amazon EC2 Auto Scaling group, which is a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.</p><p>Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to manage incoming requests by optimally routing traffic so that no one instance is overwhelmed.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_nested_stacks.png\"></p><p>To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. An Application Load Balancer is ideal for this scenario as routes and load balances at the application layer (HTTP/HTTPS) and supports path-based routing.</p><p>On CloudFormation you can set the RDS DeletionPolicy as “Retain” which keeps the resource without deleting it or its contents when its stack is deleted. This is helpful in the event that the stack is deleted and you need to quickly provision a new stack. You can quickly use the retained RDS instance from the old stack.</p><p>Therefore, the correct answer is:<strong> Launch a CloudFormation stack that contains an Auto Scaling Group of Amazon EC2 instances spanning multiple Availability Zones that are behind an Application Load Balancer. Use the stack to launch an Amazon Aurora MySQL database cluster in a Multi-AZ configuration with a “retain” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB.</strong></p><p>The option that says: <strong>Create an AWS Elastic Beanstalk application that contains a web server tier and an Amazon RDS MySQL Multi-AZ database tier. The web server tier should launch a fleet of Amazon EC2 Auto Scaling Group spanning multiple Availability Zones and behind a Network Load Balancer. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the NLB </strong>is incorrect. You do not need an expensive Network Load Balancer (NLB) to handle the expected peak traffic. An NLB is a good choice if you expect millions of requests per second.</p><p>The option that says: <strong>Create an AWS Elastic Beanstalk application with an Auto Scaling group of EC2 instances as web servers that spans two separate regions. Put the EC2 instances behind an Application Load Balancer in each region. Launch a Multi-AZ Amazon Aurora MySQL database with cross-region read replica to the other region. Create zone entries in Route 53 with geoproximity routing policy to direct the traffic between the two regions</strong> is incorrect. Creating two Auto Scaling groups on separate regions is unnecessary and expensive. Distributing the EC2 instance in multiple Availability Zones is enough to handle the traffic. In this setup, the database on the second region is a read-replica only so any writes to the database will have to be sent on the main region’s RDS instance.</p><p>The option that says: <strong>Launch a CloudFormation stack that contains an Amazon ECS cluster that spans multiple Availability Zones using Spot Instances. Create an Application Load Balancer in front of the ECS cluster. Use the stack to launch an Amazon RDS MySQL database in Multi-AZ configuration with a “snapshot” deletion policy. Create a Route 53 zone entry for the company’s domain name with an Alias-record pointed to the ALB</strong> is incorrect. Although the Spot instance provides good cost savings for the web tier, the reliability of the site will suffer as the Spot instances are usually reclaimed by AWS based on the supply and demand of its global computing capacity. The “snapshot” deletion policy on the database tier is also not ideal as this will require a significant time to restore if you delete the CloudFormation stack.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/parallel-stack-processing-and-nested-stack-updates-for-aws-cloudformation/\">https://aws.amazon.com/blogs/aws/parallel-stack-processing-and-nested-stack-updates-for-aws-cloudformation/</a></p><p><br></p><p><strong>Check out these Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>AWS CloudFormation - Templates, Stacks, Change Sets:</strong></p><p><a href=\"https://youtu.be/9Xpuprxg7aY\">https://youtu.be/9Xpuprxg7aY </a></p></div>"
	},
	{
		"question": "<p>A global financial company is launching its new trading platform in AWS which allows people to buy and sell their bitcoin, ethereum, ripple, and other cryptocurrencies, as well as access to various financial reports. To meet the anti-money laundering and counter-terrorist financing (AML/CFT) measures compliance, all report files of the trading platform must not be accessible in certain countries which are listed in the Financial Action Task Force (FATF) list of non-cooperative countries or territories. You were given a task to ensure that the company complies with this requirement to avoid hefty monetary penalties.</p><p>In this scenario, what is the best way to satisfy this security requirement in AWS while still delivering content to users around the globe with lower latency?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use Route53 with a Geoproximity routing policy that blocks all traffic from the blacklisted countries."
			},
			{
				"correct": false,
				"answer": "Use Route53 with a Geolocation routing policy that blocks all traffic from the blacklisted countries."
			},
			{
				"correct": false,
				"answer": "Deploy the trading platform using Elastic Beanstalk and deny all incoming traffic from the IP addresses of the blacklisted countries in the Network Access Control List (ACL) of the VPC."
			},
			{
				"correct": true,
				"answer": "<p>Create a CloudFront distribution with Geo-Restriction enabled to block all of the blacklisted countries from accessing the trading platform.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use <strong>geo restriction</strong> - also known as geoblocking - to prevent users in specific geographic locations from accessing content that you're distributing through a CloudFront web distribution. To use geo restriction, you have two options:</p><p>Use the CloudFront geo restriction feature. Use this option to restrict access to all of the files that are associated with a distribution and to restrict access at the country level.</p><p>Use a third-party geolocation service. Use this option to restrict access to a subset of the files that are associated with a distribution or to restrict access at a finer granularity than the country level.</p><p>When a user requests your content, <strong>CloudFront</strong> typically serves the requested content regardless of where the user is located. If you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:</p><p>Allow your users to access your content only if they're in one of the countries on a whitelist of approved countries.</p><p>Prevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries.</p><p>For example, if a request comes from a country where, for copyright reasons, you are not authorized to distribute your content, you can use CloudFront geo restriction to block the request.</p><p>Hence, the option that says: <strong>Create a CloudFront distribution with Geo-Restriction enabled to block all of the blacklisted countries from accessing the trading platform</strong> is correct. CloudFront can provide the users low-latency access to the files as well as block certain countries on the FTAF list.</p><p>The option that says:<strong> Deploy the trading platform using Elastic Beanstalk and deny all incoming traffic from the IP addresses of the blacklisted countries in the Network Access Control List (ACL) of the VPC</strong> is incorrect. Blocking all of the IP addresses of each blacklisted country in the Network Access Control List entails a lot of work and is not a recommended way to accomplish the task. Using CloudFront geo restriction feature is a better solution for this.</p><p>The following options are incorrect because Route 53 only provides Domain Name Resolution and sends the requests based on the configured entries. It does not provide low-latency access to users around the globe, unlike CloudFront.</p><p><strong>Use Route 53 with a Geolocation routing policy that blocks all traffic from the blacklisted countries.</strong></p><p><strong>Use Route 53 with a Geoproximity routing policy that blocks all traffic from the blacklisted countries.</strong></p><p>Geolocation routing policy is used when you want to route traffic based on the location of your users while Geoproximity routing policy is for scenarios where you want to route traffic based on the location of your resources and, optionally, shift traffic from resources on one location to resources in another.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Latency Routing vs Geoproximity Routing vs Geolocation Routing:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/?src=udemy\">https://tutorialsdojo.com/aws-cheat-sheet-latency-routing-vs-geoproximity-routing-vs-geolocation-routing/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services-for-udemy-students/</a></p></div>"
	},
	{
		"question": "A print media company has a popular web application hosted on their on-premises network which allows anyone around the globe to search its back catalog and retrieve individual newspaper pages on their web portal. They have scanned the old newspapers into PNG image format and used Optical Character Recognition (OCR) software to automatically convert images to a text file. The license of their OCR software will expire soon and the news organization decided to move to AWS and produce a scalable, durable, and highly available architecture.\n\nWhich is the best option to achieve this requirement?",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use S3 Intelligent-Tiering storage class to store and serve the scanned files. Migrate the on-premises web application as well as the Optical Character Recognition (OCR) software to an Auto Scaling group of Spot EC2 Instances across multiple Availability Zones with an Application Load Balancer to balance the incoming load. Use Amazon Rekognition to detect and recognize text from the scanned old newspapers.</p>"
			},
			{
				"correct": false,
				"answer": "Store the images in an S3 bucket and prepare a separate bucket to host the static website. Utilize S3 Select for searching the images stored in S3. Set up a lifecycle policy to move the images to Glacier after 3 months and if needed, use Glacier Select to query the archives."
			},
			{
				"correct": false,
				"answer": "Create a new CloudFormation template which has EBS-backed EC2 instances with an Application Load Balancer in front. Install and run an NGINX web server and an open source search application. Store the images to EBS volumes with Amazon Data Lifecycle Manager configured, and which automatically attach new volumes to the EC2 instances as required."
			},
			{
				"correct": true,
				"answer": "<p>Create a new S3 bucket to store and serve the scanned image files using a CloudFront web distribution. Launch a new Elastic Beanstalk environment to host the website across multiple Availability Zones and set up a CloudSearch for query processing, which the website can use. Use Amazon Rekognition to detect and recognize text from the scanned old newspapers.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudSearch</strong> is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application.</p><p>With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance. With a few clicks in the AWS Management Console, you can create a search domain and upload the data that you want to make searchable, and Amazon CloudSearch will automatically provision the required resources and deploy a highly tuned search index.</p><p>You can easily change your search parameters, fine-tune search relevance, and apply new settings at any time. As your volume of data and traffic fluctuates, Amazon CloudSearch seamlessly scales to meet your needs.</p><p>Hence, the option that says: <strong>Create a new S3 bucket to store and serve the scanned image files using a CloudFront web distribution. Launch a new Elastic Beanstalk environment to host the website across multiple Availability Zones and set up a CloudSearch for query processing, which the website can use. Use Amazon Rekognition to detect and recognize text from the scanned old newspapers</strong> is correct because it satisfies the requirement given in the scenario i.e. it uses S3 to store the images, instead of the commercial product which will be decommissioned soon. More importantly, it uses CloudSearch for query processing, and in addition, it uses Multi-AZ implementation which provides high availability. It is also correct to use Amazon Rekognition to detect and recognize text from the scanned old newspapers.</p><p><strong>Amazon Rekognition</strong> makes it easy to add image and video analysis to your applications. You just provide an image or video to the Rekognition API, and the service can identify the objects, people, text, scenes, and activities, as well as detect any inappropriate content. Amazon Rekognition also provides highly accurate facial analysis and facial recognition on images and video that you provide. You can detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rekognition_flow.jpg\"></p><p>The option that says:<strong><em> </em>Create a new CloudFormation template which has EBS-backed EC2 instances with an Application Load Balancer in front. Install and run an NGINX web server and an open source search application. Store the images to EBS volumes with Amazon Data Lifecycle Manager configured, and which automatically attach new volumes to the EC2 instances as required</strong> is incorrect. An EBS volume is not a scalable nor a durable solution compared with S3. In addition, it is not as cost-effective compared to S3 since it entails maintenance overhead unlike the fully managed storage service provided by S3.</p><p>The option that says: <strong>Store the images in an S3 bucket and prepare a separate bucket to host the static website. Utilize S3 Select for searching the images stored in S3. Set up a lifecycle policy to move the images to Glacier after 3 months and if needed, use Glacier Select to query the archives</strong> is incorrect. Although using S3 Select is a feasible option, it is not as scalable compared to CloudSearch. Amazon S3 Select can only retrieve a subset of data using SQL statements. Storing your data to Amazon Glacier will also affect the retrieval time of your data.</p><p>The option that says: <strong>Use S3 Intelligent-Tiering storage class to store and serve the scanned files. Migrate the on-premises web application as well as the Optical Character Recognition (OCR) software to an Auto Scaling group of Spot EC2 Instances across multiple Availability Zones with an Application Load Balancer to balance the incoming load. Use Amazon Rekognition to detect and recognize text from the scanned old newspapers</strong> is incorrect. Even though it properly uses S3 for durable and scalable storage, it still uses the Optical Character Recognition (OCR) software which will be decommissioned soon. It is better to use CloudSearch instead.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/cloudsearch/\">https://aws.amazon.com/cloudsearch/</a></p><p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p><p><br></p><p><strong>Check out these Amazon CloudSearch and Amazon Rekognition Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudsearch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudsearch/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p></div>"
	},
	{
		"question": "<p>A company develops new android and iOS mobile apps. The company is considering storing user customization data in AWS. This would provide a more uniform cross-platform experience to their users using multiple mobile devices to access their apps. The preference data for each user is estimated to be 4 KB in size. Additionally, 3 million customers are expected to use the application on a regular basis, using their social login accounts for easier user authentication.</p><p>How should the Solutions Architect design a highly available, cost-effective, scalable, and secure solution to meet the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Launch an RDS MySQL instance in 2 availability zones to contain the user preference data. Deploy a public-facing application on a server in front of the database which will manage authentication and access controls.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data. The mobile application will then query the user preferences from the read replicas. Finally, utilize MySQL's user management and access privilege system to handle the security and access credentials of the users.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Have the user preference data stored in S3, and set up a DynamoDB table with an item for each user and an item attribute referencing the user's S3 object. The mobile app will retrieve the S3 URL from DynamoDB and then access the S3 object directly utilizing STS, Web identity Federation, and S3 Access Points.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Provision a table in DynamoDB containing an item for each user having the necessary attributes to hold the user preferences. The mobile app will query the user preferences directly from the table. Use STS, Web Identity Federation, and DynamoDB's Fine-Grained Access Control for authentication and authorization.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Take note that the question mentioned the use of social media logins. With web identity federation, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known identity provider (IdP) —such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure, because you don't have to embed and distribute long-term security credentials with your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_items.JPG\"></p><p>The option that says: <strong>Provision a table in DynamoDB containing an item for each user having the necessary attributes to hold the user preferences. The mobile app will query the user preferences directly from the table. Use STS, Web Identity Federation, and DynamoDB's Fine Grained Access Control for authentication and authorization</strong> is correct because it uses DynamoDB for scalability and cost-efficiency. It uses federated access using Web Identity Provider, and uses fine-grained access privileges for authenticating the access.</p><p>The option that says: <strong>Have the user preference data stored in S3, and set up a DynamoDB table with an item for each user and an item attribute referencing the user's S3 object. The mobile app will retrieve the S3 URL from DynamoDB and then access the S3 object directly utilizing STS, Web identity Federation, and S3 Access Points </strong>is incorrect because it doesn’t use DynamoDB's built-in Fine-Grained Access Control for authentication and authorization feature. Additionally, using Amazon S3 bucket with DynamoDB is recommended for storing large data with the metadata stored on DynamoDB. The user preference data is only 4KB in size which can be stored effectively in a DynamoDB table. The use of S3 Access points is not necessary because these are just unique hostnames that enforce distinct permissions and network controls for any request made through the access point.</p><p>The option that says: <strong>Launch an RDS MySQL instance in 2 availability zones to contain the user preference data. Deploy a public-facing application on a server in front of the database which will manage authentication and access controls</strong> is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB.</p><p>The option that says: <strong>Create an RDS MySQL instance with multiple read replicas in 2 availability zones to store the user preference data. The mobile application will then query the user preferences from the read replicas. Finally, utilize MySQL's user management and access privilege system to handle the security and access credentials of the users </strong>is incorrect because RDS MySQL is not as scalable and cost-effective as DynamoDB. Additionally, the user management and access privilege system for RDS cannot be used for controlling access.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/dynamodb/developer-resources\">https://aws.amazon.com/dynamodb/developer-resources</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html</a></p><p><a href=\"https://developer.amazon.com/blogs/appstore/post/TxZR5F5K6QINFQ/storing-user-preference-in-amazon-dynamodb-using-the-aws-sdk-for-android\">https://developer.amazon.com/blogs/appstore/post/TxZR5F5K6QINFQ/storing-user-preference-in-amazon-dynamodb-using-the-aws-sdk-for-android</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A company wants to launch its online shopping website to give customers an easy way to purchase the products they need. The proposed setup is to host the application on an AWS Fargate cluster, utilize a Load Balancer to distribute traffic between the Fargate tasks, and use Amazon CloudFront for caching and content delivery. The company wants to ensure that the website complies with industry best practices and should be able to protect customers from common “man-in-the-middle” attacks for e-commerce websites such as DNS spoofing, HTTPS spoofing, or SSL hijacking.</p><p>Which of the following configurations will provide the MOST secure access to the website?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Register the domain name on Route 53. Use a third-party DNS provider that supports the import of the customer-managed keys for DNSSEC. Import a 2048-bit TLS/SSL certificate from a third-party certificate service to AWS Certificate Manager (ACM). Configure the Application Load Balancer with an HTTPS listener to use the imported TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Route 53 for domain registration. Use a third-party DNS service that supports DNSSEC for DNS requests that use the customer-managed keys. Use AWS Certificate Manager (ACM) to generate a valid 2048-bit TLS/SSL certificate for the domain name and configure the Application Load Balancer HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Register the domain name on Route 53 and enable DNSSEC validation for all public hosted zones to ensure that all DNS requests have not been tampered with during transit. Use AWS Certificate Manager (ACM) to generate a valid TLS/SSL certificate for the domain name. Configure the Application Load Balancer with an HTTPS listener to use the ACM TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Register the domain name on Route 53. Since Route 53 only supports DNSSEC for registration, host the company DNS root servers on Amazon EC2 instances running the BIND service. Enable DNSSEC for DNS requests to ensure the replies have not been tampered with. Generate a valid certificate for the website domain name on AWS ACM and configure the Application Load Balancers HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon now allows you to enable <strong>Domain Name System Security Extensions (DNSSEC)</strong> signing for all existing and new public hosted zones, and enable DNSSEC validation for Amazon Route 53 Resolver. Amazon Route 53 DNSSEC provides data origin authentication and data integrity verification for DNS and can help customers meet compliance mandates, such as FedRAMP.</p><p>When you enable DNSSEC signing on a hosted zone, Route 53 cryptographically signs each record in that hosted zone. Route 53 manages the zone-signing key, and you can manage the key-signing key in AWS Key Management Service (AWS KMS). Amazon’s domain name registrar, Route 53 Domains, already supports DNSSEC, and customers can now register domains and host their DNS on Route 53 with DNSSEC signing enabled. When you enable DNSSEC validation on the Route 53 Resolver in your VPC, it ensures that DNS responses have not been tampered with in transit. This can prevent DNS Spoofing.</p><p><img src=\"https://media.tutorialsdojo.com/Route53_DNSSEC.png\"></p><p><strong>AWS Certificate Manager</strong> is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates. Using a valid SSL Certificate for your application load balancer ensures that all requests are encrypted on transit as well as protection against SSL hijacking.</p><p>CloudFront supports Server Name Indication (SNI) for custom SSL certificates, along with the ability to take incoming HTTP requests and redirect them to secure HTTPS requests to ensure that clients are always directed to the secure version of your website.</p><p>Therefore, the correct answer is: <strong>Register the domain name on Route 53 and enable DNSSEC validation for all public hosted zones to ensure that all DNS requests have not been tampered with during transit. Use AWS Certificate Manager (ACM) to generate a valid TLS/SSL certificate for the domain name. Configure the Application Load Balancer with an HTTPS listener to use the ACM TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront.</strong></p><p>The option that says: <strong>Register the domain name on Route 53. Use a third-party DNS provider that supports the import of the customer-managed keys for DNSSEC. Import a 2048-bit TLS/SSL certificate from a third-party certificate service to AWS Certificate Manager (ACM). Configure the Application Load Balancer with an HTTPS listener to use the imported TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront</strong> is incorrect. Although this is possible, you don’t have to rely on a third-party DNS provider as Route 53 supports DNSSEC signing. Also, ACM can secure a 2048-bit TLS/SSL Certificate for free so you don't have to buy certificates from other providers.</p><p>The option that says:<strong> Use Route 53 for domain registration. Use a third-party DNS service that supports DNSSEC for DNS requests that use the customer-managed keys. Use AWS Certificate Manager (ACM) to generate a valid 2048-bit TLS/SSL certificate for the domain name and configure the Application Load Balancer HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront </strong>is incorrect. This is also possible, but you don't have to rely on a third-party DNS provider as Amazon Route 53 already supports DNSSEC signing.</p><p>The option that says:<strong> Register the domain name on Route 53. Since Route 53 only supports DNSSEC for registration, host the company DNS root servers on Amazon EC2 instances running the BIND service. Enable DNSSEC for DNS requests to ensure the replies have not been tampered with. Generate a valid certificate for the website domain name on AWS ACM and configure the Application Load Balancers HTTPS listener to use this TLS/SSL certificate. Use Server Name Identification and HTTP to HTTPS redirection on CloudFront</strong> is incorrect as this solution is no longer recommended. This setup was previously used as a workaround when DNSSEC signing was not supported natively yet in Amazon Route 53.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-configure-dnssec.html</a></p><p><a href=\"https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html\">https://docs.aws.amazon.com/acm/latest/userguide/acm-services.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/12/announcing-amazon-route-53-support-dnssec/\">https://aws.amazon.com/about-aws/whats-new/2020/12/announcing-amazon-route-53-support-dnssec/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-certificate-manager/?src=udemy\">https://tutorialsdojo.com/aws-certificate-manager/</a></p></div>"
	},
	{
		"question": "<p>A company runs a sports web portal that covers the latest cricket news in Australia. The solutions architect manages the main AWS account which has resources in multiple AWS regions. The web portal is hosted on a fleet of on-demand EC2 instances and an RDS database which are also deployed to other AWS regions. The IT Security Compliance Officer has given the solutions architect the task of developing a reliable and durable logging solution to track changes made to all of your EC2, IAM, and RDS resources in all of the AWS regions. The solution must ensure the integrity and confidentiality of the log data.</p><p>Which of the following solutions would be the best option to choose?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create three new CloudTrail trails, each with its own S3 bucket to store the logs: one for the AWS Management console, one for AWS SDKs, and one for command line tools. Then create IAM roles and S3 bucket policies for the S3 buckets storing your logs. "
			},
			{
				"correct": false,
				"answer": "Create a new trail in AWS CloudTrail with the global services option selected, and assign it an existing S3 bucket to store the logs. Create S3 ACLs and enable Multi Factor Authentication (MFA) delete on the S3 bucket storing your logs."
			},
			{
				"correct": false,
				"answer": "Create a new trail in CloudTrail and assign it a new S3 bucket to store the logs. Configure AWS SNS to send delivery notifications to your management system. Secure the S3 bucket that stores your logs using IAM roles and S3 bucket policies."
			},
			{
				"correct": true,
				"answer": "Create a new trail in AWS CloudTrail with the global services option selected, and create one new Amazon S3 bucket to store the logs. Create IAM roles, S3 bucket policies, and enable Multi Factor Authentication (MFA) Delete on the S3 bucket storing your logs."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For most services, events are recorded in the region where the action occurred to its respective AWS CloudTrail. For global services such as AWS Identity and Access Management (IAM), AWS STS, Amazon CloudFront, and Route 53, events are delivered to any trail that includes global services (IncludeGlobalServiceEvents flag). AWS CloudTrail service should be your top choice for the scenarios where the application is tracking the changes made by any AWS service, resource, or API.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_s3_logs.PNG\"><br>AWS Identity and Access Management (IAM) is integrated with AWS CloudTrail, a service that logs AWS events made by or on behalf of your AWS account. CloudTrail logs authenticate AWS API calls and also AWS sign-in events, and collects this event information in files that are delivered to Amazon S3 buckets.</p><p>Therefore, the correct answer is: <strong>Create a new trail in AWS CloudTrail with the global services option selected, and create one new Amazon S3 bucket to store the logs. Create IAM roles, S3 bucket policies, and enable Multi Factor Authentication (MFA) Delete on the S3 bucket storing your logs.</strong> It uses AWS CloudTrail with (includeGlobalServiceEvents flag) Global Option enabled, a single new S3 bucket and IAM Roles so that it has the confidentiality, and MFA on Delete on S3 bucket so that it maintains the data integrity.</p><p>The option that says: <strong>Create a new trail in AWS CloudTrail with the global services option selected, and assign it an existing S3 bucket to store the logs. Create S3 ACLs and enable Multi Factor Authentication (MFA) delete on the S3 bucket storing your logs</strong> is incorrect. As an existing S3 bucket is used, it may already be accessed by the user, hence not maintaining the confidentiality, and it is not using IAM roles.</p><p>The option that says: <strong>Create three new CloudTrail trails, each with its own S3 bucket to store the logs: one for the AWS Management console, one for AWS SDKs, and one for command line tools. Then create IAM roles and S3 bucket policies for the S3 buckets storing your logs</strong> is incorrect. Although it uses AWS CloudTrail, the Global Option is not enabled, and three S3 buckets are not needed.</p><p>The option that says: <strong>Create a new trail in CloudTrail and assign it a new S3 bucket to store the logs. Configure AWS SNS to send delivery notifications to your management system. Secure the S3 bucket that stores your logs using IAM roles and S3 bucket policies</strong> is incorrect. Although it uses AWS CloudTrail, the Global Option is not enabled.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><br></p><p><strong>Check out these AWS CloudTrail and IAM Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A small company has several AWS accounts that are used by multiple teams. To centralize DNS record keeping, the company has created a private hosted zone in Amazon Route 53 on the main Account A. The new application and database servers are hosted on a VPC in Account B. The CNAME record set <code>db.turotialsdojo.com</code> has been created for the Amazon RDS endpoint on the private hosted zone in Amazon Route 53. Upon deployment, the application on the Amazon EC2 instances failed to start. The application logs indicate that the database endpoint <code>db.turotialsdojo.com</code> is not resolvable. However, the solutions architect can confirm that the Route 53 entry is configured correctly.</p><p>Which of the following options is the recommended solution for this issue? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create custom AMI for the Amazon EC2 instances that have an updated /etc/resolv.conf file containing the Amazon RDS endpoint to private IP address mapping.</p>"
			},
			{
				"correct": true,
				"answer": "<p>On Account A, create an authorization to associate its private hosted zone to the new VPC in Account B.</p>"
			},
			{
				"correct": true,
				"answer": "<p>On Account B, associate the VPC to the private hosted zone in Account A. Delete the association authorization after the association is created.</p>"
			},
			{
				"correct": false,
				"answer": "<p>On Account B, create a new private hosted zone in Amazon Route 53. Associate this zone to the private hosted zone in Account A to allow replication between the AWS accounts.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a VPC peering between the Account A VPC and Account B VPC. Configure the Amazon EC2 instances on Account B to use the DNS resolver IPs in Account A to resolve the Amazon RDS endpoint.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use the <strong>Amazon Route 53</strong> console to associate more VPCs with a private hosted zone if you created the hosted zone and the VPCs by using the same AWS account. Additionally, you can associate a VPC from one account with a private hosted zone in a different account.</p><p>If you want to associate VPCs that you created by using one account with a private hosted zone that you created by using a different account, you first must authorize the association. In addition, you can't use the AWS console either to authorize the association or associate the VPCs with the hosted zone.</p><p>To associate an Amazon VPC and a private hosted zone that you created with different AWS accounts, perform the following procedure:</p><p>Using the account that created the hosted zone, authorize the association of the VPC with the private hosted zone by using one of the following methods:AWS CLI – using the <code>create-vpc-association-authorization</code> in the AWS CLIAWS SDK or AWS Tools for Windows PowerShellAmazon Route 53 API – Using the <code>CreateVPCAssociationAuthorization</code> API</p><p>Note the following:</p><p>- If you want to associate multiple VPCs that you created with one account with a hosted zone that you created with a different account, you must submit one authorization request for each VPC.</p><p>- When you authorize the association, you must specify the hosted zone ID, so the private hosted zone must already exist.</p><p>- You can't use the Route 53 console either to authorize the association of a VPC with a private hosted zone or to make the association.</p><p>Using the account that created the VPC, associate the VPC with the hosted zone. As with authorizing the association, you can use the AWS SDK, Tools for Windows PowerShell, the AWS CLI, or the Route 53 API.</p><p><em>Optional but recommended</em> – Delete the authorization to associate the VPC with the hosted zone. Deleting the authorization does not affect the association, it just prevents you from reassociating the VPC with the hosted zone in the future. If you want to reassociate the VPC with the hosted zone, you'll need to repeat steps 1 and 2 of this procedure.</p><p>Therefore, the correct answers are:</p><p><strong>On Account A, create an authorization to associate its private hosted zone to the new VPC in Account B.</strong></p><p><strong>On Account B, associate the VPC to the private hosted zone in Account A. Delete the association authorization after the association is created.</strong></p><p>The option that says: <strong>Create a VPC peering between the Account A VPC and Account B VPC. Configure the Amazon EC2 instances on Account B to use the DNS resolver IPs in Account A to resolve the Amazon RDS endpoint</strong> is incorrect. This may be possible to configure, however, Route 53 in Account A has no knowledge of any RDS instance which are hosted on Account B. Therefore the EC2 instances still won't be able to resolve the DB endpoint.</p><p>The option that says: <strong>Create custom AMI for the Amazon EC2 instances that have an updated /etc/resolv.conf file containing the Amazon RDS endpoint to private IP address mapping</strong> is incorrect. Creating a static mapping of the RDS endpoint to a private IP address is not recommended. The internal private IPs of RDS instances may change over time.</p><p>The option that says: <strong>On Account B, create a new private hosted zone in Amazon Route 53. Associate this zone to the private hosted zone in Account A to allow replication between the AWS accounts</strong> is incorrect. This is not possible as you can't have replication between Route 53 in separate accounts.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/\">https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-different-account/</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zone-private-associate-vpcs-different-accounts.html</a></p><p><br></p><p><strong>Check out the Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>An IT consulting company has multiple AWS accounts for its teams and departments that have been grouped into several organizational units (OUs) using AWS Organizations. The lead solutions architect received a report from the security team that there was a suspected breach in one of the environments wherein a third-party AWS account was suddenly added to the AWS Organization without any prior approval. The external account has high-level access privileges to the accounts that the company owns. Fortunately, no detrimental action was performed yet.</p><p>Which of the following actions should the solutions architect take to properly set up a monitoring system that notifies for any changes to the company AWS accounts? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use AWS Config to monitor the compliance of your AWS Organizations. Set up an SNS Topic or CloudWatch Events that will send alerts to you for any changes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Provision an AWS-approved third-party monitoring tool from the AWS Marketplace that would send alerts if a breach was detected. Use AWS GuardDuty to analyze any possible breach and notify the administrators using AWS SNS.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a trail in Amazon CloudTrail to capture all API calls to your AWS Organizations, including calls from the AWS Organizations console and from code calls to the AWS Organizations APIs. Use CloudWatch Events and SNS to raise events when administrator-specified actions occur in an organization and send a notification to you.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Monitor all changes to your organization using Systems Manager and use CloudWatch Events to notify you of any new activity to your account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a CloudWatch Dashboard to monitor any changes to your organizations and create an SNS topic that would send you a notification.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Organizations</strong> can work with CloudWatch Events to raise events when administrator-specified actions occur in an organization. For example, because of the sensitivity of such actions, most administrators would want to be warned every time someone creates a new account in the organization or when an administrator of a member account attempts to leave the organization. You can configure CloudWatch Events rules that look for these actions and then send the generated events to administrator-defined targets. Targets can be an Amazon SNS topic that emails or text messages its subscribers. Combining this with Amazon CloudTrail, you can set an event to trigger whenever a matching API call is received.</p><p>Multi-account, multi-region data aggregation in <strong>AWS Config</strong> enables you to aggregate AWS Config data from multiple accounts and regions into a single account. Multi-account, multi-region data aggregation is useful for central IT administrators to monitor compliance for multiple AWS accounts in the enterprise. An aggregator is a new resource type in AWS Config that collects AWS Config data from multiple source accounts and regions. Create an aggregator in the Region where you want to see the aggregated AWS Config data. While creating an aggregator, you can choose to add either individual account IDs or your organization.</p><p><img src=\"https://media.tutorialsdojo.com/sap_organization_cw_events.png\"></p><p>Therefore, the following options are the correct answers:</p><p><strong>- Create a trail in Amazon CloudTrail to capture all API calls to your AWS Organizations, including calls from the AWS Organizations console and from code calls to the AWS Organizations APIs. Use CloudWatch Events and SNS to raise events when administrator-specified actions occur in an organization and send a notification to you.</strong></p><p><strong>- Use AWS Config to monitor the compliance of your AWS Organizations. Set up an SNS Topic or CloudWatch Events that will send alerts to you for any changes</strong>.</p><p>The option that says: <strong>Monitoring all changes to your organization using Systems Manager and using CloudWatch Events to notify you of any new activity to your account</strong> is incorrect. AWS Systems Manager is a collection of capabilities for configuring and managing your Amazon EC2 instances, on-premises servers and virtual machines, and other AWS resources at scale. This can't be used to monitor the changes to the set up of AWS Organizations.</p><p>The option that says: <strong>Setting up a CloudWatch Dashboard to monitor any changes to your organizations and creating an SNS topic that would send you a notification</strong> is incorrect because a CloudWatch Dashboard is primarily used to monitor your AWS resources and not the configuration of your AWS Organizations. Although you can enable sharing of all CloudWatch Events across all accounts in your organization, this can't be used to monitor if there is a new AWS account added to your AWS Organizations. Most of the time, the Amazon CloudWatch Events service is primarily used to monitor your AWS resources and the applications you run on AWS in real time.</p><p>The option that says:<strong> Provisioning an AWS-approved third-party monitoring tool from the AWS Marketplace that would send alerts if a breach was detected, then using AWS GuardDuty to analyze any possible breach and notifying the administrators using AWS SNS</strong> is incorrect because this option entails a lot of configuration, which is not fit for the scenario. GuardDuty might not determine similar future incidents as malicious if it was performed by an authenticated user already within the organization.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_monitoring.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html</a></p><p><br></p><p><strong>Check out these AWS Organizations and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><a href=\"https://tutorialsdojo.com/multi-account-multi-region-data-aggregation-on-aws-config/?src=udemy\">https://tutorialsdojo.com/multi-account-multi-region-data-aggregation-on-aws-config/</a></p></div>"
	},
	{
		"question": "<p>A multinational financial company has a suite of web applications hosted in multiple VPCs in various AWS regions. As part of their security compliance, the company’s Solutions Architect has been tasked to set up a logging solution to track all of the changes made to their AWS resources in all regions, which host their enterprise accounting systems. The company is using different AWS services such as Amazon EC2 instances, Amazon S3 buckets, CloudFront web distributions, and AWS IAM. The logging solution must ensure the security, integrity, and durability of your log data in order to pass the compliance requirements. In addition, it should provide an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command-line tools, and API calls.</p><p>In this scenario, which of the following options is the best solution to use?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass the --no-include-global-service-events and --is-multi-region-trail parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass the --include-global-service-events parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The accounting firm requires a secure and durable logging solution that will track all of the activities of all AWS resources (such as EC2, S3, CloudFront, and IAM) on all regions. CloudTrail can be used for this case with multi-region trail enabled. However, CloudTrail will only cover the activities of the regional services (EC2, S3, RDS etc.) and not for global services such as IAM, CloudFront, AWS WAF, and Route 53.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_all_regions.png\"></p><p>The option that says: <strong>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is correct because it provides security, integrity, and durability to your log data. In addition, it has the -include-global-service-events parameter enabled which will also include activity from global services such as IAM, Route 53, AWS WAF, and CloudFront.</p><p>The option that says: <strong>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass both the --is-multi-region-trail and --include-global-service-events parameters then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is incorrect because you need to use CloudTrail instead of CloudWatch.</p><p>The option that says: <strong>Create a new Amazon CloudWatch trail in a new S3 bucket using the AWS CLI and also pass the --include-global-service-events parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is incorrect because you need to use CloudTrail instead of CloudWatch. In addition, the --is-multi-region-trail parameter is also missing in this setup.</p><p>The option that says: <strong>Create a new AWS CloudTrail trail in a new S3 bucket using the AWS CLI and also pass the --no-include-global-service-events and --is-multi-region-trail parameter then encrypt log files using KMS encryption. Enable Multi-Factor Authentication (MFA) Delete on the S3 bucket and ensure that only authorized users can access the logs by configuring the bucket policies</strong> is incorrect. The --is-multi-region-trail is not enough as you also need to add the --include-global-service-events parameter to track the global service events. The --no-include-global-service-events parameter actually prevents CloudTrail from publishing events from global services such as IAM to the log files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-global-service-events</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail-by-using-the-aws-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail-by-using-the-aws-cli.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p></div>"
	},
	{
		"question": "<p>A company has just launched a new central employee registry application that contains all of the public employee registration information of each staff of the company. The application has a microservices architecture running in Docker in a single AWS Region. The management teams from other departments who have their servers located in different VPCs need to connect to the central repository application to continue their work. The Solutions Architect must ensure that the traffic to the application does not traverse the public Internet. The IT Security team must also be notified of any denied requests and be able to view the corresponding source IP.</p><p>How will the Architect implement the architecture of the new application given these circumstances?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use AWS Direct Connect to create a dedicated connection between the central VPC and each of the teams' VPCs. Enable the VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to a CloudWatch Logs group. Set up an Amazon CloudWatch Logs subscription that streams the log data to the IT Security account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up an IPSec Tunnel between the central VPC and each of the teams' VPCs. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Create a CloudWatch Logs subscription that streams the log data to the IT Security account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a Transit VPC by using third-party marketplace VPN appliances running on an On-Demand Amazon EC2 instance that dynamically routes the VPN connections to the virtual private gateways (VGWs) attached to each VPC. Set up an AWS Config rule on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Link each of the teams' VPCs to the central VPC using VPC Peering. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>VPC peering connection</strong> is a networking connection between two VPCs that enables you to route traffic between them privately. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_vpc_peering.png\"></p><p>VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. Flow log data can be published to Amazon CloudWatch Logs or Amazon S3. After you've created a flow log, you can retrieve and view its data in the chosen destination.</p><p>Flow logs can help you with a number of tasks, such as:</p><p>- Diagnosing overly restrictive security group rules</p><p>- Monitoring the traffic that is reaching your instance</p><p>- Determining the direction of the traffic to and from the network interfaces</p><p>Flow log data is collected outside of the path of your network traffic, and therefore does not affect network throughput or latency. You can create or delete flow logs without any risk of impact to network performance.</p><p>Hence, the correct answer is: <strong>Link each of the teams' VPCs to the central VPC using VPC Peering. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account.</strong></p><p>The option that says: <strong>Set up an IPSec Tunnel between the central VPC and each of the teams' VPCs. Create VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Create a CloudWatch Logs subscription that streams the log data to the IT Security account</strong> is incorrect. It is mentiond in the scenario that the traffic to the application must not traverse the public Internet. Since an IPSec tunnel uses the Internet to transfer data from your VPC to a specified destination, this solution is definitely incorrect.</p><p>The option that says: <strong>Use AWS Direct Connect to create a dedicated connection between the central VPC and each of the teams' VPCs. Enable the VPC Flow Logs on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to a CloudWatch Logs group. Set up an Amazon CloudWatch Logs subscription that streams the log data to the IT Security account</strong> is incorrect. You cannot set up Direct Connect between different VPCs. AWS Direct Connect is primarily used to set up a dedicated connection between your on-premises data center and your Amazon VPC.</p><p>The option that says: <strong>Set up a Transit VPC by using third-party marketplace VPN appliances running on an On-Demand Amazon EC2 instance that dynamically routes the VPN connections to the virtual private gateways (VGWs) attached to each VPC. Set up an AWS Config rule on each VPC to capture rejected traffic requests, including the source IPs, that will be delivered to an Amazon CloudWatch Logs group. Set up a CloudWatch Logs subscription that streams the log data to the IT Security account</strong> is incorrect. An AWS Config rule is not capable of capturing the source IP of the incoming requests. A VPN appliance is using the public Internet to transfer data. Thus, it violates the requirement of ensuring that the data is securely within the AWS network.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html</a></p><p><br></p><p><strong>Check out these Amazon VPC and VPC Peering Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/vpc-peering/?src=udemy\">https://tutorialsdojo.com/vpc-peering/</a></p></div>"
	},
	{
		"question": "<p>A company hosts its multi-tiered web application on a fleet of Auto Scaling EC2 instances spread across two Availability Zones. The Application Load Balancer is in the public subnets and the Amazon EC2 instances are in the private subnets. After a few weeks of operations, the users are reporting that the web application is not working properly. Upon testing, the Solutions Architect found that the website is accessible and the login is successful. However, when the “find a nearby store” function is clicked on the website, the map loads only about 50% of the time when the page is refreshed. This function involves a third-party RESTful API call to a maps provider. Amazon EC2 NAT instances are used for these outbound API calls.</p><p>Which of the following options are the MOST likely reason for this failure and the recommended solution?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>One of the subnets in the VPC has a misconfigured Network ACL that blocks outbound traffic to the third-party provider. Update the network ACL to allow this connection and configure IAM permissions to restrict these changes in the future.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The error is caused by a failure in one of their availability zones in the VPC of the third-party provider. Contact the third-party provider support hotline and request for them to fix it.</p>"
			},
			{
				"correct": false,
				"answer": "<p>This error is caused by an overloaded NAT instance in one of the subnets. Scale the EC2 NAT instances to larger-sized instances to ensure that they can handle the growing traffic.</p>"
			},
			{
				"correct": true,
				"answer": "<p>This error is caused by failed NAT instance in one of the public subnets. Use NAT Gateways instead of EC2 NAT instances to ensure availability and scalability.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use a NAT device to enable instances in a private subnet to connect to the Internet (for example, for software updates) or other AWS services, but prevent the Internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the Internet or other AWS services, and then sends the response back to the instances. When traffic goes to the Internet, the source IPv4 address is replaced with the NAT device’s address, and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances’ private IPv4 addresses.</p><p>You can either use a managed NAT device offered by AWS called a NAT gateway, or you can create your own NAT device in an EC2 instance, referred to here as a <strong>NAT instance</strong>. The bandwidth of NAT instances depends on the instance size – higher instances size will have high bandwidth capacity. NAT instances are managed by the customer so if the instance goes down, there could be a potential impact on the availability of your application. You have to manually check and fix the NAT instances.</p><p>AWS recommends <strong>NAT gateways</strong> because they provide better availability and bandwidth over NAT instances. The NAT gateway service is also a managed service that does not require your administration efforts. NAT Gateways are highly available. In each Availability Zone, they are implemented with redundancy. It is managed by AWS so you do not have to perform maintenance or monitoring if it is UP. NAT gateways automatically scale in bandwidth so you don’t have to choose instance types.</p><p>Check out the full comparison in the table below:</p><p><img src=\"https://tutorialsdojo.com/wp-content/uploads/2018/12/Natcomparison.jpg\"></p><p>The correct answer is: <strong>This error is caused by failed NAT instance in one of the public subnets. Use NAT Gateways instead of EC2 NAT instances to ensure availability and scalability. </strong>This is very likely as we have two subnets in the scenario and NAT instances reside in only one AZ. With a failure rate of 50%, one of the NAT instances must have been down. AWS does not automatically recover the failed NAT instances. AWS recommends using NAT gateways because they provide better availability and bandwidth. Even if NAT gateway is deployed on a single AZ, AWS implements redundancy to ensure that it is always available on that AZ.</p><p>The option that says: <strong>One of the subnets in the VPC has a misconfigured Network ACL that blocks outbound traffic to the third-party provider. Update the network ACL to allow this connection and configure IAM permissions to restrict these changes in the future</strong> is incorrect. Network ACLs affect all the subnets associated with it. If there is a misconfigured rule, the other subnets will be affected too, which could result in a 100% failure of requests to the third-party provider.</p><p>The option that says: <strong>The error is caused by a failure in one of their availability zones in the VPC of the third-party provider. Contact the third-party provider support hotline and request for them to fix it </strong>is incorrect. If there is a failure on one availability zone of the third-party provider, the traffic should have stopped sending to that AZ so this failure is most likely caused by a local failure in your VPC.</p><p>The option that says: <strong>This error is caused by an overloaded NAT instance in one of the subnets. Scale the EC2 NAT instances to larger-sized instances to ensure that they can handle the growing traffic</strong> is incorrect. If the NAT instances are overloaded, you will notice inconsistent performance or slowdown for the third-party requests. And this failure should have been gone during off-peak hours. If the failure rate is 50% of the requests, it is most likely that one of the NAT instances is down.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company has several AWS accounts that are managed using AWS Organizations. The company created only one organizational unit (OU) so all child accounts are members of the Production OU. The Solutions Architects control access to certain AWS services using SCPs that define the restricted services. The SCPs are attached at the root of the organization so that they will be applied to all AWS accounts under the organization. The company recently acquired a small business firm and its existing AWS account was invited to join the organization. Upon onboarding, the administrators of the small business firm cannot apply the required AWS Config rules to meet the parent company’s security policies.</p><p>Which of the following options will allow the administrators to update the AWS Config rules on their AWS account without introducing long-term management overhead?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Remove the SCPs on the organization’s root and apply them to the Production OU instead. Create a temporary Onboarding OU that has an attached SCP allowing changes to AWS Config. Add the new account to this temporary OU and make the required changes before moving it to Production OU.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Add the new account to a temporary Onboarding organization unit (OU) that has an attached SCP allowing changes to AWS Config. Perform the needed changes while on this temporary OU before moving the new account to Production OU.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Update the SCPs applied in the root of the AWS organization and remove the rule that restricts changes to the AWS Config service. Deploy a new AWS Service Catalog to the whole organization containing the company’s AWS Config policies.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Instead of using a “deny list” to AWS services on the organization’s root SCPs, use an “allow list” to allow only the required AWS services. Temporarily add the AWS Config service on the “allow list” for the principals of the new account and make the required changes.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Organizations</strong> helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts.</p><p>With AWS Organizations, you can consolidate multiple AWS accounts into an organization that you create and centrally manage. You can create member accounts and invite existing accounts to join your organization. You can organize those accounts into groups and attach policy-based controls.</p><p><strong><img src=\"https://media.tutorialsdojo.com/sap_aws_orgganization_nested.JPG\"></strong></p><p><strong>Service control policies (SCPs)</strong> are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled.</p><p>An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions allowed by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the <code><strong>AdministratorAccess</strong> </code>IAM policy with */* permissions to the user.</p><p>AWS strongly recommends that you don't attach SCPs to the root of your organization without thoroughly testing the impact that the policy has on accounts. Instead, create an OU that you can move your accounts into one at a time, or at least in small numbers, to ensure that you don't inadvertently lock users out of key services.</p><p>Therefore, the correct answer is: <strong>Remove the SCPs on the organization’s root and apply them to the Production OU instead. Create a temporary Onboarding OU that has an attached SCP allowing changes to AWS Config. Add the new account to this temporary OU and make the required changes before moving it to Production OU.</strong> It is not recommended to attach the SCPs to the root of the organization, so it is better to move all the SCPs to the Production OU. This way, the temporary Onboarding OU can have an independent SCP to allow the required changes on AWS Config. Then, you can move the new AWS account to the Production OU.</p><p>The option that says: <strong>Update the SCPs applied in the root of the AWS organization and remove the rule that restricts changes to the AWS Config service. Deploy a new AWS Service Catalog to the whole organization containing the company’s AWS Config policies</strong> is incorrect. Although AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS, this will cause possible problems in the future for the administrators. Removing the AWS Config restriction on the root of the AWS organization's SCP will allow all Admins on all AWS accounts to manage/change/update their own AWS Config rules.</p><p>The option that says: <strong>Add the new account to a temporary Onboarding organization unit (OU) that has an attached SCP allowing changes to AWS Config. Perform the needed changes while on this temporary OU before moving the new account to Production OU</strong> is incorrect. If the SCP applied on the organization's root has a \"deny\" permission, all OUs under the organization will inherit that rule. You cannot override an explicit \"deny\" permission with an explicit \"allow\" applied to the temporary Onboarding OU.</p><p>The option that says: <strong>Instead of using a “deny list” to AWS services on the organization’s root SCPs, use an “allow list” to allow only the required AWS services. Temporarily add the AWS Config service on the “allow list” for the principals of the new account and make the required changes</strong> is incorrect. This is possible, however, it will cause more management problems in the future as you will have to update the \"allow list\" for any service that users may require in the future.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html\">https://docs.aws.amazon.com/controltower/latest/userguide/organizations.html</a></p><p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html</a></p><p><br></p><p><strong>Check out these AWS Organizations and SCP Comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p></div>"
	},
	{
		"question": "<p>A company has launched a company-wide bug bounty program to find and patch up security vulnerabilities in your web applications as well as the underlying cloud resources. As the solutions architect, you are focused on checking system vulnerabilities on AWS resources for DDoS attacks. Due to budget constraints, the company cannot afford to enable AWS Shield Advanced to prevent higher-level attacks.</p><p>Which of the following are the best techniques to help mitigate Distributed Denial of Service (DDoS) attacks for cloud infrastructure hosted in AWS? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use Reserved EC2 instances to ensure that each instance has the maximum performance possible. Use AWS WAF to protect your web applications from common web exploits that could affect application availability.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Add multiple Elastic Network Interfaces to each EC2 instance and use Enhanced Networking to increase the network bandwidth.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use S3 as a POSIX-compliant storage instead of EBS Volumes for storing data. Install the SSM agent to all of your instances and use AWS Systems Manager Patch Manager to automatically patch your instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use an Amazon CloudFront distribution for both static and dynamic content of your web applications. Add CloudWatch alerts to automatically look and notify the Operations team for high <code>CPUUtilization</code> and <code>NetworkIn</code> metrics, as well as to trigger Auto Scaling of your EC2 instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use an Application Load Balancer (ALB) to reduce the risk of overloading your application by distributing traffic across many backend instances. Integrate AWS WAF and the ALB to protect your web applications from common web exploits that could affect application availability.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The following options are the correct answers in this scenario as they can help mitigate the effects of DDoS attacks:</p><p><strong>- Use an Amazon CloudFront distribution for both static and dynamic content of your web applications. Add CloudWatch alerts to automatically look and notify the Operations team for high </strong><code><strong>CPUUtilization</strong></code><strong> and </strong><code><strong>NetworkIn</strong></code><strong> metrics, as well as to trigger Auto Scaling of your EC2 instances</strong>.</p><p><strong>- Use an Application Load Balancer (ALB) to reduce the risk of overloading your application by distributing traffic across many backend instances. Integrate AWS WAF and the ALB to protect your web applications from common web exploits that could affect application availability</strong>.</p><p><strong>Amazon CloudFront</strong> is a content delivery network (CDN) service that can be used to deliver your entire website, including static, dynamic, streaming, and interactive content. Persistent TCP connections and variable time-to-live (TTL) can be used to accelerate delivery of content, even if it cannot be cached at an edge location. This allows you to use Amazon CloudFront to protect your web application, even if you are not serving static content. Amazon CloudFront only accepts well-formed connections to prevent many common DDoS attacks like SYN floods and UDP reflection attacks from reaching your origin.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront.png\"></p><p>Larger DDoS attacks can exceed the size of a single Amazon EC2 instance. To mitigate these attacks, you will want to consider options for load balancing excess traffic. With Elastic Load Balancing (ELB), you can reduce the risk of overloading your application by distributing traffic across many backend instances. ELB can scale automatically, allowing you to manage larger volumes of unanticipated traffic, like flash crowds or DDoS attacks.</p><p>Another way to deal with application layer attacks is to operate at scale. In the case of web applications, you can use ELB to distribute traffic to many Amazon EC2 instances that are overprovisioned or configured to auto scale for the purpose of serving surges of traffic, whether it is the result of a flash crowd or an application layer DDoS attack. Amazon CloudWatch alarms are used to initiate Auto Scaling, which automatically scales the size of your Amazon EC2 fleet in response to events that you define. This protects application availability even when dealing with an unexpected volume of requests.</p><p>The option that says: <strong>Use S3 as a POSIX-compliant storage instead of EBS Volumes for storing data. Install the SSM agent to all of your instances and use AWS Systems Manager Patch Manager to automatically patch your instances</strong> is incorrect because using S3 instead of EBS Volumes is mainly for addressing scalability to your storage requirements and not for avoiding DDoS attacks. In addition, Amazon S3 is not a POSIX-compliant storage.</p><p>The option that says:<strong> Add multiple Elastic Network Interfaces to each EC2 instance and use Enhanced Networking to increase the network bandwidth</strong> is incorrect. Even if you add multiple ENIs and are using Enhanced Networking to increase the network throughput of the instances, the CPU of the instance will be saturated with the DDoS requests which will cause the application to be unresponsive.</p><p>The option that says: <strong>Use Reserved EC2 instances to ensure that each instance has the maximum performance possible. Use AWS WAF to protect your web applications from common web exploits that could affect application availability</strong> is incorrect because using Reserved EC2 instances does not provide any additional computing performance compared to other EC2 types.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html\">https://docs.aws.amazon.com/waf/latest/developerguide/shield-chapter.html</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf\">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Best practices on DDoS Attack Mitigation:</strong></p><p><a href=\"https://youtu.be/HnoZS5jj7pk\">https://youtu.be/HnoZS5jj7pk</a></p></div>"
	},
	{
		"question": "<p>An innovative Business Process Outsourcing (BPO) startup is planning to launch a scalable and cost-effective call center system using AWS. The system should be able to receive inbound calls from thousands of customers and generate user contact flows. Callers must have the capability to perform basic tasks such as changing their password or checking their balance without them having to speak to a call center agent. It should also have advanced deep learning functionalities such as automatic speech recognition (ASR) to achieve highly engaging user experiences and lifelike conversational interactions. A feature that allows the solution to query other business applications and send relevant data back to callers must also be implemented. </p><p>Which of the following is the MOST suitable solution that the Solutions Architect should implement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up a cloud-based contact center using the AWS Elemental MediaConnect service. Create a conversational chatbot using Amazon Polly with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Elemental MediaConnect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a cloud-based contact center using the AWS Ground Station service. Create a conversational chatbot using Amazon Alexa for Business with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Ground Station. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a cloud-based contact center using the Amazon Connect service. Create a conversational chatbot using Amazon Lex with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a cloud-based contact center using the Amazon Direct Connect service. Create a conversational chatbot using Amazon Rekognition with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Direct Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Connect</strong> provides a seamless omnichannel experience through a single unified contact center for voice and chat. Contact center agents and managers don’t have to learn multiple tools, because Amazon Connect has the same contact routing, queuing, analytics, and management tools in a single UI across voice, web chat, and mobile chat.</p><p><strong>Amazon Lex</strong> is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions. With Amazon Lex, the same deep learning technologies that power Amazon Alexa are now available to any developer, enabling you to quickly and easily build sophisticated, natural language, conversational bots (\"chatbots\").</p><p><img src=\"https://media.tutorialsdojo.com/sap_connect_lex.png\"></p><p>Contact flows define the experience your customers have when they interact with your contact center. These are similar in concept to Interactive Voice Response (IVR). Contact flows are comprised of blocks, with each block defining a step or interaction in your contact center. For example, there are blocks to play a prompt, get input from a customer, branch based on customer input, or invoke an AWS Lambda function or an Amazon Lex bot.</p><p>By using an Amazon Lex chatbot in your Amazon Connect call center, callers can perform tasks such as changing a password, requesting a balance on an account, or scheduling an appointment, without needing to speak to an agent. These chatbots use automatic speech recognition and natural language understanding to recognize the intent of the caller. They are able to recognize human speech at an optimal (8 kHz) telephony audio sampling rate, and understand the caller’s intent without requiring the caller to speak in specific phrases. Amazon Lex uses AWS Lambda functions to query your business applications, provide information back to callers, and make updates as requested. Amazon Lex chatbots also maintain context and manage the dialogue, dynamically adjusting responses based on the conversation.</p><p>Hence, the correct answer is: <strong>Set up a cloud-based contact center using the Amazon Connect service. Create a conversational chatbot using Amazon Lex with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions.</strong></p><p>The option that says: <strong>Set up a cloud-based contact center using the AWS Ground Station service. Create a conversational chatbot using Amazon Alexa for Business with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Ground Station. Connect the solution to various business applications and other internal systems using AWS Lambda functions</strong> is incorrect. AWS Ground Station is a fully managed service that lets you control satellite communications, process data, and scale your operations without having to worry about building or managing your own ground station infrastructure. This service is not suitable as a cloud-based contact center. Moreover, Alexa for Business simply empowers companies to use Alexa devices as their intelligent assistant to be more productive in meeting rooms, at their desks, and even with the Alexa devices they already use at home or on the go. You should use a different machine learning service, such as Amazon Lex, to create a conversational chatbot.</p><p>The option that says: <strong>Set up a cloud-based contact center using the AWS Elemental MediaConnect service. Create a conversational chatbot using Amazon Polly with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with AWS Elemental MediaConnect. Connect the solution to various business applications and other internal systems using AWS Lambda functions </strong>is incorrect because AWS Elemental MediaConnect is just a high-quality transport service for live video. You have to use Amazon Connect instead to set up your cloud-based contact center. And although Amazon Polly is a machine learning service, it is quite limited as it just turns text into lifelike speech that allows you to create applications that talk and build entirely new categories of speech-enabled products. A more suitable service to use here is Amazon Lex.</p><p>The option that says: <strong>Set up a cloud-based contact center using the Amazon Direct Connect service. Create a conversational chatbot using Amazon Rekognition with automatic speech recognition and natural language understanding to recognize the intent of the caller then integrate it with Amazon Direct Connect. Connect the solution to various business applications and other internal systems using AWS Lambda functions </strong>is incorrect because Amazon Direct Connect is simply a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS. The most suitable service in setting up a cloud-based contact center is Amazon Connect. Amazon Rekognition only automates your image and video analysis with machine learning. It doesn't have automatic speech recognition (ASR) for converting speech to text nor natural language understanding (NLU). You have to use Amazon Lex instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/connect/\">https://aws.amazon.com/connect/</a></p><p><a href=\"https://aws.amazon.com/lex/\">https://aws.amazon.com/lex/</a></p><p><a href=\"https://aws.amazon.com/blogs/contact-center/easily-set-up-interactive-messages-for-your-amazon-connect-chatbot/\">https://aws.amazon.com/blogs/contact-center/easily-set-up-interactive-messages-for-your-amazon-connect-chatbot/</a></p><p><br></p><p><strong>Check out this Amazon Lex Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-lex/?src=udemy\">https://tutorialsdojo.com/amazon-lex/</a></p></div>"
	},
	{
		"question": "<p>A company is using AWS Organizations to manage their multi-account and multi-region AWS infrastructure. They are currently doing large-scale automation for their key daily processes to save costs. One of these key processes is sharing specified AWS resources, which an organizational account owns, with other AWS accounts of the company using AWS RAM. There is already an existing service which was previously managed by a separate organization account moderator, who also maintained the specific configuration details. </p><p>In this scenario, what could be a simple and effective solution that would allow the service to perform its tasks on the organization accounts on the moderator's behalf?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Configure a service-linked role for AWS RAM and modify the permissions policy to specify what the role can and cannot do. Lastly, modify the trust policy of the role so that other processes can utilize AWS RAM.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Enable cross-account access with AWS Organizations in the Resource Access Manager Console. Mirror the configuration changes that was performed by the account that previously managed this service.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use trusted access by running the <code>enable-sharing-with-aws-organization</code> command in the AWS RAM CLI. Mirror the configuration changes that was performed by the account that previously managed this service.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Attach an IAM role on the service detailing all the allowed actions that it will be able to perform. Install an SSM agent in each of the worker VMs. Use AWS Systems Manager to build automation workflows that involve the daily key processes.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Resource Access Manager</strong> (AWS RAM) enables you to share specified AWS resources that you own with other AWS accounts. To enable trusted access with AWS Organizations:</p><p>From the AWS RAM CLI, use the <code>enable-sharing-with-aws-organizations</code> command.</p><p>Name of the IAM service-linked role that can be created in accounts when trusted access is enabled: <em>AWSResourceAccessManagerServiceRolePolicy</em>.</p><p>You can use <strong><em>trusted access</em></strong> to enable an AWS service that you specify, called the <em>trusted service</em>, to perform tasks in your organization and its accounts on your behalf. This involves granting permissions to the trusted service but does not otherwise affect the permissions for IAM users or roles. When you enable access, the trusted service can create an IAM role called a service-linked role in every account in your organization. That role has a permissions policy that allows the trusted service to do the tasks that are described in that service's documentation. This enables you to specify settings and configuration details that you would like the trusted service to maintain in your organization's accounts on your behalf.</p><p><img src=\"https://media.tutorialsdojo.com/sap_resources_access_manager.jpg\"></p><p>Therefore the correct answer is: <strong>Use trusted access by running the </strong><code><strong>enable-sharing-with-aws-organization</strong></code><strong> command in the AWS RAM CLI. Mirror the configuration changes that was performed by the account that previously managed this service.</strong></p><p>The option that says: <strong>Attach an IAM role on the service detailing all the allowed actions that it will be able to perform. Install an SSM agent in each of the worker VMs. Use AWS Systems Manager to build automation workflows that involve the daily key processes</strong> is incorrect because this is not the simplest way to automate the interaction of AWS RAM with AWS Organizations. AWS Systems Manager is a tool that helps with the automation of EC2 instances, on-premises servers, and other virtual machines. It might not support all the services being used by the key processes.</p><p>The option that says: <strong>Configure a service-linked role for AWS RAM and modify the permissions policy to specify what the role can and cannot do. Lastly, modify the trust policy of the role so that other processes can utilize AWS RAM</strong> is incorrect. This is not the simplest solution for integrating AWS RAM and AWS Organizations since using AWS Organization's trusted access will create the service-linked role for you. Also, the trust policy of a service-linked role cannot be modified. Only the linked AWS service can assume a service-linked role, which is why you cannot modify the trust policy of a service-linked role.</p><p>The option that says: <strong>Enable cross-account access with AWS Organizations in the Resources Access Manager Console. Mirror the configuration changes that was performed by the account that previously managed this service</strong> is incorrect because you should enable trusted access to AWS RAM, not cross-account access.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_services.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_integrate_services.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html\">https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-ram.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/introducing-an-easier-way-to-delegate-permissions-to-aws-services-service-linked-roles/\">https://aws.amazon.com/blogs/security/introducing-an-easier-way-to-delegate-permissions-to-aws-services-service-linked-roles/</a></p><p><br></p><p><strong>Check out this AWS Resource Access Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-resource-access-manager/?src=udemy\">https://tutorialsdojo.com/aws-resource-access-manager/</a></p></div>"
	},
	{
		"question": "<p>A company provides big data services to enterprise clients around the globe. One of the clients has 60 TB of raw data from their on-premises Oracle data warehouse. The data is to be migrated to Amazon Redshift. However, the database receives minor updates on a daily basis while major updates are scheduled every end of the month. The migration process must be completed within approximately 30 days before the next major update on the Redshift database. The company can only allocate 50 Mbps of Internet connection for this activity to avoid impacting business operations.</p><p>Which of the following actions will satisfy the migration requirements of the company while keeping the costs low?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an AWS Snowball Edge job using the AWS Snowball console. Export all data from the Oracle data warehouse to the Snowball Edge device. Once the Snowball device is returned to Amazon and data is imported to an S3 bucket, create an Oracle RDS instance to import the data. Create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Copy the missing daily updates from Oracle in the data center to the RDS for Oracle database over the Internet. Monitor and verify if the data migration is complete before the cut-over.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an AWS Snowball import job to request for a Snowball Edge device. Use the AWS Schema Conversion Tool (SCT) to process the on-premises data warehouse and load it to the Snowball Edge device. Install the extraction agent on a separate on-premises server and register it with AWS SCT. Once the Snowball Edge imports data to the S3 bucket, use AWS SCT to migrate the data to Amazon Redshift. Configure a local task and AWS DMS task to replicate the ongoing updates to the data warehouse. Monitor and verify that the data migration is complete.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Since you have a 30-day window for migration, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Launch an Oracle Real Application Clusters (RAC) database on an EC2 instance and set it up to fetch and synchronize the data from the on-premises Oracle database. Once replication is complete, create an AWS DMS task on an AWS SCT project to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new Oracle Database on Amazon RDS. Configure Site-to-Site VPN connection from the on-premises data center to the Amazon VPC. Configure replication from the on-premises database to Amazon RDS. Once replication is complete, create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use an <strong>AWS SCT</strong> agent to extract data from your on-premises data warehouse and migrate it to Amazon Redshift. The agent extracts your data and uploads the data to either Amazon S3 or, for large-scale migrations, an AWS Snowball Edge device. You can then use AWS SCT to copy the data to Amazon Redshift.</p><p>Large-scale data migrations can include many terabytes of information and can be slowed by network performance and by the sheer amount of data that has to be moved. AWS Snowball Edge is an AWS service you can use to transfer data to the cloud at faster-than-network speeds using an AWS-owned appliance. An AWS Snowball Edge device can hold up to 100 TB of data. It uses 256-bit encryption and an industry-standard Trusted Platform Module (TPM) to ensure both security and full chain-of-custody for your data. AWS SCT works with AWS Snowball Edge devices.</p><p>When you use AWS SCT and an AWS Snowball Edge device, you migrate your data in two stages. First, you use the AWS SCT to process the data locally and then move that data to the AWS Snowball Edge device. You then send the device to AWS using the AWS Snowball Edge process, and then AWS automatically loads the data into an Amazon S3 bucket. Next, when the data is available on Amazon S3, you use AWS SCT to migrate the data to Amazon Redshift. Data extraction agents can work in the background while AWS SCT is closed. You manage your extraction agents by using AWS SCT. The extraction agents act as listeners. When they receive instructions from AWS SCT, they extract data from your data warehouse.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_sct.jpg\"></p><p>Therefore, the correct answer is: <strong>Create an AWS Snowball import job to request for a Snowball Edge device. Use the AWS Schema Conversion Tool (SCT) to process the on-premises data warehouse and load it to the Snowball Edge device. Install the extraction agent on a separate on-premises server and register it with AWS SCT. Once the Snowball Edge imports data to the S3 bucket, use AWS SCT to migrate the data to Amazon Redshift. Configure a local task and AWS DMS task to replicate the ongoing updates to the data warehouse. Monitor and verify that the data migration is complete.</strong></p><p>The option that says: <strong>Create a new Oracle Database on Amazon RDS. Configure Site-to-Site VPN connection from the on-premises data center to the Amazon VPC. Configure replication from the on-premises database to Amazon RDS. Once replication is complete, create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over</strong> is incorrect. Replicating 60 TB worth of data over the public Internet will take several days over the 30-day migration window. It is also stated in the scenario that the company can only allocate 50 Mbps of Internet connection for the migration activity. Sending the data over the Internet could potentially affect business operations.</p><p>The option that says: <strong>Create an AWS Snowball Edge job using the AWS Snowball console. Export all data from the Oracle data warehouse to the Snowball Edge device. Once the Snowball device is returned to Amazon and data is imported to an S3 bucket, create an Oracle RDS instance to import the data. Create an AWS Schema Conversion Tool (SCT) project with AWS DMS task to migrate the Oracle database to Amazon Redshift. Copy the missing daily updates from Oracle in the data center to the RDS for Oracle database over the internet. Monitor and verify if the data migration is complete before the cut-over</strong> is incorrect. You need to configure the data extraction agent first on your on-premises server. In addition, you don't need the data to be imported and exported via Amazon RDS. AWS DMS can directly migrate the data to Amazon Redshift.</p><p>The option that says: <strong>Since you have a 30-day window for migration, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Install Oracle database on an EC2 instance that is configured to synchronize with the on-premises Oracle database. Once replication is complete, create an AWS DMS task on an AWS SCT project to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over Since you have a 30-day window for migration, configure VPN connectivity between AWS and the company's data center by provisioning a 1 Gbps AWS Direct Connect connection. Launch an Oracle Real Application Clusters (RAC) database on an EC2 instance and set it up to fetch and synchronize the data from the on-premises Oracle database. Once replication is complete, create an AWS DMS task on an AWS SCT project to migrate the Oracle database to Amazon Redshift. Monitor and verify if the data migration is complete before the cut-over</strong> is incorrect. Although this is possible, the company wants to keep the cost low. Using a Direct Connect connection for a one-time migration is not a cost-effective solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/\">https://aws.amazon.com/getting-started/hands-on/migrate-oracle-to-amazon-redshift/</a></p><p><a href=\"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html\">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.dw.html</a></p><p><a href=\"https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.html\">https://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/agents.html</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company runs a Flight Deals web application which is currently hosted on their on-premises data center. The website hosts high-resolution photos of top tourist destinations in the world and uses a third-party payment platform to accept payments. Recently, the company heavily invested in their global marketing campaign and there is a high probability that the incoming traffic to their Flight Deals website will increase in the coming days. Due to a tight deadline, the company does not have the time to fully migrate the website to the AWS cloud. A set of security rules that block common attack patterns, such as SQL injection and cross-site scripting should also be implemented to improve website security.</p><p>Which of the following options will maintain the website's functionality despite the massive amount of incoming traffic?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use CloudFront to cache and distribute the high resolution images and other static assets of the website. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Generate an AMI based on the existing Flight Deals website. Launch the AMI to a fleet of EC2 instances with Auto Scaling group enabled, for it to automatically scale up or scale down based on the incoming traffic. Place these EC2 instances behind an ALB which can balance traffic between the web servers in the on-premises data center and the web servers hosted in AWS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create and configure an S3 bucket as a static website hosting. Move the web domain of the website from your on-premises data center to Route 53 then route the newly created S3 bucket as the origin. Enable Amazon S3 server-side encryption with AWS Key Management Service managed keys.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Server Migration Service to easily migrate the website from your on-premises data center to your VPC. Create an Auto Scaling group to automatically scale the web tier based on the incoming traffic. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudFront</strong> is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define. You can get started quickly using Managed Rules for AWS WAF, a pre-configured set of rules managed by AWS or AWS Marketplace Sellers. The Managed Rules for WAF address issues like the OWASP Top 10 security risks. These rules are regularly updated as new issues emerge. AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of security rules.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_waf.png\"></p><p>AWS WAF is easy to deploy and protect applications deployed on either Amazon CloudFront as part of your CDN solution, the Application Load Balancer that fronts all your origin servers, or Amazon API Gateway for your APIs. There is no additional software to deploy, DNS configuration, SSL/TLS certificate to manage, or need for a reverse proxy setup. With AWS Firewall Manager integration, you can centrally define and manage your rules, and reuse them across all the web applications that you need to protect.</p><p>Hence, the option that says: <strong>Use CloudFront to cache and distribute the high resolution images and other static assets of the website. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks</strong> is correct because CloudFront will provide the scalability the website needs without doing major infrastructure changes. Take note that the website has a lot of high-resolution images which can easily be cached using CloudFront to alleviate the massive incoming traffic going to the on-premises web server and also provide a faster page load time for the web visitors.</p><p>The option that says: <strong>Use the AWS Server Migration Service to easily migrate the website from your on-premises data center to your VPC. Create an Auto Scaling group to automatically scale the web tier based on the incoming traffic. Deploy AWS WAF on the Amazon CloudFront distribution to protect the website from common web attacks</strong> is incorrect as migrating to AWS would be time-consuming compared with simply using CloudFront. Although this option can provide a more scalable solution, the scenario says that the company does not have ample time to do the migration.</p><p>The option that says:<strong><em> </em>Create and configure an S3 bucket as a static website hosting. Move the web domain of the website from your on-premises data center to Route53 then route the newly created S3 bucket as the origin. Enable Amazon S3 server-side encryption with AWS Key Management Service managed keys</strong> is incorrect because the website is a dynamic website that accepts payments and bookings. Migrating your web domain to Route 53 may also take some time.</p><p>The option that says: <strong>Generate an AMI based on the existing Flight Deals website. Launch the AMI to a fleet of EC2 instances with Auto Scaling group enabled, for it to automatically scale up or scale down based on the incoming traffic. Place these EC2 instances behind an ALB which can balance traffic between the web servers in the on-premises data center and the web servers hosted in AWS</strong> is incorrect because it didn't mention any existing AWS Direct Connect or VPN connection. Although an Application Load Balancer can load balance the traffic between the EC2 instances in AWS Cloud and web servers located in the on-premises data center, your systems should be connected via Direct Connect or VPN connection first. In addition, the application seems to be used around the world because the company launched a global marketing campaign. Hence, CloudFront is a more suitable option for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html\">https://docs.aws.amazon.com/waf/latest/developerguide/cloudfront-features.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A media company has a suite of internet-facing web applications hosted in US West (N. California) region in AWS. The architecture is composed of several On-Demand Amazon EC2 instances behind an Application Load Balancer, which is configured to use public SSL/TLS certificates. The Application Load Balancer also enables incoming HTTPS traffic through the fully qualified domain names (FQDNs) of the applications for SSL termination. A Solutions Architect has been instructed to upgrade the corporate web applications to a multi-region architecture that uses various AWS Regions such as ap-southeast-2, ca-central-1, eu-west-3, and so forth. </p><p>Which of the following approach should the Architect implement to ensure that all HTTPS services will continue to work without interruption?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>In each new AWS Region, request for SSL/TLS certificates using AWS KMS for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region.</p>"
			},
			{
				"correct": true,
				"answer": "<p>In each new AWS Region, request for SSL/TLS certificates using the AWS Certificate Manager for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Certificate Manager service in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS KMS in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Certificate Manager</strong> is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.</p><p>With AWS Certificate Manager, you can quickly request a certificate, deploy it on ACM-integrated AWS resources, such as Elastic Load Balancers, Amazon CloudFront distributions, and APIs on API Gateway, and let AWS Certificate Manager handle certificate renewals. It also enables you to create private certificates for your internal resources and manage the certificate lifecycle centrally. Public and private certificates provisioned through AWS Certificate Manager for use with ACM-integrated services are free. You pay only for the AWS resources you create to run your application. With AWS Certificate Manager Private Certificate Authority, you pay monthly for the operation of the private CA and for the private certificates you issue.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_https_acm.png\"></p><p>You can use the same SSL certificate from ACM in more than one AWS Region but it depends on whether you’re using Elastic Load Balancing or Amazon CloudFront. To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it. To use an ACM certificate with Amazon CloudFront, you must request the certificate in the US East (N. Virginia) region. ACM certificates in this region that are associated with a CloudFront distribution are distributed to all the geographic locations configured for that distribution.</p><p>Hence, the correct answer is the option that says: <strong>In each new AWS Region, request for SSL/TLS certificates using the AWS Certificate Manager for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region.</strong></p><p>The option that says: <strong>In each new AWS Region, request for SSL/TLS certificates using AWS KMS for each FQDN. Associate the new certificates to the corresponding Application Load Balancer of the same AWS Region</strong> is incorrect because AWS KMS is not the right service to use to generate the SSL/TLS certificates. You have to utilize ACM instead.</p><p>The option that says: <strong>Use the AWS KMS in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add </strong>is incorrect. You have to use the AWS Certificate Manager (ACM) service to generate the certificates and not AWS KMS as this service is primarily used for data encryption. Moreover, you have to associate the certificates that were generated from the same AWS Region where the load balancer is launched.</p><p>The option that says: <strong>Use the AWS Certificate Manager service in the US West (N. California) region to request for SSL/TLS certificates for each FQDN which will be used to all regions. Associate the new certificates to the new Application Load Balancer on each new AWS Region that the Architect will add<em> </em></strong>is incorrect. You can only use the same SSL certificate from ACM in more than one AWS Region if you are attaching it to your CloudFront distribution only, and not to your Application Load Balancer. To use a certificate with Elastic Load Balancing for the same site (the same fully qualified domain name, or FQDN, or set of FQDNs) in a different Region, you must request a new certificate for each Region in which you plan to use it.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/certificate-manager/faqs/\">https://aws.amazon.com/certificate-manager/faqs/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/\">https://aws.amazon.com/premiumsupport/knowledge-center/associate-acm-certificate-alb-nlb/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-add-or-delete-listeners.html#add-listener-console\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-add-or-delete-listeners.html#add-listener-console</a></p><p><br></p><p><strong>Check out these AWS Certificate Manager and Elastic Load Balancer Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certificate-manager/?src=udemy\">https://tutorialsdojo.com/aws-certificate-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p></div>"
	},
	{
		"question": "<p>A clinic runs its medical record system using a fleet of Windows-based Amazon EC2 instances with several EBS volumes attached to it. Since the records that they are storing are confidential health files of their patients, it is a requirement that the latest security patches are installed on the EC2 instances. In addition, there should be a system in the cloud architecture that checks all of the EC2 instances if they are using an approved Amazon Machine Image (AMI). The system that will be implemented should not impede developers from launching instances using an unapproved AMI, but you still have to be notified if there are non-compliant EC2 instances in your VPC.</p><p>Which of the following should the solutions architect implement to protect and monitor all of your instances as required above? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use AWS Shield Advanced to automatically patch all of your EC2 instances and detect uncompliant EC2 instances which do not use approved AMIs.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a patch baseline that defines which patches are approved for installation on your instances using AWS Systems Manager Patch Manager.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up Amazon GuardDuty that continuously monitors your instances if the latest security patches are installed and if there is an instance that is using an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Set up CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\"></p><p>Patch Manager uses <strong>patch baselines</strong>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>AWS Config provides <strong>AWS managed rules</strong>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources. You can set up and activate these rules without writing the code to create an AWS Lambda function, which is required if you want to create custom rules. The AWS Config console guides you through the process of configuring and activating a managed rule. You can also use the AWS Command Line Interface or AWS Config API to pass the JSON code that defines your configuration of a managed rule.</p><p>In this scenario, you can use a combination of AWS Config Managed Rules and AWS Systems Manager Patch Manager to meet the requirements.</p><p>Therefore, the correct answers are:</p><p><strong>- Set up a patch baseline that defines which patches are approved for installation on your instances using AWS Systems Manager Patch Manager.</strong></p><p><strong>- Use the AWS Config Managed Rule which automatically checks whether your running EC2 instances are using approved AMIs. Set up CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong>.</p><p>The option that says: <strong>Creating an IAM policy that will restrict the developers from launching EC2 instances with an unapproved AMI </strong>is incorrect. Although you can use an IAM policy to prohibit your developers from launching unapproved AMIs, this will impede their work which violates what the scenario requires. Remember, as per the scenario, the system that you will implement should not impede developers from launching instances using an unapproved AMI.</p><p>The option that says: <strong>Set up Amazon GuardDuty that continuously monitors your instances if the latest security patches are installed and if there is an instance that is using an unapproved AMI. Use CloudWatch Alarms to notify you if there are any non-compliant instances running in your VPC</strong> is incorrect. Amazon GuardDuty is primarily used as a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise however, it does not check if your EC2 instances are using an approved AMI or not.</p><p><strong>Using AWS Shield Advanced to automatically patch all of your EC2 instances and detecting uncompliant EC2 instances which do not use approved AMIs </strong>is incorrect. The AWS Shield Advanced service is most suitable to prevent DDoS attacks in your AWS resources. It cannot check the specific AMIs that your EC2 instances are using.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Config and AWS Systems Manager:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A company has several development teams using AWS CodeCommit to store their source code. With the number of code updates every day, the management is having difficulty tracking if the developers are adhering to company security policies. On a recent audit, the security team found several IAM access keys and secret keys in the CodeCommit repository. This is a big security risk so the company wants to have an automated solution that will scan the CodeCommit repositories for committed IAM credentials and delete/disable the IAM keys for those users.</p><p>Which of the following options will meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Download and scan the source code from AWS CodeCommit using a custom AWS Lambda function. Schedule this Lambda function to run daily. If credentials are found, notify the user of the violation, generate new IAM credentials and store them in AWS KMS for encryption.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Scan the CodeCommit repositories for IAM credentials using Amazon Macie. Using machine learning, Amazon Macie can scan your repository for security violations. If violations are found, invoke an AWS Lambda function to notify the user and delete the IAM keys.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Using a development instance, use the AWS Systems Manager Run Command to scan the AWS CodeCommit repository for IAM credentials on a daily basis. If credentials are found, rotate them using AWS Secrets Manager. Notify the user of the violation.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Write a custom AWS Lambda function to search for credentials on new code submissions. Set the function trigger as AWS CodeCommit push events. If credentials are found, notify the user of the violation, and disable the IAM keys.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CodeCommit</strong> is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud.</p><p>You can configure a CodeCommit repository so that code pushes or other events trigger actions, such as sending a notification from Amazon Simple Notification Service (Amazon SNS) or invoking a function in AWS Lambda. You can create up to 10 triggers for each CodeCommit repository.</p><p>Triggers are commonly configured to:</p><p>- Send emails to subscribed users every time someone pushes to the repository.</p><p>- Notify an external build system to start a build after someone pushes to the main branch of the repository.</p><p>Scenarios like notifying an external build system require writing a Lambda function to interact with other applications. The email scenario simply requires creating an Amazon SNS topic. You can create a trigger for a CodeCommit repository so that events in that repository trigger notifications from an Amazon Simple Notification Service (Amazon SNS) topic.</p><p>You can also create an AWS Lambda trigger for a CodeCommit repository so that events in the repository invoke a Lambda function. For example, you can create a Lambda function that will scan the CodeCommit code submissions for IAM credentials, and then send out notifications or perform corrective actions.</p><p>When you use the Lambda console to create the function, you can create a CodeCommit trigger for the Lambda function. Here is an example of the trigger for all push events:</p><p><img src=\"https://docs.aws.amazon.com/codecommit/latest/userguide/images/codecommit-lambda-trigger.png\"></p><p>Therefore, the correct answer is: <strong>Write a custom AWS Lambda function to search for credentials on new code submissions. Set the function trigger as AWS CodeCommit push events. If credentials are found, notify the user of the violation, and disable the IAM keys.</strong></p><p>The option that says: <strong>Using a development instance, use the AWS Systems Manager Run Command to scan the AWS CodeCommit repository for IAM credentials on a daily basis. If credentials are found, rotate them using AWS Secrets Manager. Notify the user of the violation</strong> is incorrect. You cannot rotate IAM keys on AWS Secrets Manager. Using the Run Command on a development instance just for scanning the repository is costly. It is cheaper to just write your own Lambda function to do the scanning.</p><p>The option that says: <strong>Download and scan the source code from AWS CodeCommit using a custom AWS Lambda function. Schedule this Lambda function to run daily. If credentials are found, notify the user of the violation, generate new IAM credentials and store them in AWS KMS for encryption</strong> is incorrect. You store encryption keys on AWS KMS, not IAM keys.</p><p>The option that says: <strong>Scan the CodeCommit repositories for IAM credentials using Amazon Macie. Using machine learning, Amazon Macie can scan your repository for security violations. If violations are found, invoke an AWS Lambda function to notify the user and delete the IAM keys</strong> is incorrect. Amazon Macie is designed to use machine learning and pattern matching to discover and protect your sensitive data in AWS. Macie primarily scans Amazon S3 buckets for data security and data privacy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-lambda.html</a></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-sns.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify-sns.html</a></p><p><a href=\"https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html\">https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html</a></p><p><br></p><p><strong>Check out the AWS CodeCommit Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codecommit/?src=udemy\">https://tutorialsdojo.com/aws-codecommit/</a></p></div>"
	},
	{
		"question": "<p>A company is hosting a multi-tier web application in AWS. It is composed of an Application Load Balancer and EC2 instances across three Availability Zones. During peak load, its stateless web servers operate at 95% utilization. The system is set up to use Reserved Instances to handle the steady-state load and On-Demand Instances to handle the peak load. Your manager instructed you to review the current architecture and do the necessary changes to improve the system.</p><p>Which of the following provides the most cost-effective architecture to allow the application to recover quickly in the event that an Availability Zone is unavailable during peak load?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Launch a Spot Fleet using a diversified allocation strategy, with Auto Scaling enabled on each AZ to handle the peak load instead of On-Demand instances. Retain the current setup for handling the steady state load.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an Auto Scaling group of Reserved instances on each AZ to handle the peak load. Retain the current setup for handling the steady state load.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a combination of Spot and On-Demand instances on each AZ to handle both the steady state and peak load.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a combination of Reserved and On-Demand instances on each AZ to handle both the steady state and peak load.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The scenario requires a cost-effective architecture to allow the application to recover quickly, hence, using an <strong>Auto Scaling group</strong> is a must to handle the peak load and improve both the availability and scalability of the application.</p><p>Setting up a diversified allocation strategy for your<strong> Spot Fleet</strong> is a best practice to increase the chances that a spot request can be fulfilled by EC2 capacity in the event of an outage in one of the Availability Zones. You can include each AZ available to you in the launch specification. And instead of using the same subnet each time, use three unique subnets (each mapping to a different AZ).</p><p><img src=\"https://media.tutorialsdojo.com/sap_asg_spot_ondemand.png\"></p><p>Therefore the correct answer is: <strong>Launch a Spot Fleet using a diversified allocation strategy, with Auto Scaling enabled on each AZ to handle the peak load instead of On-Demand instances. Retain the current setup for handling the steady state load. </strong>The Spot instances are the most cost-effective for handling the temporary peak loads of the application.</p><p>The option that says: <strong>Launch an Auto Scaling group of Reserved instances on each AZ to handle the peak load. Retain the current setup for handling the steady state load</strong> is incorrect. Even though it uses Auto Scaling, Reserved Instances cost more than Spot instances so it is more suitable to use the latter to handle the peak load.</p><p>The following options are incorrect because they did not mention the use of Auto Scaling Groups, which is a requirement for this architecture:</p><p><strong>- Use a combination of Spot and On-Demand instances on each AZ to handle both the steady state and peak load.</strong></p><p><strong>- Use a combination of Reserved and On-Demand instances on each AZ to handle both the steady state and peak load.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-examples.html#fleet-config5\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet-examples.html#fleet-config5</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11</a></p><p><a href=\"https://github.com/awsdocs/amazon-ec2-user-guide/pull/56\">https://github.com/awsdocs/amazon-ec2-user-guide/pull/56</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p></div>"
	},
	{
		"question": "<p>A government agency has multiple VPCs in various AWS regions across the United States that need to be linked up to an on-premises central office network in Washington, D.C. The central office requires inter-region VPC access over a private network that is dedicated to each region for enhanced security and more predictable data transfer performance. Your team is tasked to quickly build this network mesh and to minimize the management overhead to maintain these connections. </p><p>Which of the following options is the most secure, highly available, and durable solution that you should use to set up this kind of interconnectivity?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Implement a hub-and-spoke network topology in each region that routes all traffic through a network transit center using AWS Transit Gateway. Route traffic between VPCs and the on-premise network over AWS Site-to-Site VPN.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Utilize AWS Direct Connect Gateway for inter-region VPC access. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a link aggregation group (LAG) in the central office network to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Use AWS Direct Connect Gateway to achieve inter-region VPC access to all of your AWS resources. Create a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Enable inter-region VPC peering which allows peering relationships to be established between VPCs across different AWS regions. This will ensure that the traffic will always stay on the global AWS backbone and will never traverse the public Internet.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Direct Connect</strong> is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS to achieve higher privacy benefits, additional data transfer bandwidth, and more predictable data transfer performance. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p><p>Using industry standard 802.1q VLANs, this dedicated connection can be partitioned into multiple virtual interfaces. Virtual interfaces can be reconfigured at any time to meet your changing needs. You can use an <strong>AWS Direct Connect gateway </strong>to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different Regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC. Then, create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway.</p><p>With <strong>Direct Connect Gateway</strong>, you no longer need to establish multiple BGP sessions for each VPC; this reduces your administrative workload as well as the load on your network devices.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dc_private_vif.png\"></p><p>Therefore, the correct answer is: <strong>Utilize AWS Direct Connect Gateway for inter-region VPC access. Create a virtual private gateway in each VPC, then create a private virtual interface for each AWS Direct Connect connection to the Direct Connect gateway.</strong></p><p>The option that says: <strong>Create a link aggregation group (LAG) in the central office network to aggregate multiple connections at a single AWS Direct Connect endpoint in order to treat them as a single, managed connection. Use AWS Direct Connect Gateway to achieve inter-region VPC access to all of your AWS resources. Create a virtual private gateway in each VPC and then create a public virtual interface for each AWS Direct Connect connection to the Direct Connect Gateway</strong> is incorrect. You only need to create <strong>private</strong> virtual interfaces to the Direct Connect gateway since you are only connecting to resources inside a VPC. Using a link aggregation group (LAG) is also irrelevant in this scenario because it is just a logical interface that uses the Link Aggregation Control Protocol (LACP) to aggregate multiple connections at a single AWS Direct Connect endpoint, allowing you to treat them as a single, managed connection.</p><p>The option that says:<strong> Implement a hub-and-spoke network topology in each region that routes all traffic through a network transit center using AWS Transit Gateway. Route traffic between VPCs and the on-premise network over AWS Site-to-Site VPN</strong> is incorrect since the scenario requires a service that can provide a dedicated network between the VPCs and the on-premises network, as well as enhanced privacy and predictable data transfer performance. Simply using AWS Transit Gateway will not fulfill the conditions above. This option is best suited for customers who want to leverage AWS-provided, automated high availability network connectivity features and also optimize their investments in third-party product licensing such as VPN software.</p><p>The option that says: <strong>Enable inter-region VPC peering which allows peering relationships to be established between VPCs across different AWS regions. This will ensure that the traffic will always stay on the global AWS backbone and will never traverse the public internet</strong> is incorrect. This solution would require a lot of manual setup and management overhead to successfully build a functional, error-free inter-region VPC network compared with just using a Direct Connect Gateway. Although the Inter-Region VPC Peering provides a cost-effective way to share resources between regions or replicate data for geographic redundancy, its connections are not dedicated and highly available.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/\">https://aws.amazon.com/blogs/aws/new-aws-direct-connect-gateway-inter-region-vpc-access/</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html</a></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/\">https://aws.amazon.com/answers/networking/aws-multiple-region-multi-vpc-connectivity/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p></div>"
	},
	{
		"question": "<p>A multinational investment bank has a hybrid cloud architecture that uses a single 1 Gbps AWS Direct Connect connection to integrate their on-premises network to AWS Cloud. The bank has a total of 10 VPCs which are all connected to their on-premises data center via the same Direct Connect connection that you manage. Based on the recent IT audit, the existing network setup has a single point of failure which needs to be addressed immediately.</p><p>Which of the following is the MOST cost-effective solution that you should implement in order to improve the connection redundancy of your hybrid network?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Establish VPN tunnels from your on-premises data center to each of the 10 VPCs. Terminate each VPN tunnel connection at the virtual private gateway (VGW) of the respective VPC. Configure BGP for route management.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Establish another 1 Gbps AWS Direct Connect connection with corresponding private Virtual Interfaces (VIFs) to connect all of the 10 VPCs individually. Set up a Border Gateway Protocol (BGP) peering session for all of the VIFs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Establish another 1 Gbps AWS Direct Connect connection using a public Virtual Interface (VIF). Prepare a VPN tunnel that will terminate on the virtual private gateway (VGW) of the respective VPC using the public VIF. Handle the failover to the VPN connection through the use of BGP.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Establish a new point-to-point Multiprotocol Label Switching (MPLS) connection to all of your 10 VPCs. Configure BGP to use this new connection with an active/passive routing.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With AWS Direct Connect plus VPN, you can combine one or more AWS Direct Connect dedicated network connections with the Amazon VPC VPN. This combination provides an IPsec-encrypted private connection that also reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than Internet-based VPN connections.</p><p>You can use AWS Direct Connect to establish a dedicated network connection between your network and create a logical connection to public AWS resources, such as an Amazon virtual private gateway IPsec endpoint. This solution combines the AWS managed benefits of the VPN solution with low latency, increased bandwidth, more consistent benefits of the AWS Direct Connect solution, and an end-to-end, secure IPsec connection.</p><p><img src=\"https://media.tutorialsdojo.com/aws_direct_conneect_plus_vpn.png\"></p><p>It costs a lot of money to establish a Direct Connect connection which you rarely use. For a more cost-effective solution, you can configure a backup VPN connection for failover with your AWS Direct Connect connection.</p><p>If you want a short-term or lower-cost solution, you might consider configuring a hardware VPN as a failover option for a Direct Connect connection. VPN connections are not designed to provide the same level of bandwidth available to most Direct Connect connections. Ensure that your use case or application can tolerate a lower bandwidth if you are configuring a VPN as a backup to a Direct Connect connection.</p><p>Hence, the correct answer is: <strong>Establish VPN tunnels from your on-premises data center to each of the 10 VPCs. Terminate each VPN tunnel connection at the virtual private gateway (VGW) of the respective VPC. Configure BGP for route management.</strong></p><p>The following options are incorrect:</p><p><strong>- Establish another 1 Gbps AWS Direct Connect connection using a public Virtual Interface (VIF). Prepare a VPN tunnel that will terminate on the virtual private gateway (VGW) of the respective VPC using the public VIF. Handle the failover to the VPN connection through the use of BGP.</strong></p><p><strong>- Establish another 1 Gbps AWS Direct Connect connection with corresponding private Virtual Interfaces (VIFs) to connect all of the 10 VPCs individually. Set up a Border Gateway Protocol (BGP) peering session for all of the VIFs.</strong></p><p>Establishing yet another 1 Gbps AWS Direct Connect connection is not a cost-effective solution. It is better to establish a VPN connection instead as a backup.</p><p>The option that says: <strong>Establishing a new point-to-point Multiprotocol Label Switching (MPLS) connection to all of your 10 VPCs and Configuring BGP to use this new connection with an active/passive routing</strong> is incorrect because you can't directly connect to your Multiprotocol Label Switching (MPLS) to AWS. To integrate your MPLS infrastructure, you need to set up a colocation with Direct Connect by placing the CGW in the same physical facility as the Direct Connect location, which will facilitate a local cross-connect between the CGW and AWS devices.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-plus-vpn-network-to-amazon.html</a></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-network-connectivity-over-mpls/\">https://aws.amazon.com/answers/networking/aws-network-connectivity-over-mpls/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p></div>"
	},
	{
		"question": "<p>A finance company hosts an online banking portal in AWS. The application is recently deployed in an Auto Scaling group of Amazon EC2 instances inside a VPC. However, after several days, the solutions architect found out that there is an unusually high amount of inbound HTTP traffic coming from a set of 15 specific IP addresses from a certain country where your company has absolutely no customers. The EC2 instances are flooded with incoming requests that the system administrators cannot even establish an SSH connection to the instances.</p><p>What should the Solutions Architect do to address this security vulnerability in the MOST cost-effective way?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Set up deny rules on your inbound Network Access control list associated with the web application tier subnet to block access to the group of attacking IP addresses. "
			},
			{
				"correct": false,
				"answer": "Use AWS Shield Advanced to protect your VPC from common, most frequently occurring network and transport layer DDoS attacks."
			},
			{
				"correct": false,
				"answer": "Create inbound rules in the Security Group of your EC2 instances to block the attacking IP addresses."
			},
			{
				"correct": false,
				"answer": "Use AWS WAF to protect your VPC against web attacks. Place the online banking portal behind a CloudFront RTMP distribution."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>A network access control list (ACL)</strong> is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p><p>When you create NACLs, you can specify both <em>allow and deny</em> rules. This is useful if you want to explicitly deny certain types of traffic to your application. For example, you can define IP addresses (as CIDR ranges), protocols, and destination ports that are denied access to the entire subnet. If your application is used only for TCP traffic, you can create a rule to <em>deny</em> all UDP traffic, or vice versa. This option is useful when responding to DDoS attacks because it lets you create your own rules to mitigate the attack when you know the source IPs or other signature.</p><p><img src=\"https://media.tutorialsdojo.com/sap_security_nacl.png\"></p><p>Therefore, the correct answer is: <strong>Setting up deny rules on your inbound Network Access control list associated with the web application tier subnet to block access to the group of attacking IP addresses.</strong> You can add deny rules in Network ACL and block access to certain IP addresses.</p><p>The option that says: <strong>Using AWS WAF to protect your VPC against web attacks and placing the online banking portal behind a CloudFront RTMP distribution</strong> is incorrect. Although you can use AWS WAF to protect your application from common web attacks, placing the online banking portal behind a CloudFront RTMP distribution is wrong because this is primarily used for real-time streaming and not for web applications. You should place it on a CloudFront Web distribution instead.</p><p>The option that says: <strong>Using AWS Shield Advanced to protect your VPC from common, most frequently occurring network and transport layer DDoS attacks</strong> is incorrect. Although you can definitely use AWS Shield Advanced in protecting your cloud infrastructure, it is not cost-effective as compared to blocking the IP addresses using Network ACL. Take note that there are two types of AWS Shield: the Standard one which is free and the Advanced type which has an additional cost of around $3,000 per month.</p><p>The option that says: <strong>Creating inbound rules in the Security Group of your EC2 instances to block the attacking IP addresses</strong> is incorrect. You cannot deny port access as well as specific IP addresses using security groups. By default, all requests are denied in your security groups so you are required to explicity allow access from a particular IP address, port or range to access your EC2 instances using your security groups.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p><p><a href=\"https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/wat.concept.network-acl.en.html\">https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/wat.concept.network-acl.en.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company has several IoT enabled devices and sells them to customers around the globe. Every 5 minutes, each IoT device sends back a data file that includes the device status and other information to an Amazon S3 bucket. Every midnight, a Python cron job runs from an Amazon EC2 instance to read and process each data file on the S3 bucket and loads the values on a designated Amazon RDS database. The cron job takes about 10 minutes to process a day’s worth of data. After each data file is processed, it is eventually deleted from the S3 bucket. The company wants to expedite the process and access the processed data on the Amazon RDS as soon as possible.</p><p>Which of the following actions would you implement to achieve this requirement with the LEAST amount of effort?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Convert the Python script cron job to an AWS Lambda function. Configure the Amazon S3 bucket event notifications to trigger the Lambda function whenever an object is uploaded to the bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Convert the Python script cron job to an AWS Lambda function. Configure AWS CloudTrail to log data events of the Amazon S3 bucket. Set up a CloudWatch Events rule to trigger the Lambda function whenever an upload event on the S3 bucket occurs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Increase the Amazon EC2 instance size and spawn more instances to speed up the processing of the data files. Set the Python script cron job schedule to a 1-minute interval to further improve the access time.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Convert the Python script cron job to an AWS Lambda function. Create an AWS CloudWatch Events rule scheduled at 1-minute intervals and trigger the Lambda function. Create parallel CloudWatch rules that trigger the same Lambda function to further reduce the processing time.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The <strong>Amazon S3 notification feature</strong> enables you to receive notifications when certain events happen in your bucket. To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications. You store this configuration in the notification subresource that is associated with a bucket. Amazon S3 event notifications are designed to be delivered at least once. Typically, event notifications are delivered in seconds but can sometimes take a minute or longer.</p><p><img src=\"https://media.tutorialsdojo.com/S3-events-notification.png\"></p><p>Currently, Amazon S3 can publish notifications for the following events:</p><p><strong>New object created events</strong> — Amazon S3 supports multiple APIs to create objects. You can request notification when only a specific API is used (for example, s3:ObjectCreated:Put), or you can use a wildcard (for example, s3:ObjectCreated:*) to request notification when an object is created regardless of the API used.</p><p><strong>Object removal events</strong> — Amazon S3 supports deletes of versioned and unversioned objects. For information about object versioning, see Object Versioning and Using versioning.</p><p><strong>Restore object events</strong> — Amazon S3 supports the restoration of objects archived to the S3 Glacier storage classes. You request to be notified of object restoration completion by using s3:ObjectRestore:Completed. You use s3:ObjectRestore:Post to request notification of the initiation of a restore.</p><p><strong>Reduced Redundancy Storage (RRS) object lost events</strong> — Amazon S3 sends a notification message when it detects that an object of the RRS storage class has been lost.</p><p><strong>Replication events</strong> — Amazon S3 sends event notifications for replication configurations that have S3 Replication Time Control (S3 RTC) enabled. It sends these notifications when an object fails replication, when an object exceeds the 15-minute threshold, when an object is replicated after the 15-minute threshold, and when an object is no longer tracked by replication metrics. It publishes a second event when that object replicates to the destination Region.</p><p>Enabling notifications is a bucket-level operation; that is, you store notification configuration information in the notification subresource associated with a bucket. After creating or changing the bucket notification configuration, typically you need to wait 5 minutes for the changes to take effect. Amazon S3 supports the following destinations where it can publish events - Amazon Simple Notification Service (Amazon SNS) topic, Amazon Simple Queue Service (Amazon SQS) queue, and AWS Lambda.</p><p>Therefore, the correct answer is: <strong>Convert the Python script cron job to an AWS Lambda function. Configure the Amazon S3 bucket event notifications to trigger the Lambda function whenever an object is uploaded to the bucket</strong> because this provides the best processing and access time. Each of the data files will be processed almost immediately once uploaded on the S3 bucket.</p><p>The option that says: <strong>Convert the Python script cron job to an AWS Lambda function. Configure AWS CloudTrail to log data events of the Amazon S3 bucket. Setup a CloudWatch Events rule to trigger the Lambda function whenever an upload event on the S3 bucket occurs </strong>is incorrect. Although this is possible, you do not have to use CloudTrail and CloudWatch Events to satisfy the given requirement. This solution entails a lot of steps. You can simply use the Amazon S3 event notification feature that can trigger the Lambda function directly.</p><p>The option that says: <strong>Increase the Amazon EC2 instance size and spawn more instances to speed up the processing of the data files. Set the Python script cron job schedule to a 1-minute interval to further improve the access time </strong>is incorrect. This solution is unreliable since the Amazon EC2 instances can process the same data file at the same time, and because of the limitations of <code>cron</code>, the minimum interval for processing is only 1 minute.</p><p>The option that says: <strong>Convert the Python script cron job to an AWS Lambda function. Create an AWS CloudWatch Events rule scheduled at 1-minute intervals and trigger the Lambda function. Create parallel CloudWatch rules that trigger the same Lambda function to further reduce the processing time </strong>is incorrect. The scheduled CloudWatch events rule can only have a minimum of 1-minute intervals. Using Amazon S3 Event notifications as triggers will result in almost near real-time processing of the data files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html#with-s3-example-configure-event-source\">https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html#with-s3-example-configure-event-source</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A company currently hosts its online immigration system on one large Amazon EC2 instance with attached EBS volumes to store all of the applicants' data. The registration system accepts the information from the user including documents and photos and then performs automated verification and processing to check if the applicant is eligible for immigration. The immigration system becomes unavailable at times when there is a surge of applicants using the system. The existing architecture needs improvement as it takes a long time for the system to complete the processing and the attached EBS volumes are not enough to store the ever-growing data being uploaded by the users.</p><p>Which of the following options is the recommended option to achieve high availability and more scalable data storage?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use SNS to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue.</p>"
			},
			{
				"correct": false,
				"answer": "Upgrade to EBS with Provisioned IOPS as your main storage service and change your architecture to use an SQS queue to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue."
			},
			{
				"correct": false,
				"answer": "<p>Use EBS with Provisioned IOPS to store files, SNS to distribute tasks to a group of EC2 instances working in parallel, and Auto Scaling to dynamically size the group of EC2 instances depending on the number of SNS notifications. Use CloudFormation to replicate your architecture to another region.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Upgrade your architecture to use an S3 bucket with cross-region replication (CRR) enabled, as the storage service. Set up an SQS queue to distribute the tasks to a group of EC2 instances with Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue. Use CloudFormation to replicate your architecture to another region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, you need to overhaul the existing immigration service to upgrade its storage and computing capacity. Since EBS Volumes can only provide limited storage capacity and are not scalable, you should use S3 instead. The system goes down at times when there is a surge of requests which indicates that the existing large EC2 instance could not handle the requests any longer. In this case, you should implement a highly-available architecture and a queueing system with SQS and Auto Scaling.</p><p><img src=\"https://media.tutorialsdojo.com/sap_disaster_recovery.png\"></p><p>The option that says: <strong>Upgrade your architecture to use an S3 bucket with cross-region replication (CRR) enabled, as the storage service. Set up an SQS queue to distribute the tasks to a group of EC2 instances with Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue. Use CloudFormation to replicate your architecture to another region</strong> is correct. This option provides high availability and scalable data storage with S3. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps in distributing the tasks to a group of EC2 instances.</p><p>The option that says: <strong>Use EBS with Provisioned IOPS to store files, SNS to distribute tasks to a group of EC2 instances working in parallel, and Auto Scaling to dynamically size the group of EC2 instances depending on the number of SNS notifications. Use CloudFormation to replicate your architecture to another region</strong> is incorrect because EBS is not an easily scalable and durable storage solution compared to Amazon S3. Using SQS is more suitable in distributing the tasks to an Auto Scaling group of EC2 instances and not SNS.</p><p>The option that says: <strong>Use SNS to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue </strong>is incorrect because SNS is not a valid choice in this scenario. Using SQS is more suitable in distributing the tasks to an Auto Scaling group of EC2 instances and not SNS.</p><p>The option that says: <strong>Upgrade to EBS with Provisioned IOPS as your main storage service and change your architecture to use an SQS queue to distribute the tasks to a group of EC2 instances. Use Auto Scaling to dynamically increase or decrease the group of EC2 instances depending on the length of the SQS queue</strong> is incorrect. Having a large EBS volume attached to each of the EC2 instance of the auto-scaling group is not economical. And it will be hard to sync the growing data across these EBS volumes. You should use S3 instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A company uses computer simulations for modeling weather patterns in a certain country. The simulations generate terabytes of data, which is stored in a MySQL 5.6 database that runs in an Amazon EC2 instance. A Ruby on Rails application is hosted on a separate EC2 instance to process the data. The current database size is 16 TiB and is expected to grow as more complex simulations are created continuously. The facility wants to re-architect its infrastructure to be highly scalable and highly available as they need to run the application reliably 24x7.</p><p>Which of the following is the MOST cost-effective solution that can satisfy the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase reserved EC2 instances for fixed capacity and let the Auto Scaling instances run on demand. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Adjust the RDS Storage volume manually as demand increases.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Convert the application tier to Lambda@Edge functions for high availability, scalability, and cost-effectiveness. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Enable storage autoscaling on the RDS instance.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Purchase Reserved Amazon EC2 instances for the application instance and database instances to save costs. Create an EC2 Auto Scaling group with Multi-AZ configuration behind a load balancer for the application tier. Implement a master-slave setup for your database tier. Use Logical Volume Management on the database instances to easily attach new EBS volumes and expand the file system.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase Reserved EC2 instances for fixed capacity and let the Auto Scaling instances run on demand. Migrate the MySQL database to Amazon Aurora. Create a read-replica on another Availability Zone of the Aurora instance for high availability.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can launch and automatically scale a fleet of On-Demand Instances and Spot Instances within a single Auto Scaling group. In addition to receiving discounts for using Spot Instances, you can use Reserved Instances or a Savings Plan to receive discounted rates of the regular On-Demand Instance pricing. All of these factors combined help you to optimize your cost savings for Amazon EC2 instances, while making sure that you obtain the desired scale and performance for your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aurora_readers.png\"></p><p>Amazon Aurora storage automatically scales with the data in your cluster volume. As your data grows, your cluster volume storage expands up to a maximum of 128 tebibytes (TiB). Even though an Aurora cluster volume can scale up in size to many tebibytes, you are only charged for the space that you use in the volume. The mechanism for determining billed storage space depends on the version of your Aurora cluster. Aurora stores copies of the data in a DB cluster across multiple Availability Zones in a single AWS Region. Aurora stores these copies regardless of whether the instances in the DB cluster span multiple Availability Zones. However, you still need to create a read-replica for the high availability of the Aurora DB instance. A single read-replica is enough to quickly recover the database in case the primary instance fails.</p><p>Therefore, the correct answer is:<strong> Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase Reserved EC2 instances for fixed capacity and let the Auto Scaling instances to run on demand. Migrate the MySQL database to Amazon Aurora. Create a read-replica on another Availability Zone of the Aurora instance for high-availability.</strong></p><p>The option that says: <strong>Purchase Reserved Amazon EC2 instances for the application instance and database instances to save costs. Create an EC2 Auto Scaling group with Multi-AZ configuration behind a load balancer for the application tier. Implement a master-slave setup for your database tier. Use Logical Volume Management on the database instances to easily attach new EBS volumes and expand the file system</strong> is incorrect because running your own MySQL server on an EC2 instance entails significant management overhead. This is not scalable because you need to manually resize and configure the EC2 instance as you will eventually need more compute or storage capacity in the future. A better solution is to use Amazon Aurora instead.</p><p>The option that says: <strong>Convert the application tier to Lambda@Edge functions for high availability, scalability, and cost-effectiveness. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Enable storage autoscaling on the RDS instance</strong> is incorrect. Although it is true that MySQL RDS instances can support storage autoscaling for up to 64TiB, using Lambda@Edge to run the application is not warranted. Lambda@Edge is just an extension of AWS Lambda that customizes the content of what Amazon CloudFront delivers. It is not stated in the scenario that the application is used globally. This would be a viable option if you convert your application to Docker-based containers and run them on Amazon ECS.</p><p>The option that says: <strong>Configure your application tier to run on an Auto Scaling group of smaller sized EC2 instances behind an Application Load Balancer. Purchase reserved EC2 instances for fixed capacity and let the Auto Scaling instances to run on demand. Migrate the MySQL database to an Amazon RDS MySQL instance with Multi-AZ enabled. Adjust the RDS Storage volume manually as demand increases</strong> is incorrect. Although this is possible, you still need to manually adjust the disk capacity of your RDS instance. It is better to use Amazon Aurora as it automatically scales disk storage by default.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-rds-mysql-mariadb-postgresql-64tib-support/\">https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-rds-mysql-mariadb-postgresql-64tib-support/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company has a fitness tracking app that accompanies its smartwatch. The primary customers are North American and Asian users. The application is read-heavy as it pings the servers at regular intervals for user-authorization. The company wants the infrastructure to have the following capabilities:</p><p>- The application must be fault-tolerant to problems in any Region.</p><p>- The database writes must be highly-available in a single Region.</p><p>- The application tier must be able to read the database on multiple Regions.</p><p>- The application tier must be resilient in each Region.</p><p>- Relational database semantics must be reflected in the application.</p><p>Which of the following options must the Solutions Architect implement to meet the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create a geolocation routing policy on Amazon Route 53 to point the global users to their designated regions. Combine this with a failover answer routing policy with health checks to direct users to a healthy region at any given time.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a geoproximity routing policy on Amazon Route 53 to control traffic and direct users to their closest regional endpoint. Combine this with a multivalue answer routing policy with health checks to direct users to a healthy region at any given time.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable cross-Region replication for the database servers. Create snapshots of the application and database servers regularly. Store the snapshots in Amazon S3 buckets in both regions.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region in an active-active configuration. Create a cluster of Amazon Aurora global database in both Regions. Configure the application to use the in-Region Aurora database endpoint for the read/write operations. Create snapshots of the application servers regularly. Store the snapshots in Amazon S3 buckets in both regions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable Multi-AZ failover support for the EC2 Auto Scaling group and the RDS database. Enable cross-Region replication for the database servers.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Aurora Global Database</strong> is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.</p><p>By using an Amazon Aurora global database, you can have a single Aurora database that spans multiple AWS Regions to support your globally distributed applications.</p><p>An Aurora global database consists of one primary AWS Region where your data is mastered, and up to five read-only secondary AWS Regions. You issue write operations directly to the primary DB cluster in the primary AWS Region. Aurora replicates data to the secondary AWS Regions using dedicated infrastructure, with latency typically under a second.</p><p>You can also change the configuration of your Aurora global database while it's running to support various use cases. For example, you might want the read/write capabilities to move from one Region to another, say, in different time zones, to 'follow the sun.' Or, you might need to respond to an outage in one Region. With Aurora global database, you can promote one of the secondary Regions to the primary role to take full read/write workloads in under a minute.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aurora_architecture.png\"></p><p>On <strong>Amazon Route 53</strong>, after you create a hosted zone for your domain, such as tutorialsdojo.com, you can create records to tell the Domain Name System (DNS) how you want traffic to be routed for that domain. You can create a record that points to the DNS name of your Application Load Balancer on AWS.</p><p>When you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to queries:</p><p><strong>Simple routing policy</strong> – Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.</p><p><strong>Failover routing policy</strong> – Use when you want to configure active-passive failover.</p><p><strong>Geolocation routing policy</strong> – Use when you want to route traffic based on the location of your users.</p><p><strong>Geoproximity routing policy</strong> – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</p><p><strong>Latency routing policy </strong>– Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.</p><p><strong>Multivalue answer routing policy</strong> – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.</p><p><strong>Weighted routing policy</strong> – Use to route traffic to multiple resources in proportions that you specify.</p><p>You can use Route 53 health checks to configure active-active and active-passive failover configurations. You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy.</p><p>The option that says: <strong>Create a geolocation routing policy on Amazon Route 53 to point the global users to their designated regions. Combine this with a failover answer routing policy with health checks to direct users to a healthy region at any given time</strong> is correct. You can use geolocation routing policy to direct the North American users to your servers on the North America region and configure failover routing to the Asia region in case the North America region fails. You can configure the same for the Asian users pointed to the Asia region servers and have the North America region as its backup.</p><p>The option that says: <strong>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region in an active-active configuration. Create a cluster of Amazon Aurora global database in both Regions. Configure the application to use the in-Region Aurora database endpoint for the read/write operations. Create snapshots of the application servers regularly. Store the snapshots in Amazon S3 buckets in both regions</strong> is correct. The Amazon Aurora global database solves the problem on read/write as well as syncing the data across all the regions. With both regions in an active-active configuration, each region can accept the traffic from users around the world.</p><p>The option that says: <strong>Create a geoproximity routing policy on Amazon Route 53 to control traffic and direct users to their closest regional endpoint. Combine this with a multivalue answer routing policy with health checks to direct users to a healthy region at any given time</strong> is incorrect. Geoproximity routing policy is good to control the user traffic to specific regions. However, a multivalue answer routing policy may cause the users to be randomly sent to other healthy regions that may be far away from the user’s location.</p><p>The option that says: <strong>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable cross-Region replication for the database servers. Create snapshots of the application and database servers regularly. Store the snapshots in Amazon S3 buckets in both regions</strong> is incorrect. You can’t sync two MySQL master databases that both accept writes on their respective regions.</p><p>The option that says: <strong>Deploy the application tier on an Auto Scaling group of EC2 instances for each Region. Create an RDS for MySQL database on each region. Configure the application to perform read/write operations on the local RDS. Enable Multi-AZ failover support for the EC2 Auto Scaling group and the RDS database. Enable cross-Region replication for the database servers</strong> is incorrect. You can’t sync two MySQL master databases that both accept writes on their respective regions. Enabling Multi-AZ on the RDS MySQL server does not protect you from AWS Regional failures.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database-overview\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database-overview</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-connecting.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database-connecting.html</a></p><p><br></p><p><strong>Check out these Amazon Aurora and Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>A retail company hosts its web application on an Auto Scaling group of Amazon EC2 instances deployed across multiple Availability Zones. The Auto Scaling group is configured to maintain a minimum EC2 cluster size and automatically replace unhealthy instances. The EC2 instances are behind an Application Load Balancer so that the load can be spread evenly on all instances. The application target group health check is configured with a fixed HTTP page that queries a dummy item on the database. The web application connects to a Multi-AZ Amazon RDS MySQL instance. A recent outage caused a major loss to the company's revenue. Upon investigation, it was found that the web server metrics are within the normal range but the database CPU usage is very high, causing the EC2 health checks to timeout. Failing the health checks, the Auto Scaling group continuously replaced the unhealthy instances thus causing the downtime.</p><p>Which of the following options should the Solution Architect implement to prevent this from happening again and allow the application to handle more traffic in the future? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon CloudWatch alarm to monitor the Amazon RDS MySQL instance if it has a high-load or in impaired status. Set the alarm action to recover the RDS instance. This will automatically reboot the database to reset the queries.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Change the target group health check to a simple HTML page instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reduce the load on the database tier by creating multiple read replicas for the Amazon RDS MySQL Multi-AZ cluster. Configure the web application to use the single reader endpoint of RDS for all read operations.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Change the target group health check to use a TCP check on the EC2 instances instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Reduce the load on the database tier by creating an Amazon ElastiCache cluster to cache frequently requested database queries. Configure the application to use this cache when querying the RDS MySQL instance.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Route 53 health checks</strong> monitor the health and performance of your web applications, web servers, and other resources. Each health check that you create can monitor one of the following:</p><p><strong>The health of a specified resource, such as a web server</strong> - You can configure a health check that monitors an endpoint that you specify either by IP address or by the domain name. At regular intervals that you specify, Route 53 submits automated requests over the Internet to your application. You can configure the health check to make requests similar to those that your users make, such as requesting a web page from a specific URL.</p><p><strong>The status of other health checks</strong> - You can create a health check that monitors whether Route 53 considers other health checks healthy or unhealthy. One situation where this might be useful is when you have multiple resources that perform the same function, such as multiple web servers, and your chief concern is whether some minimum number of your resources are healthy.</p><p><strong>The status of an Amazon CloudWatch alarm</strong> - You can create CloudWatch alarms that monitor the status of CloudWatch metrics, such as the number of throttled read events for an Amazon DynamoDB database or the number of Elastic Load Balancing hosts that are considered healthy.</p><p>After you create a health check, you can get the status of the health check, get notifications when the status changes, and configure DNS failover. To improve resiliency and availability, Route 53 doesn't wait for the CloudWatch alarm to go into the ALARM state. The status of a health check changes from healthy to unhealthy based on the data stream and on the criteria in the CloudWatch alarm.</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_failover_sns.png\"></p><p>Your <strong>Application Load Balancer</strong> periodically sends requests to its registered targets to test their status. These tests are called health checks. Each load balancer node routes requests only to the healthy targets in the enabled Availability Zones for the load balancer. Each load balancer node checks the health of each target, using the health check settings for the target groups with which the target is registered. After your target is registered, it must pass one health check to be considered healthy. After each health check is completed, the load balancer node closes the connection that was established for the health check. If a target group contains only unhealthy registered targets, the load balancer nodes route requests across its unhealthy targets.</p><p>Each health check will be executed at configured intervals to all the EC2 instances so if the health check query page involves a database query, there will be several simultaneous queries to the database. This can increase the load of your database tier if there are many EC2 instances and the health check interval period is very quick.</p><p><strong>Amazon ElastiCache</strong> is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud. It provides a high-performance, scalable, and cost-effective caching solution. At the same time, it helps remove the complexity associated with deploying and managing a distributed cache environment.</p><p>ElastiCache for Memcached has multiple features to enhance reliability for critical production deployments:</p><p>- Automatic detection and recovery from cache node failures.</p><p>- Automatic discovery of nodes within a cluster enabled for automatic discovery, so that no changes need to be made to your application when you add or remove nodes.</p><p>- Flexible Availability Zone placement of nodes and clusters.</p><p>- Integration with other AWS services such as Amazon EC2, Amazon CloudWatch, AWS CloudTrail, and Amazon SNS to provide a secure, high-performance, managed in-memory caching solution.</p><p>The option that says: <strong>Change the target group health check to a simple HTML page instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails </strong>is correct. Changing the target group health check to a simple HTML page will reduce the queries to the database tier. The Route 53 health check can act as the “external” check on a specific page that queries the database to ensure that the application is working as expected. The Route 53 health check has an overall lower request count compared to using the target group health check.</p><p>The option that says: <strong>Reduce the load on the database tier by creating an Amazon ElastiCache cluster to cache frequently requested database queries. Configure the application to use this cache when querying the RDS MySQL instance </strong>is correct. Since this is a retail web application, most of the queries will be read-intensive as customers are searching for products. ElastiCache is effective at caching frequent requests which overall improves the application response time and reduces database queries.</p><p>The option that says: <strong>Reduce the load on the database tier by creating multiple read replicas for the Amazon RDS MySQL Multi-AZ cluster. Configure the web application to use the single reader endpoint of RDS for all read operations </strong>is incorrect. Creating read replicas is recommended to increase the read performance of an RDS cluster. However, the Amazon RDS MySQL does not have a single reader endpoint for read replicas. You must use Amazon Aurora for MySQL to support this.</p><p>The option that says: <strong>Change the target group health check to use a TCP check on the EC2 instances instead of a page that queries the database. Create an Amazon Route 53 health check for the database dummy item web page to ensure that the application works as expected. Set up an Amazon CloudWatch alarm to send a notification to Admins when the health check fails</strong> is incorrect. An Application Load Balancer does not support a TCP health check. ALB only supports HTTP and HTTPS target health checks.</p><p>The option that says: <strong>Create an Amazon CloudWatch alarm to monitor the Amazon RDS MySQL instance if it has a high-load or in impaired status. Set the alarm action to recover the RDS instance. This will automatically reboot the database to reset the queries </strong>is incorrect. Recovering the database instance results in downtime. If you have the Multi-AZ enabled, the standby database will shoulder all the load causing it to crash too. It is better to scale the database by creating read replicas or adding an ElastiCache cluster in front of it.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-health-checks.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.html</a></p><p><br></p><p><strong>Check out these Amazon ElastiCache and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A telecommunications company is planning to host a WordPress website on an Amazon ECS Cluster which uses the Fargate launch type. For security purposes, the database credentials should be provided to the WordPress image by using environment variables. Your manager instructed you to ensure that the credentials are secure when passed to the image and that they cannot be viewed on the cluster itself. The credentials must be kept in a dedicated storage with lifecycle management and key rotation. </p><p>Which of the following is the most suitable solution in this scenario that you can implement with the least effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>In the ECS task definition file of the ECS Cluster, store the database credentials using Docker Secrets to centrally manage this sensitive data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest. A given secret is only accessible to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store the database credentials using the AWS Systems Manager Parameter Store and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container.</p>"
			},
			{
				"correct": false,
				"answer": "<p>In the ECS task definition file of the ECS Cluster, store the database credentials and encrypt with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the <code>--cli-input-json</code> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Store the database credentials using the AWS Secrets Manager and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon ECS</strong> enables you to inject sensitive data into your containers by storing your sensitive data in either AWS Secrets Manager secrets or AWS Systems Manager Parameter Store parameters and then referencing them in your container definition. This feature is supported by tasks using both the EC2 and Fargate launch types.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ecs_steps.png\"></p><p>Within your container definition, specify <code>secrets</code> with the name of the environment variable to set in the container and the full ARN of either the Secrets Manager secret or Systems Manager Parameter Store parameter containing the sensitive data to present to the container. The parameter that you reference can be from a different Region than the container using it, but must be from within the same account.</p><p>AWS Secrets Manager is a secrets management service that helps you protect access to your applications, services, and IT resources. This service enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Using Secrets Manager, you can secure and manage secrets used to access resources in the AWS Cloud, on third-party services, and on-premises.</p><p>If you want a single store for configuration and secrets, you can use Parameter Store. If you want a dedicated secrets store with lifecycle management, use Secrets Manager.</p><p>Hence, the correct answer is the option that says: <strong>Store the database credentials using the AWS Secrets Manager and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition which allows access to both KMS and AWS Secrets Manager. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Secrets Manager secret which contains the sensitive data, to present to the container.</strong></p><p>The option that says: <strong>Store the database credentials using the AWS Systems Manager Parameter Store and then encrypt them using AWS KMS. Create an IAM Role for your Amazon ECS task execution role and reference it with your task definition, which allows access to both KMS and the Parameter Store. Within your container definition, specify secrets with the name of the environment variable to set in the container and the full ARN of the Systems Manager Parameter Store parameter containing the sensitive data to present to the container</strong> is incorrect. Although the use of Systems Manager Parameter Store in securing sensitive data in ECS is valid, this service doesn't provide dedicated storage with lifecycle management and key rotation, unlike Secrets Manager.</p><p>The option that says: <strong>In the ECS task definition file of the ECS Cluster, store the database credentials and encrypt with KMS. Store the task definition JSON file in a private S3 bucket and ensure that HTTPS is enabled on the bucket to encrypt the data in-flight. Create an IAM role to the ECS task definiton script that allows access to the specific S3 bucket and then pass the </strong><code><strong>--cli-input-json</strong></code><strong> parameter when calling the ECS register-task-definition. Reference the task definition JSON file in the S3 bucket which contains the database credentials</strong> is incorrect. Although the solution may work, it is not recommended to store sensitive credentials in S3. This entails a lot of overhead and manual configuration steps which can be simplified by using the Secrets Manager or Systems Manager Parameter Store.</p><p>The option that says:<strong><em> </em>In the ECS task definition file of the ECS Cluster, store the database credentials using Docker Secrets to centrally manage this sensitive data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest. A given secret is only accessible to those services which have been granted explicit access to it via IAM Role, and only while those service tasks are running</strong> is incorrect. Although you can use Docker Secrets to secure the sensitive database credentials, this feature is only applicable in Docker Swarm. In AWS, the recommended way to secure sensitive data is either through the use of Secrets Manager or Systems Manager Parameter Store.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/\">https://aws.amazon.com/blogs/mt/the-right-way-to-store-secrets-using-parameter-store/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>Check out this AWS Secrets Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p></div>"
	},
	{
		"question": "<p>A stocks brokerage firm hosts its legacy application on Amazon EC2 in a private subnet of its Amazon VPC. The application is accessed by the employees from their corporate laptops through a proprietary desktop program. The company network is peered with the AWS Direct Connect (DX) connection to provide a fast and reliable connection to the private EC2 instances inside the VPC. To comply with the strict security requirements of financial institutions, the firm is required to encrypt its network traffic that flows from the employees' laptops to the resources inside the VPC.</p><p>Which of the following solution will comply with this requirement while maintaining the consistent network performance of Direct Connect?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Using the current Direct Connect connection, create a new public virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the company network to route employee traffic to this VPN.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Using the current Direct Connect connection, create a new private virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the Internet. Configure the employees’ laptops to connect to this VPN.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Using the current Direct Connect connection, create a new private virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the company network to route employee traffic to this VPN.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Using the current Direct Connect connection, create a new public virtual interface and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the Internet. Configure the employees’ laptops to connect to this VPN.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>To connect to services such as EC2 using just Direct Connect you need to create a private virtual interface. However, if you want to encrypt the traffic flowing through Direct Connect, you will need to use the public virtual interface of DX to create a VPN connection that will allow access to AWS services such as S3, EC2, and other services.</p><p>To connect to AWS resources that are reachable by a public IP address (such as an Amazon Simple Storage Service bucket) or AWS public endpoints, use a <strong>public virtual interface</strong>. With a public virtual interface, you can:</p><p>- Connect to all AWS public IP addresses globally.</p><p>- Create public virtual interfaces in any DX location to receive Amazon’s global IP routes.</p><p>- Access publicly routable Amazon services in any AWS Region (except for the AWS China Region).</p><p><img src=\"https://media.tutorialsdojo.com/sap_dc_public_vif.png\"></p><p>To connect to your resources hosted in an Amazon Virtual Private Cloud (Amazon VPC) using their private IP addresses, use a private virtual interface. With a <strong>private virtual interface</strong>, you can:</p><p>- Connect VPC resources (such as Amazon Elastic Compute Cloud (Amazon EC2) instances or load balancers) on your private IP address or endpoint.</p><p>- Connect a private virtual interface to a DX gateway. Then, associate the DX gateway with one or more virtual private gateways in any AWS Region (except the AWS China Region).</p><p>- Connect to multiple VPCs in any AWS Region (except the AWS China Region), because a virtual private gateway is associated with a single VPC.</p><p>If you want to establish a virtual private network (VPN) connection from your company network to an Amazon Virtual Private Cloud (Amazon VPC) over an AWS Direct Connect (DX) connection, you must use a public virtual interface for your DX connection.</p><p>Therefore, the correct answer is: <strong>Using the current Direct Connect connection, create a new public virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the company network to route employee traffic to this VPN.</strong></p><p>The option that says: <strong>Using the current Direct Connect connection, create a new private virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC with the BGP protocol using the DX connection. Configure the employees’ laptops to connect to this VPN</strong> is incorrect because you must use a public virtual interface for your AWS Direct Connect (DX) connection and not a private one. You won't be able to establish an encrypted VPN along with your DX connection if you create a private virtual interface.</p><p>The following options are incorrect because you need to establish the VPN connection through the DX connection, and not over the Internet.</p><p><strong>- Using the current Direct Connect connection, create a new public virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the internet. Configure the employees’ laptops to connect to this VPN.</strong></p><p><strong>- Using the current Direct Connect connection, create a new private virtual interface, and input the network prefixes that you want to advertise. Create a new site-to-site VPN connection to the VPC over the internet. Configure the company network to route employee traffic to this VPN.</strong></p><p><br></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-vpn.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/\">https://aws.amazon.com/premiumsupport/knowledge-center/create-vpn-direct-connect/</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p></div>"
	},
	{
		"question": "<p>An accounting firm hosts a mix of Windows and Linux Amazon EC2 instances in its AWS account. The solutions architect has been tasked to conduct a monthly performance check on all production instances. There are more than 200 On-Demand EC2 instances running in their production environment and it is required to ensure that each instance has a logging feature that collects various system details such as memory usage, disk space, and other metrics. The system logs will be analyzed using AWS Analytics tools and the results will be stored in an S3 bucket.</p><p>Which of the following is the most efficient way to collect and analyze logs from the instances with minimal effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up and install the AWS Systems Manager Agent (SSM Agent) on each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Enable the Traffic Mirroring feature and install AWS CDK on each On-Demand EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Set up CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up and install AWS Inspector Agent on each On-Demand EC2 instance which will collect and push data to CloudWatch Logs periodically. Set up a CloudWatch dashboard to properly analyze the log data of all instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up and configure a unified CloudWatch Logs agent in each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs. Analyze the log data with CloudWatch Logs Insights.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>To collect logs from your Amazon EC2 instances and on-premises servers into CloudWatch Logs, AWS offers both a new unified CloudWatch agent, and an older CloudWatch Logs agent. It is recommended to use the unified CloudWatch agent which has the following advantages:</p><p>- You can collect both logs and advanced metrics with the installation and configuration of just one agent.</p><p>- The unified agent enables the collection of logs from servers running Windows Server.</p><p>- If you are using the agent to collect CloudWatch metrics, the unified agent also enables the collection of additional system metrics, for in-guest visibility.</p><p>- The unified agent provides better performance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_cloudwatch.png\"></p><p><strong>CloudWatch Logs Insights</strong> enables you to interactively search and analyze your log data in Amazon CloudWatch Logs. You can perform queries to help you quickly and effectively respond to operational issues. If an issue occurs, you can use CloudWatch Logs Insights to identify potential causes and validate deployed fixes.</p><p>CloudWatch Logs Insights includes a purpose-built query language with a few simple but powerful commands. CloudWatch Logs Insights provides sample queries, command descriptions, query autocompletion, and log field discovery to help you get started quickly. Sample queries are included for several types of AWS service logs.</p><p>Therefore, the correct answer is: <strong>Set up and configure a unified CloudWatch Logs agent in each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs, then analyze the log data with CloudWatch Logs Insights.</strong></p><p>The option that says:<strong> Enable the Traffic Mirroring feature and install AWS CDK on each On-Demand EC2 instance. Create a custom daemon script that would collect and push data to CloudWatch Logs periodically. Set up CloudWatch detailed monitoring and use CloudWatch Logs Insights to analyze the log data of all instances</strong> is incorrect. Although this is a valid solution, this entails a lot of effort to implement as you have to allocate time to install the AWS CDK to each instance and develop a custom monitoring solution. Traffic Mirroring is simply an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon EC2 instances. As per the scenario, you are specifically looking for a solution that can be implemented with minimal effort. In addition, it is unnecessary and not cost-efficient to enable detailed monitoring in CloudWatch in order to meet the requirements since this can be done using CloudWatch Logs.</p><p>The option that says:<strong> Setting up and installing the AWS Systems Manager Agent (SSM Agent) on each On-Demand EC2 instance which will automatically collect and push data to CloudWatch Logs, then analyzing the log data with CloudWatch Logs Insights</strong> is incorrect. Although this is also a valid solution, it is more efficient to use a CloudWatch agent than an SSM agent. Manually connecting to an instance to view log files and troubleshoot an issue with SSM Agent is time-consuming hence, for more efficient instance monitoring, you can use the CloudWatch Agent instead to send the log data to Amazon CloudWatch Logs.</p><p>The option that says:<strong> Setting up and installing AWS Inspector Agent on each On-Demand EC2 instance which will collect and push data to CloudWatch Logs periodically, then setting up a CloudWatch dashboard to properly analyze the log data of all instances</strong> is incorrect. AWS Inspector is simply a security assessments service that only helps you in checking for unintended network accessibility of your EC2 instances and for vulnerabilities on those EC2 instances. Furthermore, setting up an Amazon CloudWatch dashboard is not suitable since it's primarily used for scenarios where you have to monitor your resources in a single view, even those resources that are spread across different AWS Regions. It is better to use CloudWatch Logs Insights instead since it enables you to interactively search and analyze your log data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/monitoring-ssm-agent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p><p><br></p><p><strong>CloudWatch Agent vs SSM Agent vs Custom Daemon Scripts</strong></p><p><a href=\"https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/?src=udemy\">https://tutorialsdojo.com/cloudwatch-agent-vs-ssm-agent-vs-custom-daemon-scripts/</a></p></div>"
	},
	{
		"question": "<p>A tech company plans to host a website using an Amazon S3 bucket. The solutions architect created a new S3 bucket called “www.tutorialsdojo.com\" in us-west-2 AWS region, enabled static website hosting, and uploaded the static web content files including the index.html file. The custom domain <code>www.tutorialsdojo.com</code> has been registered using Amazon Route 53 to be associated with the S3 bucket. The next day, a new Route 53 Alias record set was created which points to the S3 website endpoint: <code>http://www.tutorialsdojo.com.s3-website-us-west-2.amazonaws.com</code>. Upon testing, users cannot see any content on the bucket. Both the domains <code>tutorialsdojo.com</code> and <code>www.tutorialsdojo.com</code> do not work properly.</p><p>Which of the following is the MOST likely cause of this issue that the Architect should fix?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>The site will not work because the URL does not include a file name at the end. This means that you need to use this URL instead: <code>www.tutorialsdojo.com/index.html</code></p>"
			},
			{
				"correct": true,
				"answer": "The S3 bucket does not have public read access which blocks the website visitors from seeing the content. "
			},
			{
				"correct": false,
				"answer": "The site does not work because you have not set a value for the error.html file, which is a required step."
			},
			{
				"correct": false,
				"answer": "<p>Route 53 is still propagating the domain name changes. Wait for another 12 hours and then try again.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can host a static website on <strong>Amazon Simple Storage Service (Amazon S3)</strong>. On a static website, individual webpages include static content. They might also contain client-side scripts. To host a static website, you configure an Amazon S3 bucket for website hosting, and then upload your website content to the bucket. This bucket must have public read access. It is intentional that everyone in the world will have read access to this bucket.</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_s3bucket.png\"></p><p>When you configure an Amazon S3 bucket for website hosting, you must give the bucket the same name as the record that you want to use to route traffic to the bucket. For example, if you want to route traffic for <code>example.com</code> to an S3 bucket that is configured for website hosting, the name of the bucket must be <code>example.com</code>.</p><p>If you want to route traffic to an S3 bucket that is configured for website hosting but the name of the bucket doesn't appear in the <strong>Alias Target</strong> list in the <strong>Amazon Route 53</strong> console, check the following:</p><p>- The name of the bucket exactly matches the name of the record, such as <code>tutorialsdojo.com</code> or <code>www.tutorialsdojo.com</code>.</p><p>- The S3 bucket is correctly configured for website hosting.</p><p>In this scenario, the static S3 website does not work because the bucket does not have a public read access.</p><p>Therefore, the correct answer is: <strong>The S3 bucket does not have public read access which blocks the website visitors from seeing the content.</strong> This is the root cause why the static S3 website is inaccessible.</p><p>The option that says:<strong><em> </em>The site does not work because you have not set a value for the error.html file, which is a required step</strong> is incorrect as the error.html is not required and won't affect the availability of the static S3 website.</p><p>The option that says: <strong>The site will not work because the URL does not include a file name at the end. This means that you need to use this URL instead: www.tutorialsdojo.com/index.html</strong> is incorrect as it is not required to manually append the exact filename in S3.</p><p>The option that says: <strong>Route 53 is still propagating the domain name changes. Wait for another 12 hours and then try again</strong> is incorrect as the Route 53 domain name propagation does not take that long. Remember that Amazon Route 53 is designed to propagate updates you make to your DNS records to its worldwide network of authoritative DNS servers within 60 seconds under normal conditions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A graphics design startup is using multiple Amazon S3 buckets to store high-resolution media files for their various digital artworks. After securing a partnership deal with a leading media company, the two parties shall be sharing digital resources with one another as part of the contract. The media company frequently performs multiple object retrievals from the S3 buckets every day, which increased the startup's data transfer costs.</p><p>As the Solutions Architect, what should you do to help the startup lower their operational costs?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Provide cross-account access for the media company, which has permissions to access contents in the S3 bucket. Cross-account retrieval of S3 objects is charged to the account that made the request.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Advise the media company to create their own S3 bucket. Then run the <code>aws s3 sync s3://sourcebucket s3://destinationbucket</code> command to copy the objects from their S3 bucket to the other party's S3 bucket. In this way, future retrievals can be made on the media company's S3 bucket instead.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Enable the Requester Pays feature in all of the startup's S3 buckets to make the media company pay the cost of the data transfer from the buckets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new billing account for the social media company by using AWS Organizations. Apply SCPs on the organization to ensure that each account has access only to its own resources and each other's S3 buckets.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their bucket. A bucket owner, however, can configure a bucket to be a <strong>Requester Pays</strong> bucket. With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.</p><p><img src=\"https://media.tutorialsdojo.com/S3_Requester_Pays.png\"></p><p>You must authenticate all requests involving Requester Pays buckets. The request authentication enables Amazon S3 to identify and charge the requester for their use of the Requester Pays bucket. After you configure a bucket to be a Requester Pays bucket, requesters must include x-amz-request-payer in their requests either in the header, for POST, GET and HEAD requests, or as a parameter in a REST request to show that they understand that they will be charged for the request and the data download.</p><p>Hence, the correct answer is to <strong>enable the Requester Pays feature in all of the startup's S3 buckets to make the media company pay the cost of the data transfer from the buckets.</strong></p><p>The option that says: <strong>Advise the media company to create their own S3 bucket. Then run the </strong><code><strong>aws s3 sync s3://sourcebucket s3://destinationbucket</strong></code><strong> command to copy the objects from their S3 bucket to the other party's S3 bucket. In this way, future retrievals can be made on the media company's S3 bucket instead</strong> is incorrect because sharing all the assets of the startup to the media entails a lot of costs considering that you will be charged for the data transfer charges made during the sync process.</p><p><strong>Creating a new billing account for the social media company by using AWS Organizations, then applying SCPs on the organization to ensure that each account has access only to its own resources and each other's S3 buckets</strong> is incorrect because AWS Organizations does not create a separate billing account for every account under it. Instead, what AWS Organizations has is consolidated billing. You can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts. Every organization in AWS Organizations has a master account that pays the charges of all the member accounts.</p><p>The option that says: <strong>Provide cross-account access for the media company, which has permissions to access contents in the S3 bucket. Cross-account retrieval of S3 objects is charged to the account that made the request</strong> is incorrect because cross-account access does not shoulder the charges that are made during S3 object requests. Unless Requester Pays is enabled on the bucket, the bucket owner is still the one that is charged.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A company regularly processes large product catalogs for its online retail platform, which is needed to index and extract metadata of its items. These are done in batches and are sent out to a small team to process them using the Amazon Mechanical Turk service. The Solutions Architect has been tasked to design a workflow orchestration process to allow multiple concurrent Mechanical Turk operations while dealing with the result assessment process and the ability to reprocess the failed jobs.</p><p>Which of the following solutions will allow the company to visualize and control the state of every workflow with the LEAST amount of effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>With AWS Step Functions, create a workflow that will orchestrate multiple concurrent workflows. Visualize each workflow status on the AWS Management Console and write the historical data on an Amazon S3 bucket. Use Amazon QuickSight to visualize the data on the S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a workflow on Amazon SWF that will handle a single batch of catalog records. Create multiple worker tasks that will extract and transform the data before sending it through Amazon Mechanical Turk. To visualize the workflow states, process the logs using AWS Lambda functions and use Amazon OpenSearch and Kibana.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Run a Lambda function to regularly poll for status changes in an Amazon RDS database that stores the workflow information. Create worker Lambda functions that will then process the next workflow steps. Visualize the workflow states using Amazon QuickSight on the Amazon RDS database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a queue per workflow stage on Amazon SQS and trigger an Amazon CloudWatch Alarm based on the message visibility on each queue. Send messages using Amazon SNS to trigger the AWS Lambda functions that process the next step. Visualize the Lambda processing logs for each workflow state using Amazon OpenSearch and Kibana.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>While validating data in large catalogs, the products in the catalog are processed in batches. Different batches can be processed concurrently. For each batch, the product data is extracted from servers in the datacenter and transformed into CSV (Comma Separated Values) files required by Amazon Mechanical Turk’s Requester User Interface (RUI). The CSV is uploaded to populate and run the HITs (Human Intelligence Tasks). When HITs complete, the resulting CSV file is reverse transformed to get the data back into the original format. The results are then assessed and Amazon Mechanical Turk workers are paid for acceptable results. Failures are weeded out and reprocessed, while the acceptable HIT results are used to update the catalog. As batches are processed, the system needs to track the quality of the Amazon Mechanical Turk workers and adjust the payments accordingly. Failed HITs are re-batched and sent through the pipeline again.</p><p><img src=\"https://media.tutorialsdojo.com/aws_swf_mechanical_turk.png\"></p><p>With Amazon SWF: The use case above is implemented as a set of workflows. A <strong>BatchProcess</strong> workflow handles the processing for a single batch. It has workers that extract the data, transform it, and send it through Amazon Mechanical Turk. The <strong>BatchProcess</strong> workflow outputs the acceptable HITs and the failed ones. This is used as the input for three other workflows: <strong>MTurkManager</strong>, <strong>UpdateCatalogWorkflow</strong>, and <strong>RerunProducts</strong>.</p><p>The <strong>MTurkManager</strong> workflow makes payments for acceptable HITs, responds to the human workers who produced failed HITs, and updates its own database for tracking results quality. The <strong>UpdateCatalogWorkflow</strong> updates the master catalog based on acceptable HITs. The <strong>RerunProducts</strong> workflow waits until there is a large enough batch of products with failed HITs. It then creates a batch and sends it back to the <strong>BatchProcess</strong> workflow. The entire end-to-end catalog processing is performed by a <strong>CleanupCatalog</strong> workflow that initiates child executions of the above workflows. Having a system of well-defined workflows enables this use case to be architected, audited, and run systematically for catalogs with several million products.</p><p><strong>Amazon Elasticsearch Service</strong> is now called <strong>Amazon OpenSearch Service</strong>.</p><p>The scenario for this question is similar to the Amazon Simple Workflow Service FAQs. See the FAQs Use Case #2: <a href=\"https://aws.amazon.com/swf/faqs/#:~:text=Processing%20large%20product%20catalogs%20using%20Amazon%20Mechanical%20Turk\"><em>Processing large product catalogs using Amazon Mechanical Turk.</em></a></p><p>Therefore, the correct answer is:<strong> Create a workflow on Amazon SWF that will handle a single batch of catalog records. Create multiple worker tasks that will extract and transform the data before sending it through Amazon Mechanical Turk. To visualize the workflow states, process the logs using AWS Lambda functions and use Amazon OpenSearch and Kibana.</strong></p><p>The option that says: <strong>Create a queue per workflow stage on Amazon SQS and trigger an Amazon CloudWatch Alarm based on the message visibility on each queue. Send messages using Amazon SNS to trigger the AWS Lambda functions that process the next step. Visualize the Lambda processing logs for each workflow state using Amazon OpenSearch and Kibana </strong>is incorrect because it requires a lot of effort to configure each component on various stages. The reliance on multiple AWS services adds complexity and a chance for the workflow to encounter an error when running.</p><p>The option that says: <strong>Run a Lambda function to regularly poll for status changes in an Amazon RDS database that stores the workflow information. Create worker Lambda functions that will then process the next workflow steps. Visualize the workflow states using Amazon QuickSight on the Amazon RDS database</strong> is incorrect. This is inefficient as the Lambda function constantly polls the RDS database for changes. Additionally, this incurs unnecessary costs as you keep the RDS instance running even if there are no batch jobs to be processed.</p><p>The option that says: <strong>With AWS Step Functions, create a workflow that will orchestrate multiple concurrent workflows. Visualize each workflow status on the AWS Management Console, and write the historical data on an Amazon S3 bucket. Use Amazon QuickSight to visualize the data on the S3 bucket </strong>is incorrect because Step Functions do not directly support Mechanical Turk. You will need to use Amazon SWF for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/swf/faqs/\">https://aws.amazon.com/swf/faqs/</a></p><p><a href=\"https://aws.amazon.com/swf/\">https://aws.amazon.com/swf/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SvcIntro.html\">https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SvcIntro.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/NextSteps.html\">https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/NextSteps.html</a></p><p><br></p><p><strong>Check out these Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-simple-workflow-amazon-swf/?src=udemy\">https://tutorialsdojo.com/amazon-simple-workflow-amazon-swf/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-mechanical-turk/?src=udemy\">https://tutorialsdojo.com/amazon-mechanical-turk/</a></p></div>"
	},
	{
		"question": "<p>A private bank is hosting a secure web application that allows its agents to view highly sensitive information about the clients. The amount of traffic that the web app will receive is known and not expected to fluctuate. An SSL will be used as part of the application's data security. The chief information security officer (CISO) is concerned about the security of the SSL private key. The CISO wants to ensure that the key cannot be accidentally or intentionally moved outside the corporate environment. The solutions architect is also concerned that the application logs might contain some sensitive information. The EBS volumes used to store the data are already encrypted. In this scenario, the application logs must be stored securely and durably so that they can only be decrypted by authorized employees.</p><p>Which of the following is the most suitable and highly available architecture that can meet all of the requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use an AWS CloudHSM to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption.</p>"
			},
			{
				"correct": false,
				"answer": "Distribute traffic to a set of web servers using an Elastic Load Balancer. To secure the SSL private key, upload the key to the load balancer and configure the load balancer to offload the SSL traffic. Lastly, write your application logs to an instance store volume that has been encrypted using a randomly generated AES key."
			},
			{
				"correct": false,
				"answer": "Distribute traffic to a set of web servers using an Elastic Load Balancer. Use TCP load balancing for the load balancer and configure your web servers to retrieve the SSL private key from a private Amazon S3 bucket on boot. Use another private Amazon S3 bucket to store your web server logs using Amazon S3 server-side encryption."
			},
			{
				"correct": true,
				"answer": "<p>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use CloudHSM deployed to two Availability Zones to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudHSM</strong> is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With CloudHSM, you can manage your own encryption keys and automate time-consuming administrative tasks for you, such as hardware provisioning, software patching, high-availability, and backups.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudhsm_s3_logs.PNG\"></p><p>The correct answer is the option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use CloudHSM deployed to two Availability Zones to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption. </strong>It uses CloudHSM for performing the SSL transaction without requiring any additional way of storing or managing the SSL private key. This is the most secure way of ensuring that the key will not be moved outside of the AWS environment. Also, it uses the highly available and durable S3 service for storing the logs. Take note that this option says \"server-side encryption\" and not \"Amazon S3-Managed Encryption Keys\", which are two different things.</p><p>The option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer. Use TCP load balancing for the load balancer and configure your web servers to retrieve the SSL private key from a private Amazon S3 bucket on boot. Use another private Amazon S3 bucket to store your web server logs using Amazon S3 server-side encryption</strong> is incorrect because it does not use a secure way of managing the SSL private key for the SSL transaction.</p><p>The option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer. To secure the SSL private key, upload the key to the load balancer and configure the load balancer to offload the SSL traffic. Lastly, write your application logs to an instance store volume that has been encrypted using a randomly generated AES key</strong> is incorrect. The application logs are written to an ephemeral volume, which means that the data will be lost when the EC2 instance is terminated. Hence, this solution is neither durable nor secure.</p><p>The option that says: <strong>Distribute traffic to a set of web servers using an Elastic Load Balancer that performs TCP load balancing. Use an AWS CloudHSM to perform the SSL transactions and deliver your application logs to a private Amazon S3 bucket using server-side encryption</strong> is incorrect. Although it is almost similar to the correct option, the architecture did not explicitly say that the CloudHSM is deployed to multiple Availability Zones, which means that this architecture is not highly available compared with the correct option.</p><p>We deliberately designed the question to have this sort of ambiguity to make it more challenging as we know how hard the actual AWS SA Professional exam is. So in this case, the correct option does not say if it is using SSE-S3 or SSE-C. This is one of the trick part of this question. If it is using SSE-S3, then the current correct answer would definitely be wrong because AWS will be managing the AES-256 key.</p><p>However, if it uses SSE-C, then the AES-256 key would be managed by the authorized government employees only. Therefore, it is implied that the correct option is using SSE-C, instead of SSE-S3.</p><p>Remember that in SSE-C, when you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. Amazon S3 does not store the encryption key you provide. Instead, they store a randomly salted HMAC value of the encryption key in order to validate future requests. The salted HMAC value cannot be used to derive the value of the encryption key or to decrypt the contents of the encrypted object. That means, if you lose the encryption key, you lose the object.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/\">https://aws.amazon.com/cloudhsm/</a></p><p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A company is developing an online voting application for a photo competition. The infrastructure is deployed in AWS using CloudFormation. The application accepts high-quality images of each contestant and stores them in S3 then records the information about the image as well as the contestant's profile in RDS. After the competition, the CloudFormation stack is not used anymore, and to save costs, the stack can be terminated. The manager instructed the solutions architect to back up the RDS database and the S3 bucket so the data can still be used even after the CloudFormation template is deleted.</p><p>Which of the following options is the MOST suitable solution to fulfill this requirement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set the DeletionPolicy on the RDS resource to <code>snapshot</code> and set the S3 bucket to <code>retain</code>.</p>"
			},
			{
				"correct": false,
				"answer": "Set the DeletionPolicy on the S3 bucket to snapshot."
			},
			{
				"correct": false,
				"answer": "Set the DeletionPolicy for the RDS instance to snapshot and then enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects."
			},
			{
				"correct": false,
				"answer": "<p>Set the DeletionPolicy to <code>retain</code> on both the RDS and S3 resource types on the CloudFormation template.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With the <strong>DeletionPolicy</strong> attribute you can preserve, and in some cases, backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.</p><p>This capability also applies to stack update operations that lead to resources being deleted from stacks. For example, if you remove the resource from the stack template, and then update the stack with the template. This capability doesn't apply to resources whose physical instance is replaced during stack update operations. For example, if you edit a resource's properties such that CloudFormation replaces that resource during a stack update.</p><p>To keep a resource when its stack is deleted, specify <code>Retain</code> for that resource. You can use retain for any resource. For example, you can retain a nested stack, Amazon S3 bucket, or EC2 instance so that you can continue to use or modify those resources after you delete their stacks.</p><p>There are 3 types of DeletionPolicy Options:</p><p>Delete</p><p>Retain</p><p>Snapshot</p><p>For <code>Delete</code>, CloudFormation deletes the resource and all its contents if applicable during stack deletion. For <code>Retain</code>, CloudFormation keeps the resource without deleting the resource or its contents when its stack is deleted. For <code>Snapshot</code>, CloudFormation creates a snapshot of the resource before deleting it.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_deletionpolicy.JPG\"></p><p>Therefore the correct answer is: <strong>Set the DeletionPolicy on the RDS resource to </strong><code><strong>snapshot</strong></code><strong> and set the S3 bucket to </strong><code><strong>retain</strong></code>. It correctly sets the DeletionPolicy of retain on S3 bucket and snapshot on RDS instance.</p><p>The option that says:<strong> Set the DeletionPolicy for the RDS instance to snapshot and then enable S3 bucket replication on the source bucket to a destination bucket to maintain a copy of all the S3 objects</strong> is incorrect. Even if the images are backed up to another bucket, the original bucket would be deleted if the CloudFormation stack is deleted. Although this option is valid, it is certainly not the most suitable solution because you don't need to back up the data to another region in the first place. You simply have to set the DeletionPolicy of the S3 bucket to <code>retain.</code></p><p>The option that says: <strong>Set the DeletionPolicy to </strong><code><strong>retain</strong></code><strong> on both the RDS and S3 resource types on the CloudFormation template</strong> is incorrect. The DeletionPolicy attribute for RDS should be set to <code>Snapshot</code> and not <code>Retain</code> because with the snapshot option, the backup of the RDS instance would be stored in the form of snapshots. With the retain option, CloudFormation will keep the RDS instance running.</p><p>The option that says: <strong>Set the DeletionPolicy on the S3 bucket to snapshot</strong> is incorrect because the DeletionPolicy of the S3 bucket should be retained, not snapshot.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/\">https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p></div>"
	},
	{
		"question": "<p>A company is hosting its production environment on its on-premises servers. Most of the applications are packed as Docker containers that are manually run on self-managed virtual machines. The web servers are using the latest commercial Oracle Java SE suite which costs the company thousands of dollars in licensing costs. The MySQL databases are installed on separate servers configured on a master-slave setup for high availability. The company wants to migrate the whole environment to AWS Cloud to take advantage of its flexibility and agility, as well as use OpenJDK to save licensing costs without major changes in their applications.</p><p>Which of the following application migration strategies meet this requirement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Re-platform the environment on the AWS Cloud platform by running the Docker containers on Amazon ECS. Test the new OpenJDK Docker containers and upload them on Amazon Elastic Container Registry. Migrate the MySQL database to Amazon RDS using AWS Database Migration Service.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Retire the current on-premises environment and create a brand new environment on the AWS Cloud platform using AWS AppSync. In this way, you can optimize the application and modify it to be more cost-effective and highly available.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Re-factor/re-architect the environment on AWS Cloud by converting the Docker containers to run on AWS Lambda Functions. Convert the MySQL database to Amazon DynamoDB using the AWS Schema Conversion Tool (AWS SCT) to save on costs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Re-host the environment on the AWS Cloud platform by creating EC2 instances that mirror the current web servers and database servers. Host the Docker instances on Amazon EC2 and test the new OpenJDK Docker containers on these instances. Create a dump of the on-premises MySQL databases and upload it to an Amazon S3 bucket. Launch a new Amazon EC2 instance with a MySQL database and import the data from Amazon S3.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The six most common application migration strategies are:</p><p><strong>Rehosting</strong> — Otherwise known as “lift-and-shift”. Many early cloud projects gravitate toward net new development using cloud-native capabilities, but in a large legacy migration scenario where the organization is looking to scale its migration quickly to meet a business case, applications can be rehosted. Most rehosting can be automated with tools (e.g. CloudEndure Migration, AWS VM Import/Export), although you can do this manually to apply changes on legacy systems to the new cloud platform.</p><p><strong>Replatforming</strong> — Sometimes, this is called “lift-tinker-and-shift.” Here you might make a few cloud (or other) optimizations in order to achieve some tangible benefit, but you aren’t otherwise changing the core architecture of the application. You may be looking to reduce the amount of time you spend managing database instances by migrating to a database-as-a-service platform like Amazon Relational Database Service (Amazon RDS), or migrating your application to a fully managed platform like Amazon Elastic Beanstalk.</p><p><img src=\"https://media.tutorialsdojo.com/sap_replatform.png\"></p><p><strong>Repurchasing</strong> — Moving to a different product. Repurchasing is a move to a SaaS platform. Moving a CRM to Salesforce.com, an HR system to Workday, a CMS to Drupal, etc.</p><p><strong>Refactoring / Re-architecting</strong> — Re-imagining how the application is architected and developed, typically using cloud-native features. This is typically driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment. For example, migrating from a monolithic architecture to a service-oriented (or server-less) architecture to boost agility.</p><p><strong>Retire</strong> — This strategy basically means: \"Get rid of.\" Once you’ve discovered everything in your environment, you might ask each functional area who owns each application and see that some of the applications are no longer used. You can save costs by retiring these applications.</p><p><strong>Retain</strong> — Usually this means “revisit” or do nothing (for now). Maybe you aren’t ready to prioritize an application that was recently upgraded or is otherwise not inclined to migrate some applications. You can retain these applications and revisit your migration strategy.</p><p>Therefore, the correct answer is:<strong> Re-platform the environment on the AWS Cloud platform by running the Docker containers on Amazon ECS. Test the new OpenJDK Docker containers and upload them on Amazon Elastic Container Registry. Migrate the MySQL database to Amazon RDS using AWS Database Migration Service.</strong></p><p>The option that says: <strong>Re-host the environment on the AWS Cloud platform by creating EC2 instances that mirror the current web servers and database servers. Host the Docker instances on Amazon EC2 and test the new OpenJDK Docker images on these instances. Create a dump of the on-premises MySQL databases and upload it to an Amazon S3 bucket. Launch a new Amazon EC2 instance with a MySQL database and import the data from Amazon S3 </strong>is incorrect. Although this is possible, simply re-hosting your applications by mirroring your current on-premises setup does not take advantage of the cloud’s elasticity and agility. A better approach is to use Amazon ECS to run the Docker containers and migrate the MySQL database to Amazon RDS.</p><p>The option that says:<strong> Re-factor/re-architect the environment on AWS Cloud by converting the Docker containers to run on AWS Lambda Functions. Convert the MySQL database to Amazon DynamoDB using the AWS Schema Conversion Tool (AWS SCT) to save on costs </strong>is incorrect because this solution requires major changes on the current application to execute successfully. In addition, there is nothing mentioned in the scenario that warrants the conversion of the MySQL database to Amazon DynamoDB.</p><p>The option that says:<strong> Retire the current on-premises environment and create a brand new environment on the AWS Cloud platform using AWS AppSync. In this way, you can optimize the application and modify it to be more cost-effective and highly available</strong> is incorrect. This will also require a lot of effort since you will be starting from scratch. Moreover, AWS AppSync is just a service that accelerates application development with scalable GraphQL APIs. The best approach for this scenario is to re-platform the environment using Amazon ECS and Amazon RDS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/\">https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/</a></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/214-2/\">https://aws.amazon.com/blogs/enterprise-strategy/214-2/</a></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/considering-a-mass-migration-to-the-cloud/\">https://aws.amazon.com/blogs/enterprise-strategy/considering-a-mass-migration-to-the-cloud/</a></p><p><br></p><p><strong>AWS Migration Strategies Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/?src=udemy\">https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/</a></p></div>"
	},
	{
		"question": "<p>A company wants to implement a multi-account strategy that will be distributed across its several research facilities. There will be approximately 50 teams in total that will need their own AWS accounts. A solution is needed to simplify the DNS management as there is only one team that manages all the domains and subdomains for the whole organization. This means that the solution should allow private DNS to be shared among virtual private clouds (VPCs) in different AWS accounts.</p><p>Which of the following solutions has the LEAST complex DNS architecture and allows all VPCs to resolve the needed domain names?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Create a peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account.</p>"
			},
			{
				"correct": true,
				"answer": "<p>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Set up VPC peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this zone. Programmatically associate the VPCs from other accounts with this hosted zone.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up Direct Connect connections among the VPCs of each account using private virtual interfaces. Ensure that each VPC has the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> set to “FALSE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. Programmatically associate the VPCs from other accounts with this hosted zone.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a VPC peering connection among the VPC of each account. Ensure that the each VPC has the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> set to “TRUE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you create a VPC using Amazon VPC, Route 53 Resolver automatically answers DNS queries for local VPC domain names for EC2 instances (ec2-192-0-2-44.compute-1.amazonaws.com) and records in private hosted zones (acme.example.com). For all other domain names, Resolver performs recursive lookups against public name servers.</p><p>You also can integrate DNS resolution between Resolver and DNS resolvers on your network by configuring forwarding rules. Your network can include any network that is reachable from your VPC, such as the following:</p><p>- The VPC itself</p><p>- Another peered VPC</p><p>- An on-premises network that is connected to AWS with AWS Direct Connect, a VPN, or a network address translation (NAT) gateway</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_resolver_multi_account.jpg\"></p><p>VPC sharing allows customers to share subnets with other AWS accounts within the same AWS Organization. This is a very powerful concept that allows for a number of benefits:</p><p>- Separation of duties: centrally controlled VPC structure, routing, IP address allocation.</p><p>- Application owners continue to own resources, accounts, and security groups.</p><p>- VPC sharing participants can reference security group IDs of each other.</p><p>- Efficiencies: higher density in subnets, efficient use of VPNs and AWS Direct Connect.</p><p>- Hard limits can be avoided, for example, 50 VIFs per AWS Direct Connect connection through simplified network architecture.</p><p>- Costs can be optimized through reuse of NAT gateways, VPC interface endpoints, and intra-Availability Zone traffic.</p><p>Essentially, we can decouple accounts and networks. In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants) that belong to the same organization from AWS Organizations. After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. You can simplify network topologies by interconnecting shared Amazon VPCs using connectivity features, such as AWS PrivateLink, AWS Transit Gateway, and Amazon VPC peering.</p><p>Therefore, the correct answer is: <strong>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Set up VPC peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this zone. Programmatically associate the VPCs from other accounts with this hosted zone.</strong></p><p>The option that says: <strong>Set up Direct Connect connections among the VPCs of each account using private virtual interfaces. Ensure that each VPC has the attributes </strong><code><strong>enableDnsHostnames</strong></code><strong> and </strong><code><strong>enableDnsSupport</strong></code><strong> set to “FALSE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. Programmatically associate the VPCs from other accounts with this hosted zone</strong> is incorrect. Using AWS Direct Connect is not a suitable service to connect the various VPCs. In addition, attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> are set to “TRUE” by default and are needed for VPC resources to query Route 53 zone entries.</p><p>The option that says: <strong>Set up a VPC peering connection among the VPC of each account. Ensure that each VPC has the attributes </strong><code><strong>enableDnsHostnames</strong></code><strong> and </strong><code><strong>enableDnsSupport</strong></code><strong> set to “TRUE”. On Amazon Route 53, create a private hosted zone associated with the central account’s VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account</strong> is incorrect. You won't be able to resolve the hosted private zone entries even if you configure your Route 53 zone NS entry to use the central accounts' DNS servers.</p><p>The option that says: <strong>On AWS Resource Access Manager (RAM), set up a shared services VPC on your central account. Create a peering from this VPC to each VPC on the other accounts. On Amazon Route 53, create a private hosted zone associated with the shared services VPC. Manage all domains and subdomains on this hosted zone. On each of the other AWS Accounts, create a Route 53 private hosted zone and configure the Name Server entry to use the DNS of the central account</strong> is incorrect. Although creating the shared services VPC is a good solution, configuring Route 53 Name Server (NS) records to point to the shared services VPC’s Route 53 is not enough. You need to associate the VPCs from other accounts to the hosted zone on the central account.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-sharing.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/\">https://aws.amazon.com/blogs/networking-and-content-delivery/vpc-sharing-a-new-approach-to-multiple-accounts-and-vpc-management/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/\">https://aws.amazon.com/blogs/security/simplify-dns-management-in-a-multiaccount-environment-with-route-53-resolver/</a></p><p><br></p><p><strong>Check out these Amazon VPC and Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>A company plans to decommission its legacy web application that is hosted in AWS. It is composed of an Auto Scaling group of EC2 instances and an Application Load Balancer (ALB). The new application is built on a new framework. The solutions architect has been tasked to set up a new serverless architecture that is comprised of AWS Lambda, API Gateway, and DynamoDB. In addition, it is required to build a CI/CD pipeline to automate the build process and to support gradual deployments.</p><p>Which is the most suitable way to build, test, and deploy the new architecture in AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use the AWS Serverless Application Repository to organize related components, share configuration such as memory and timeouts between resources, and deploy all related resources together as a single, versioned entity.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Serverless Application Model (AWS SAM) and set up AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline to build a CI/CD pipeline.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a CI/CD pipeline using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline to build the CI/CD pipeline then use AWS Systems Manager Automation to automate the build process and support gradual deployments.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use CloudFormation and OpsWorks for your build, deployment, and configuration management service.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The <strong>AWS Serverless Application Model (AWS SAM)</strong> is an open-source framework that you can use to build serverless applications on AWS. It consists of the AWS SAM template specification that you use to define your serverless applications, and the AWS SAM command line interface (AWS SAM CLI) that you use to build, test, and deploy your serverless applications.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sam_cloudformation.jpg\"></p><p>Because AWS SAM is an extension of AWS CloudFormation, you get the reliable deployment capabilities of AWS CloudFormation. You can define resources by using AWS CloudFormation in your AWS SAM template. Also, you can use the full suite of resources, intrinsic functions, and other template features that are available in AWS CloudFormation.</p><p>You can use AWS SAM with a suite of AWS tools for building serverless applications. To build a deployment pipeline for your serverless applications, you can use CodeBuild, CodeDeploy, and CodePipeline. You can also use AWS CodeStar to get started with a project structure, code repository, and a CI/CD pipeline that's automatically configured for you. To deploy your serverless application, you can use the Jenkins plugin, and you can use Stackery.io's toolkit to build production-ready applications.</p><p>Therefore the correct answer is: <strong>Use AWS Serverless Application Model (AWS SAM) and set up AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline to build a CI/CD pipeline.</strong></p><p>The options that says: <strong>Use CloudFormation and OpsWorks for your build, deployment, and configuration management service</strong> is incorrect. OpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments. It can't deploy AWS Lambda applications.</p><p>The options that says: <strong>Set up a CI/CD pipeline using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline to build the CI/CD pipeline then use AWS Systems Manager Automation to automate the build process and support gradual deployments</strong> is incorrect. Systems Manager Automation is designed to configure and manage instances with custom runbooks or pre-defined runbooks maintained by AWS, not for building and deploying serverless applications on AWS Lambda.</p><p>The options that says: <strong>Use the AWS Serverless Application Repository to organize related components, share configuration such as memory and timeouts between resources, and deploy all related resources together as a single, versioned entity</strong> is incorrect. AWS Serverless Application Repository is just a managed repository for serverless applications. This solution is incomplete as you will need other AWS tools to build and deploy your application.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html</a></p><p><a href=\"https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\">https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html</a></p><p><br></p><p><strong>Check out this AWS Serverless Application Model Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-serverless-application-model-sam/?src=udemy\">https://tutorialsdojo.com/aws-serverless-application-model-sam/</a></p></div>"
	},
	{
		"question": "<p>A company has recently adopted a hybrid cloud architecture which requires them to migrate their databases from their on-premises data center to AWS. One of their applications requires a heterogeneous database migration in which they need to transform their on-premises Oracle database to PostgreSQL. A schema and code transformation should be done first in order to successfully migrate the data. </p><p>Which of the following options is the most suitable approach to migrate the database in AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use the AWS Serverless Application Model (SAM) service to transform your database to PostgreSQL using AWS Lambda functions. Migrate the database to RDS using the AWS Database Migration Service (DMS).</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the AWS Schema Conversion Tool (SCT) to convert the source schema to match that of the target database. Migrate the data using the AWS Database Migration Service (DMS) from the source database to an Amazon RDS for PostgreSQL database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a combination of AWS Data Pipeline service and CodeCommit to convert the source schema and code to match that of the target PostgreSQL database in RDS. Use AWS Batch with Spot EC2 instances to cost-effectively migrate the data from the source database to the target database in a batch process.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the database from your on-premises data center using the AWS Server Migration Service (SMS). Afterward, use the AWS Database Migration Service to convert and migrate your data to Amazon RDS for PostgreSQL database.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Database Migration Service</strong> helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.</p><p>AWS Database Migration Service can migrate your data to and from most of the widely used commercial and open source databases. It supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora. Migrations can be from on-premises databases to Amazon RDS or Amazon EC2, databases running on EC2 to RDS, or vice versa, as well as from one RDS database to another RDS database. It can also move data between SQL, NoSQL, and text-based targets.</p><p><img src=\"https://media.tutorialsdojo.com/sap_db_migration.JPG\"></p><p>In heterogeneous database migrations, the source and target databases engines are different, like in the case of Oracle to Amazon Aurora, Oracle to PostgreSQL, or Microsoft SQL Server to MySQL migrations. In this case, the schema structure, data types, and database code of source and target databases can be quite different, requiring a schema and code transformation before the data migration starts. That makes heterogeneous migrations a two-step process.</p><p><strong>First, use the AWS Schema Conversion Tool to convert the source schema and code to match that of the target database, and then use the AWS Database Migration Service to migrate data from the source database to the target database.</strong> All the required data type conversions will automatically be done by the AWS Database Migration Service during the migration. The source database can be located in your own premises outside of AWS, running on an Amazon EC2 instance, or it can be an Amazon RDS database. The target can be a database in Amazon EC2 or Amazon RDS.</p><p>The option that says: <strong>Migrate the database from your on-premises data center using the AWS Server Migration Service (SMS). Afterwards, use the AWS Database Migration Service to convert and migrate your data to Amazon RDS for PostgreSQL database</strong> is incorrect because the AWS Server Migration Service (SMS) is primarily used to migrate virtual machines such as VMware vSphere and Windows Hyper-V. Although it is correct to use AWS Database Migration Service (DMS) to migrate the database, this option is still wrong because you should use the AWS Schema Conversion Tool to convert the source schema.</p><p>The option that says: <strong>Use a combination of AWS Data Pipeline service and CodeCommit to convert the source schema and code to match that of the target PostgreSQL database in RDS. Use AWS Batch with Spot EC2 instances to cost-effectively migrate the data from the source database to the target database in a batch process</strong> is incorrect. AWS Data Pipeline is primarily used to quickly and easily provision pipelines that remove the development and maintenance effort required to manage your daily data operations which lets you focus on generating insights from that data. Although you can use this to connect your data on your on-premises data center, it is not the most suitable service to use, compared with AWS DMS.</p><p>The option that says: <strong>Use the AWS Serverless Application Model (SAM) service to transform your database to PostgreSQL using AWS Lambda functions. Migrate the database to RDS using the AWS Database Migration Service (DMS)</strong> is incorrect. The Serverless Application Model (SAM) is an open-source framework that is primarily used to build serverless applications on AWS, and not for database migration.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/dms/\">https://aws.amazon.com/dms/</a></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><br></p><p><strong>Check out these AWS Migration Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><a href=\"https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/?src=udemy\">https://tutorialsdojo.com/aws-migration-strategies-the-6-rs/</a></p></div>"
	},
	{
		"question": "<p>A company runs hundreds of Windows-based Amazon EC2 instances on AWS. The Solutions Architect has been assigned to develop a workflow to ensure that the required patches of all Windows EC2 instances are properly identified and applied automatically. To maintain their system uptime requirements, it is of utmost importance to ensure that the EC2 instance reboots do not occur at the same time on all of their Windows instances. This is to avoid any loss of revenue that could be caused by any unavailability issues of their systems.</p><p>Which of the following will meet the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code> baseline on both patch groups. Set up two non-overlapping maintenance windows and associate each with a different patch group. Using Patch Group tags, register targets with specific maintenance windows and lastly, assign the <code>AWS-RunPatchBaseline</code> document as a task within each maintenance window which has a different processing start time.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code><strong> </strong>baseline on your patch group. Set up a maintenance window and associate it with your patch group. Assign the <code>AWS-RunPatchBaseline</code><strong> </strong>document as a task within your maintenance window.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code><strong> </strong>baseline on both patch groups. Create a CloudWatch Events rule configured to use a cron expression to automate the execution of patching in a given schedule using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined <code>AWS-DefaultPatchBaseline</code><strong> </strong>baseline on both patch groups. Create two CloudWatch Events rules which are configured to use a cron expression to automate the execution of patching for the two Patch Groups using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications.</p><p>You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\"></p><p>Patch Manager uses <em>patch baselines</em>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. You can add tags to your patch baselines themselves when you create or update them.</p><p>You can use a <em>patch group</em> to associate instances with a specific patch baseline. Patch groups help ensure that you are deploying the appropriate patches, based on the associated patch baseline rules, to the correct set of instances. Patch groups can also help you avoid deploying patches before they have been adequately tested. For example, you can create patch groups for different environments (such as Development, Test, and Production) and register each patch group to an appropriate patch baseline.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_patch_group.png\"></p><p>When you run <code>AWS-RunPatchBaseline</code>, you can target managed instances using their instance ID or tags. SSM Agent and Patch Manager will then evaluate which patch baseline to use based on the patch group value that you added to the instance.</p><p>You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group <em>must</em> be defined with the tag key: <strong>Patch Group</strong>. Note that the key is case-sensitive. You can specify any value, for example, \"web servers,\" but the key must be <strong>Patch Group</strong>.</p><p>The <code>AWS-DefaultPatchBaseline</code> baseline is primarily used to approve all Windows Server operating system patches that are classified as \"CriticalUpdates\" or \"SecurityUpdates\" and that have an MSRC severity of \"Critical\" or \"Important\". Patches are auto-approved seven days after release.</p><p>Hence, the option that says: <strong>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Set up two non-overlapping maintenance windows and associate each with a different patch group. Using Patch Group tags, register targets with specific maintenance windows and lastly, assign the </strong><code><strong>AWS-RunPatchBaseline</strong></code><strong> document as a task within each maintenance window which has a different processing start time</strong> is the correct answer as it properly uses two Patch Groups, non-overlapping maintenance windows and the <code>AWS-DefaultPatchBaseline</code> baseline to ensure that the EC2 instance reboots do not occur at the same time.</p><p>The option that says: <strong>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on your patch group. Set up a maintenance window and associate it with your patch group. Assign the </strong><code><strong>AWS-RunPatchBaseline</strong></code><strong> document as a task within your maintenance window</strong> is incorrect. Although it is correct to use a Patch Group, you must create another Patch Group to avoid any unavailability issues. Having two non-overlapping maintenance windows will ensure that there will be another set of running Windows EC2 instances while the other set is being patched.</p><p>The option that says: <strong>Create two Patch Groups with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Create two CloudWatch Events rules which are configured to use a cron expression to automate the execution of patching for the two Patch Groups using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution</strong> is incorrect. The AWS Systems Manager Run Command is primarily used to remotely manage the configuration of your managed instances while AWS Systems Manager State Manager is just a configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. These two services, including CloudWatch Events, are not suitable to be used in this scenario. The better solution would be to use AWS Systems Manager Maintenance Windows which lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches.</p><p>The option that says: <strong>Create a Patch Group with unique tags that you will assign to all of your EC2 Windows Instances. Associate the predefined </strong><code><strong>AWS-DefaultPatchBaseline</strong></code><strong> baseline on both patch groups. Create a CloudWatch Events rule configured to use a cron expression to automate the execution of patching in a given schedule using the AWS Systems Manager Run command. Set up an AWS Systems Manager State Manager document to define custom commands which will be executed during patch execution</strong> is incorrect. Just as what is mentioned in the above, you have to use Maintenance Windows for scheduling the patches and you also need to set up two Patch Groups in this scenario instead of one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-ssm-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-patch-scheduletasks.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A multi-national tech company has multiple VPCs assigned for each of its IT departments. VPC peering has been set up whenever intercommunication is needed between the VPCs. The solutions architect has been instructed to launch a new central database server that can be accessed by the other VPCs of the company using the <code>database.tutorialsdojo.com</code> domain name. This server should only be resolvable and accessible within the associated VPCs since only internal applications will be using the database.</p><p>Which of the following options should the solutions architect implement to meet the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the Elastic IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>true</code> and the <code>enableDnsSupport</code> attribute to <code>false</code></p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>true</code> and the <code>enableDnsSupport</code> attribute to <code>true</code></p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>true</code> and the <code>enableDnsSupport</code> attribute to <code>true</code></p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create a CNAME record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the <code>enableDnsHostNames</code> attribute of your VPC to <code>false</code> and the <code>enableDnsSupport</code> attribute to <code>false</code></p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In AWS, a hosted zone is a container for records, and records contain information about how you want to route traffic for a specific domain, such as tutorialsdojo.com, and its subdomains (portal.tutorialsdojo.com, database.tutorialsdojo.com). A hosted zone and the corresponding domain have the same name. There are two types of hosted zones:</p><p><em>- </em><strong><em>Public hosted zones</em></strong> contain records that specify how you want to route traffic on the internet.</p><p><em>- </em><strong><em>Private hosted zones</em></strong> contain records that specify how you want to route traffic in an Amazon VPC</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_vpc_association.png\"></p><p>A <em>private hosted zone</em> is a container that holds information about how you want Amazon Route 53 to respond to DNS queries for a domain and its subdomains within one or more VPCs that you create with the Amazon VPC service. Your VPC has attributes that determine whether your EC2 instance receives public DNS hostnames, and whether DNS resolution through the Amazon DNS server is supported.</p><p><code><strong>enableDnsHostnames</strong> - </code>Indicates whether the instances launched in the VPC get public DNS hostnames. If this attribute is <code>true</code>, instances in the VPC get public DNS hostnames, but only if the <code>enableDnsSupport</code> attribute is also set to <code>true</code>.</p><p><code><strong>enableDnsSupport</strong> - </code>Indicates whether the DNS resolution is supported for the VPC. If this attribute is <code>false</code>, the Amazon-provided DNS server in the VPC that resolves public DNS hostnames to IP addresses is not enabled. If this attribute is <code>true</code>, queries to the Amazon provided DNS server at the 169.254.169.253 IP address, or the reserved IP address at the base of the VPC IPv4 network range plus two ( *.*.*.2 ) will succeed.</p><p>Hence, the option that says: <strong>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>true</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>true</strong></code> is the correct answer.</p><p>The options that say:</p><p><strong>1. Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>true</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>true</strong></code></p><p><strong>2. Set up a public hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create a CNAME record with a value of database.tutorialsdojo.com which maps to the IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>false</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>false</strong></code></p><p>are incorrect because you have to create a <strong><em>private</em></strong> hosted zone and not a public one, since the database server will only be accessed by the associated VPCs and not publicly over the Internet. In addition, you have to create an A record for your database server and then set both the <code><strong>enableDnsHostNames</strong></code> and <code><strong>enableDnsSupport</strong></code> attributes to <code>true</code>.</p><p>The option that says: <strong>Set up a private hosted zone with a domain name of tutorialsdojo.com and specify the VPCs that you want to associate with the hosted zone. Create an A record with a value of database.tutorialsdojo.com which maps to the Elastic IP address of the EC2 instance of your database server. Modify the </strong><code><strong>enableDnsHostNames</strong></code><strong> attribute of your VPC to </strong><code><strong>true</strong></code><strong> and the </strong><code><strong>enableDnsSupport</strong></code><strong> attribute to </strong><code><strong>false</strong></code> is incorrect. Even though it mentions the use of a private hosted zone, the configuration is incorrect since it is required to set both the <code><strong>enableDnsHostNames</strong></code> and <code><strong>enableDnsSupport</strong></code> attributes of your VPC to <code>true</code>. In addition, an Elastic IP address is a public IPv4 address, which is reachable from the Internet and hence, it violates the requirement that the database server should only be accessible within your associated VPCs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-support\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-support</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/hosted-zones-private.html</a></p><p><br></p><p><strong>Check out these Amazon VPC and Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>An electronics company has an on-premises network as well as a cloud infrastructure in AWS. The on-site data storage which is used by their enterprise document management system is heavily being used, and <strong>they are looking at utilizing the storage services in AWS for cost-effective backup and rapid disaster recovery</strong>. You are tasked to set up a storage solution that will provide a low-latency access to the enterprise document management system. Most of the documents uploaded in their system are printed circuit board (PCB) designs and schematic diagrams which are frequently used and accessed by their engineers, QA analysts, and their Research and Design department. Hence, you also have to ensure that these employees can access the entire dataset quickly, without sacrificing durability. </p><p>How can you satisfy the requirement for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an S3 bucket and use the <code>sync</code> command to synchronize the data to and from your on-premises file server.</p>"
			},
			{
				"correct": true,
				"answer": "Use a Stored Volume Gateway to provide cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI) devices from your on-premises application servers."
			},
			{
				"correct": false,
				"answer": "Use a Cached volume gateway to retain low-latency access to your entire data set as well as your frequently accessed data."
			},
			{
				"correct": false,
				"answer": "In AWS Storage Gateway, create a File gateway that enables you to store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the electronics company is looking for a storage service which can locally store the entire dataset and provide easier and faster access without sacrificing durability. This requirement can be fulfilled by setting up a Stored Volume gateway in AWS Storage Gateway.</p><p>By using stored volumes, you can store your primary data locally, while asynchronously back up that data to AWS. Stored volumes provide your on-premises applications with low-latency access to the entire datasets. At the same time, they provide durable, offsite backups. You can create storage volumes and mount them as iSCSI devices from your on-premises application servers. Data written to your stored volumes are stored on your on-premises storage hardware. These data are asynchronously backed up to Amazon S3 as Amazon Elastic Block Store (Amazon EBS) snapshots.</p><p>Hence, the correct answer is the option that says: <strong>Use a Stored Volume Gateway to provide cloud-backed storage volumes that you can mount as Internet Small Computer System Interface (iSCSI) devices from your on-premises application servers.</strong></p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_gateway_stored.png\"></p><p><strong>Creating an S3 bucket and using the </strong><code><strong>sync</strong></code><strong> command to synchronize the data to and from your on-premises file server</strong> is incorrect because S3 is not a preferred AWS storage service for supporting hybrid networks for this scenario. Although S3 is highly scalable, it would not be able to handle the integration needed for the on-premises document management system. It is still better to use Stored Volumes in AWS Storage Gateway instead.</p><p><strong>Using a Cached volume gateway to retain low-latency access to your entire data set as well as your frequently accessed data</strong> is incorrect because Cached volume gateway provides you low-latency access to your frequently accessed data but not to the entire data.</p><p>The option that says: <strong>In AWS Storage Gateway, create a File gateway that enables you to store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB) </strong>is incorrect because a File gateway does not provide you the required low-latency access to the entire dataset that the application needs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#volume-gateway-concepts\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#volume-gateway-concepts</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p></div>"
	},
	{
		"question": "<p>A company hosts its online delivery system on a fleet of EC2 instances deployed in multiple Availability Zones in the ap-southeast-1 region. The instances are behind an Application Load Balancer that evenly distributes the load. The system is using a MySQL RDS instance to store the deliveries and transactions of the system. To ensure business continuity, you are instructed to set up a disaster recovery system in which the RTO must be less than 3 hours and the RPO is 15 minutes when a system outage occurs. A system should also be implemented that can automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property in your data store.</p><p>As the Solutions Architect, which disaster recovery strategy should you use to achieve the required RTO and RPO targets in the most cost-effective manner?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up asynchronous replication in the database using a Multi-AZ deployments configuration. Use AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property from your RDS database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Schedule 15-minute DB backups to Amazon Glacier. Store the transaction logs to an S3 bucket every 5 minutes. Use Amazon Macie to automatically discover, classify, and protect your sensitive data.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Schedule a database backup to an S3 bucket every hour and store transaction logs to a separate S3 bucket every 5 minutes. Use Amazon Macie to automatically discover, classify, and protect your sensitive data.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Schedule a database backup to AWS Storage Gateway every hour and store transaction logs to a separate S3 bucket every 5 minutes. Use AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property on your Storage Gateway.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Recovery time objective (RTO)</strong> is the time it takes after a disruption to restore a business process to its service level, as defined by the operational level agreement (OLA). For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM.</p><p><strong>Recovery point objective (RPO)</strong> is the acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before 11:00 AM. Data loss will span only one hour, between 11:00 AM and 12:00 PM (noon).</p><p><strong>Amazon S3</strong> is an ideal destination for backup data that might be needed quickly to perform a restore. Transferring data to and from Amazon S3 is typically done through the network, and is therefore accessible from any location.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_mysql_backup_s3.png\"></p><p><strong>Amazon Macie</strong> is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property, and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved. The fully managed service continuously monitors data access activity for anomalies, and generates detailed alerts when it detects risk of unauthorized access or inadvertent data leaks.</p><p>Hence, the option that says:<em> </em><strong>Schedule a database backup to an S3 bucket every hour and store transaction logs to a separate S3 bucket every 5 minutes. Use Amazon Macie to automatically discover, classify, and protect your sensitive data</strong> is correct because by using an S3 bucket, it makes data retrievals of the backups quicker. Since the transaction logs are stored in S3 every 5 minutes, this will help to restore the application to a state that is within the required RPO of 15 minutes.</p><p><strong>Scheduling 15-minute DB Backups to Amazon Glacier and storing the transaction logs to an S3 bucket every 5 minutes, and using Amazon Macie to automatically discover, classify, and protect your sensitive data</strong> is incorrect because retrieving the database backups from Amazon Glacier archives will normally take around 3 - 5 hours using the Standard Retrievals and hence, this will not meet the RTO and RPO. Although you can use expedited retrievals, which can typically retrieve your archive within 1 – 5 minutes, this will entail additional cost and hence, not a cost-effective solution.</p><p><strong>Setting up asynchronous replication in the database using a Multi-AZ deployments configuration and using AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property from your RDS database</strong> is incorrect because asynchronous replication is only applicable for Read Replicas and not for Multi-AZ deployments configuration. Although this will improve the availability of the RDS database, it won't provide a better RTO or RPO, especially in the event of a regional outage since you can't export a standby instance to another region. In addition, you have to use AWS Macie to protect your sensitive data in Amazon S3 and not AWS Shield.</p><p><strong>Scheduling a database backup to AWS Storage Gateway every hour and storing transaction logs to a separate S3 bucket every 5 minutes and using AWS Shield to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property on your Storage Gateway<em> </em></strong>is incorrect. AWS Storage Gateway is primarily used for hybrid cloud storage service that connects your existing on-premises environments with the AWS Cloud. Although this can be a valid option, the scenario did not say that their architecture is hybrid or that they are using an on-premises data center. AWS Shield is primarily used to protect your resources from DDoS attacks, and not to automatically discover, classify, and protect any personally identifiable information (PII) or intellectual property.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws\">https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-macie/?src=udemy\">https://tutorialsdojo.com/amazon-macie/</a></p></div>"
	},
	{
		"question": "<p>A tech startup is planning to launch a new global mobile marketplace using AWS Amplify and AWS Mobile Hub. To lower the latency, the backend APIs will be launched to multiple AWS regions to process the sales and financial transactions in the region closest to the users. The solutions architect is instructed to design the system architecture to ensure that the transactions made in one region are automatically replicated to other regions. In the coming months ahead, it is expected that the marketplace will have millions of users across North America, South America, Europe, and Asia.</p><p>Which of the following is the most scalable, cost-effective, and highly available architecture that you should implement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create a Global DynamoDB table with replica tables across several AWS regions that you prefer. In each local region, store the individual transactions to a DynamoDB replica table in the same region. Any changes made in one of the replica tables will automatically be replicated across all other tables.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a combination of AWS Control Tower and Amazon Connect to launch and centrally manage multiple DynamoDB tables in various AWS Regions. In each local region, store the individual transactions to a DynamoDB replica table in the same region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>In each local region, store the individual transactions to a DynamoDB table. Set up an AWS Lambda function to read recent writes from the table, and replay the data to DynamoDB tables in all other regions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon Aurora Multi-Master database on all required regions. Store the individual transactions to the Amazon Aurora instance in the local region. Replicate the transactions table between regions using Aurora replication. In this set up, any changes made in one of the tables will be automatically replicated across all other tables.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Global Tables</strong> builds upon DynamoDB’s global footprint to provide you with a fully managed, multi-region, and multi-master database that provides fast, local, read and write performance for massively scaled, global applications. Global Tables replicates your Amazon DynamoDB tables automatically across your choice of AWS regions.</p><p><strong>Global Tables</strong> eliminates the difficult work of replicating data between regions and resolving update conflicts, enabling you to focus on your application’s business logic. In addition, Global Tables enables your applications to stay highly available even in the unlikely event of isolation or degradation of an entire region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\"></p><p>Therefore, the correct answer is:<strong> Create a Global DynamoDB table with replica tables across several AWS regions that you prefer. In each local region, store the individual transactions to a DynamoDB replica table in the same region. Any changes made in one of the replica tables will automatically be replicated across all other tables.</strong></p><p>The option that says: <strong>In each local region, store the individual transactions to a DynamoDB table. Set up an AWS Lambda function to read recent writes from the table, and replay the data to DynamoDB tables in all other regions</strong> is incorrect. Using an AWS Lambda function to replicate all data across regions is not a scalable solution. Remember that there will be millions of customers who will use the mobile app around the world, and this entails a lot of replication and compute capacity for a single Lambda function. In this scenario, the best solution is to use Global DynamoDB tables with DynamoDB Stream option enabled to automatically handle the replication process.</p><p>The option that says:<strong> Use a combination of AWS Control Tower and Amazon Connect to launch and centrally manage multiple DynamoDB tables in various AWS Regions. In each local region, store the individual transactions to a DynamoDB replica table in the same region</strong> is incorrect. Amazon Connect is just an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost, while AWS Control Tower just offers the easiest way to set up and govern a new, secure, multi-account AWS environment. You can't use these two services to set up a Global DynamoDB Table.</p><p>The option that says: <strong>Create an Amazon Aurora Multi-Master database on all required regions. Store the individual transactions to the Amazon Aurora instance in the local region. Replicate the transactions table between regions using Aurora replication. In this setup, any changes made in one of the tables will be automatically replicated across all other tables</strong> is incorrect. By default, all DB instances in a multi-master cluster must be in the same AWS Region and you can't enable cross-region replicas from multi-master clusters. In addition, DynamoDB provides better global scalability for mobile applications compared to Amazon Aurora.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItWorks.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A company runs several clusters of Amazon EC2 instances in AWS. An unusual API activity and port scanning in the VPC have been identified by the security team. They noticed that there are multiple port scans being triggered to the EC2 instances from a specific IP address. To fix the issue immediately, the solutions architect has decided to simply block the offending IP address. The solutions architect is also instructed to fortify their existing cloud infrastructure security from the most frequently occurring network and transport layer DDoS attacks.</p><p>Which of the following is the most suitable method to satisfy the above requirement in AWS?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Deny access from the IP Address block in the Network ACL. Use AWS Shield Advanced to protect your cloud resources.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Change the Windows Firewall settings to deny access from the IP address block. Use Amazon GuardDuty to detect potentially compromised instances or reconnaissance by attackers, and AWS Systems Manager Patch Manager to properly apply the latest security patches to all of your instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Block the offending IP address using Route 53. Use Amazon Macie to automatically discover, classify, and protect sensitive data in AWS, including DDoS attacks.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Deny access from the IP Address block by adding a specific rule to all of the Security Groups. Use a combination of AWS WAF and AWS Config to protect your cloud resources against common web attacks.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Shield</strong> is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection. There are two tiers of AWS Shield - Standard and Advanced.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_shield.png\"></p><p>All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications. When you use AWS Shield Standard with Amazon CloudFront and Amazon Route 53, you receive comprehensive availability protection against all known infrastructure (Layer 3 and 4) attacks.</p><p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC. A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic. Network ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p><p>Therefore the correct answer is: <strong>Deny access from the IP Address block in the Network ACL. Use AWS Shield Advanced to protect your cloud resources.</strong></p><p>The option that says: <strong>Block the offending IP address using Route 53. Use Amazon Macie to automatically discover, classify, and protect sensitive data in AWS, including DDoS attacks</strong> is incorrect because Amazon Macie is just a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. It does not provide security against DDoS attacks. In addition, you cannot block the offending IP address using Route 53. You should use Network ACL for this scenario.</p><p>The option that says: <strong>Change the Windows Firewall settings to deny access from the IP address block. Use Amazon GuardDuty to detect potentially compromised instances or reconnaissance by attackers, and AWS Systems Manager Patch Manager to properly apply the latest security patches to all of your instances</strong> is incorrect. You have to use Network ACL to block the specific IP address to your network and not just change the firewall of your Windows server. Amazon GuardDuty and AWS Systems Manager Patch Manager are not suitable to fortify your AWS Cloud against DDoS attacks.</p><p>The option that says:<strong> Deny access from the IP Address block by adding a specific rule to all of the Security Groups. Use a combination of AWS WAF and AWS Config to protect your cloud resources against common web attacks</strong> is incorrect because it is still better to block the offending IP address on the Network ACL level as you cannot directly deny an IP address in your Security Group. AWS WAF and AWS Config are helpful to improve the security of your cloud infrastructure in AWS but these services are not enough to protect your infrastructure against DDoS attacks. You have to use AWS Shield Advanced in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://aws.amazon.com/shield/\">https://aws.amazon.com/shield/</a></p><p><br></p><p><strong>Check out these AWS WAF and AWS Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/aws-shield/</a></p></div>"
	},
	{
		"question": "<p>A company hosts its online retail store on AWS. The web application is hosted on an Auto Scaling group of Amazon EC2 instances and the database is an Amazon RDS for MySQL DB instance. The database is a db.m5.2xlarge instance type with a 100GB GP2 SSD storage, which is enough overhead to hold the current database size. The website runs smoothly for a month and the company decided to hold a 3-day promotional event to drive more customers to the website. On the second day of the event, users are experiencing long wait times and request time-outs. Upon checking Amazon CloudWatch metrics, the solutions architect noticed long response times on the DB instance, however, the DB CPU and Memory metrics are hovering at around 50% only, and enough disk space is still available. The application logs do not indicate possible connectivity issues to the database.</p><p>Which of the following option is the most likely cause of this issue?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>The application requests a lot of table update/changes which require indexes to be rebuilt to optimize queries. This takes a lot of time for the database to perform which results in longer response times.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The DB instance has reached the maximum number of allowed simultaneous connections. Add read replicas on the DB cluster to handle the spike in load.</p>"
			},
			{
				"correct": true,
				"answer": "<p>The provisioned low disk size has a very low IOPS. The application exhausted all the available burst I/O credit balance due to increased traffic.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The DB instance performance is limited by the network bandwidth of the small instance. Upgrade to a high instance type to have more network capacity to handle the load.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>General Purpose SSD</strong> storage performance is governed by volume size, which dictates the base performance level of the volume and how quickly it accumulates I/O credits. Larger volumes have higher base performance levels and accumulate I/O credits faster. I/O credits represent the available bandwidth that your General Purpose SSD storage can use to burst large amounts of I/O when more than the base level of performance is needed. The more I/O credits your storage has for I/O, the more time it can burst beyond its base performance level and the better it performs when your workload requires more performance.</p><p>When using General Purpose SSD storage, your DB instance receives an initial I/O credit balance of 5.4 million I/O credits. This initial credit balance is enough to sustain a burst performance of 3,000 IOPS for 30 minutes. This balance is designed to provide a fast initial boot cycle for boot volumes and to provide a good bootstrapping experience for other applications. Volumes earn I/O credits at the baseline performance rate of 3 IOPS for each GiB of volume size. For example, a 100-GiB SSD volume has a baseline performance of 300 IOPS.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ebs_iops.png\"></p><p>If your <code>gp2</code> volume uses all of its I/O credit balance, the maximum IOPS performance of the volume remains at the baseline IOPS performance level (the rate at which your volume earns credits) and the volume's maximum throughput is reduced to the baseline IOPS multiplied by the maximum I/O size. Throughput can never exceed 250 MiB/s.</p><p><strong>Provisioned IOPS SSD (</strong><code><strong>io1</strong></code><strong> and </strong><code><strong>io2</strong></code><strong>) volumes</strong> are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency. Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume, and Amazon EBS delivers the provisioned performance 99.9 percent of the time. These volumes are recommended for high-performance database instances.</p><p>Therefore, the correct answer is: <strong>The provisioned low disk size has a very low IOPS. The application exhausted all the available burst I/O credit balance due to increased traffic.</strong> Since the issues occurred on the second day, this could indicate that the burst I/O credits have been exhausted for the <code>gp2</code> volume. It is recommended that you use Provisioned IOPS SSD for high-performance database instances.</p><p>The option that says: <strong>The application requests a lot of table update/changes which require indexes to be rebuilt to optimize queries. This takes a lot of time for the database to perform which results in longer response times</strong> is incorrect. Updating tables may impact the database performance but if this is true, then we should see significantly increased CPU or RAM usage on the RDS instance.</p><p>The option that says: <strong>The DB instance is has reached the maximum number of allowed simultaneous connections. Add read replicas on the DB cluster to handle the spike in load</strong> is incorrect. If the maximum number of allowed connections is reached, then the application logs should indicate that the database refused connections or did not respond to requests.</p><p>The option that says: <strong>The DB instance performance is limited by the network bandwidth of the small instance. Upgrade to a high instance type to have more network capacity to handle the load</strong> is incorrect. A db.m5.2xlarge instance can have up to 10Gbps of network bandwidth. This is higher than the bandwidth of the attached volume. Therefore the network interface of the instance should still be able to handle the load.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><a href=\"https://aws.amazon.com/blogs/database/understanding-burst-vs-baseline-performance-with-amazon-rds-and-gp2/\">https://aws.amazon.com/blogs/database/understanding-burst-vs-baseline-performance-with-amazon-rds-and-gp2/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html</a></p><p><a href=\"https://aws.amazon.com/blogs/database/how-to-use-cloudwatch-metrics-to-decide-between-general-purpose-or-provisioned-iops-for-your-rds-database/\">https://aws.amazon.com/blogs/database/how-to-use-cloudwatch-metrics-to-decide-between-general-purpose-or-provisioned-iops-for-your-rds-database/</a></p><p><br></p><p><strong>Amazon EBS Overview - SSD vs HDD:</strong></p><p><a href=\"https://youtu.be/LW7x8wyLFvw\">https://youtu.be/LW7x8wyLFvw</a></p></div>"
	},
	{
		"question": "<p>A company is planning to build its new customer relationship management (CRM) portal in AWS. The application architecture will be using a containerized microservices hosted on an Amazon ECS cluster. A Solutions Architect has been tasked to set up the architecture and comply with the AWS security best practice of granting the least privilege. The architecture should also support the use of security groups and standard network monitoring tools at the container level to comply with the company’s strict IT security policies. </p><p>Which of the following provides the MOST secure configuration for the CRM portal?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use the <code>awsvpc</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then pass IAM credentials into the container at launch time to access other AWS resources.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the <code>bridge</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then use IAM roles for tasks to access other resources.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the <code>bridge</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to Amazon EC2 instances then use IAM roles for EC2 instances to access other resources.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the <code>awsvpc</code> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then use IAM roles for tasks to access other resources.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Task definitions</strong> are split into separate parts: the task family, the IAM task role, the network mode, container definitions, volumes, task placement constraints, and launch types. The family and container definitions are required in a task definition, while task role, network mode, volumes, task placement constraints, and launch type are optional.</p><p>You can configure various Docker networking modes that will be used by containers in your ECS task. The valid values are <code>none</code>, <code>bridge</code>, <code>awsvpc</code>, and <code>host</code>. The default Docker network mode is <code>bridge</code>.</p><p>With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances. Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or <code>RunTask</code> API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ecs_task_definition.png\"></p><p>If the network mode is set to <code><strong>none</strong></code>, the task's containers do not have external connectivity and port mappings can't be specified in the container definition.</p><p>If the network mode is <code><strong>bridge</strong></code>, the task utilizes Docker's built-in virtual network which runs inside each container instance.</p><p>If the network mode is <code><strong>host</strong></code>, the task bypasses Docker's built-in virtual network and maps container ports directly to the EC2 instance's network interface directly. In this mode, you can't run multiple instantiations of the same task on a single container instance when port mappings are used.</p><p>If the network mode is <code><strong>awsvpc</strong></code>, the task is allocated an elastic network interface, and you must specify a <code>NetworkConfiguration</code> when you create a service or run a task with the task definition. When you use this network mode in your task definitions, every task that is launched from that task definition gets its own elastic network interface (ENI) and a primary private IP address. The task networking feature simplifies container networking and gives you more control over how containerized applications communicate with each other and other services within your VPCs.</p><p>Task networking also provides greater security for your containers by allowing you to use security groups and network monitoring tools at a more granular level within your tasks. Because each task gets its own ENI, you can also take advantage of other Amazon EC2 networking features like VPC Flow Logs so that you can monitor traffic to and from your tasks. Additionally, containers that belong to the same task can communicate over the <code>localhost</code> interface. A task can only have one ENI associated with it at a given time.</p><p>Hence, the correct answer is: <strong>Use the </strong><code><strong>awsvpc</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then use IAM roles for tasks to access other resources.</strong></p><p>The option that says: <strong>Use the </strong><code><strong>bridge</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to Amazon EC2 instances then use IAM roles for EC2 instances to access other resources</strong> is incorrect because you won't be able to attach security groups to your ECS tasks using this network mode type. This will only use the Docker's built-in virtual network which runs inside each container instance. You have to use the <code>awsvpc</code> network mode instead to allow you to use security groups and network monitoring tools at a more granular level within your tasks. Moreover, if you are using the <code>awsvpc</code> network mode, you should attach the security group to the ECS task and not to the EC2 instance.</p><p>The option that says: <strong>Use the </strong><code><strong>bridge</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then use IAM roles for tasks to access other resources</strong> is incorrect. In order for you to use security groups and network monitoring tools at a more granular level within your ECS tasks, you have to use the <code>awsvpc</code> network mode instead.</p><p>The option that says: <strong>Use the </strong><code><strong>awsvpc</strong></code><strong> network mode in the task definition in your Amazon ECS Cluster. Attach security groups to the ECS tasks then pass IAM credentials into the container at launch time to access other AWS resources</strong> is incorrect. Although it uses the correct network mode, you have to use an IAM Role instead. It is a security risk to pass the IAM credentials into the container as it could be potentially exposed.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-networking.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#network_mode\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#network_mode</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p><p><br></p><p><strong>AWS Container Services Overview:</strong></p><p><a href=\"https://youtu.be/5QBgDX7O7pw\">https://youtu.be/5QBgDX7O7pw</a></p></div>"
	},
	{
		"question": "<p>A company is migrating an interactive car registration web system hosted on its on-premises network to AWS Cloud. The current architecture of the system consists of a single NGINX web server and a MySQL database running on a Fedora server, which both reside in their on-premises data center. For the new cloud architecture, a load balancer must be used to evenly distribute the incoming traffic to the application servers. Route 53 must be used for both domain registration and domain management.</p><p>In this scenario, what would be the most efficient way to transfer the web application to AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Use the AWS Server Migration Service (SMS) to create an EC2 AMI of the NGINX web server. </p><p><br></p><p>2. Configure auto-scaling to launch in two Availability Zones. </p><p><br></p><p>3. Launch a multi-AZ MySQL Amazon RDS instance in one availability zone only. </p><p><br></p><p>4. Import the data into Amazon RDS from the latest MySQL backup. </p><p><br></p><p>5. Create an ELB to front your web servers. </p><p><br></p><p>6. Use Amazon Route 53 and create an A record pointing to the elastic load balancer.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Launch two NGINX EC2 instances in two Availability Zones. </p><p><br></p><p>2. Copy the web files from the on-premises web server to each Amazon EC2 web server, using Amazon S3 as the repository. </p><p><br></p><p>3. Migrate the database using the AWS Database Migration Service. </p><p><br></p><p>4. Create an ELB to front your web servers. </p><p><br></p><p>5. Use Route 53 and create an alias A record<strong> </strong>pointing to the ELB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Use the AWS Application Discovery Service to migrate the NGINX web server. </p><p><br></p><p>2. Configure Auto Scaling to launch two web servers in two Availability Zones. </p><p><br></p><p>3. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in one Availability Zone only. </p><p><br></p><p>4. Import the data into Amazon RDS from the latest MySQL backup. </p><p><br></p><p>5. Use Amazon Route 53 to create a private hosted zone and point a non-alias A<strong> </strong>record to the ELB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Export web files to an Amazon S3 bucket in one Availability Zone using AWS Migration Hub. </p><p><br></p><p>2. Run the website directly out of Amazon S3. </p><p><br></p><p>3. Migrate the database using the AWS Database Migration Service and AWS Schema Conversion Tool (AWS SCT). </p><p><br></p><p>4. Use Route 53 and create an alias record pointing to the ELB.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>This is a trick question that contains a lot of information to confuse you, especially if you don't know the fundamental concepts in AWS. All options seem to be correct except for their last steps in setting up Route 53.</p><p>To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias record that points to your load balancer. An alias record is a Route 53 extension to DNS. It's similar to a CNAME record, but you can create an alias record both for the root domain, such as example.com, and for subdomains, such as www.example.com. (You can create CNAME records only for subdomains). For EC2 instances, always use a Type A Record without an Alias. For ELB, Cloudfront and S3, always use a Type A Record with an Alias and finally, for RDS, always use the CNAME Record with no Alias.</p><p>Hence, the following option is the correct answer:</p><p><strong>1. Launch two NGINX EC2 instances in two Availability Zones.</strong></p><p><strong>2. Copy the web files from the on-premises web server to each Amazon EC2 web server, using Amazon S3 as the repository.</strong></p><p><strong>3. Migrate the database using the AWS Database Migration Service.</strong></p><p><strong>4. Create an ELB to front your web servers.</strong></p><p><strong>5. Use Route 53 and create an alias A record pointing to the ELB.</strong></p><p><img src=\"https://media.tutorialsdojo.com/sap_db_migration.JPG\"></p><p><strong>AWS Database Migration Service</strong> helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.</p><p>The following sets of options are incorrect because they are just using an A record without an Alias:</p><p><strong>1. Use the AWS Server Migration Service (SMS) to create an EC2 AMI of the NGINX web server.</strong></p><p><strong>2. Configure auto-scaling to launch in two Availability Zones.</strong></p><p><strong>3. Launch a multi-AZ MySQL Amazon RDS instance in one availability zone only.</strong></p><p><strong>4. Import the data into Amazon RDS from the latest MySQL backup.</strong></p><p><strong>5. Create an ELB to front your web servers.</strong></p><p><strong>6. Use Amazon Route 53 and create an A record pointing to the elastic load balancer.</strong></p><p><em>--</em></p><p><strong>1. Use the AWS Application Discovery Service to migrate the NGINX web server.</strong></p><p><strong>2. Configure Auto Scaling to launch two web servers in two Availability Zones.</strong></p><p><strong>3. Launch a Multi-AZ MySQL Amazon Relational Database Service (RDS) instance in one Availability Zone only.</strong></p><p><strong>4. Import the data into Amazon RDS from the latest MySQL backup.</strong></p><p><strong>5. Use Amazon Route 53 to create a private hosted zone and point a non-alias A record to the ELB.</strong></p><p>Take note as well that the AWS Server Migration Service (SMS) is primarily used to migrate virtual machines only, which can be from VMware vSphere and Windows Hyper-V to your AWS cloud. In addition, the AWS Application Discovery Service simply helps you to plan migration projects by gathering information about your on-premises data centers but this service is not a suitable migration service.</p><p>The following option is also incorrect because the web system that is being migrated is a non-static (dynamic) website, which cannot be hosted in S3:</p><p><strong>1. Export web files to an Amazon S3 bucket in one Availability Zone using AWS Migration Hub.</strong></p><p><strong>2. Run the website directly out of Amazon S3.</strong></p><p><strong>3. Migrate the database using the AWS Database Migration Service and AWS Schema Conversion Tool (AWS SCT).</strong></p><p><strong>4. Use Route 53 and create an alias record pointing to the ELB.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Check out this AWS Database Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><br></p><p><strong>AWS Migration Services Overview:</strong></p><p><a href=\"https://youtu.be/yqNBkFMnsL8\">https://youtu.be/yqNBkFMnsL8</a></p></div>"
	},
	{
		"question": "<p>An IT consultancy company has multiple offices located in San Francisco, Frankfurt, Tokyo, and Manila. The company is using AWS Organizations to easily manage its several AWS accounts which are being used by its regional offices and subsidiaries. A new AWS account was recently added to a specific organizational unit (OU) which is responsible for the overall systems administration. The solutions architect noticed that the account is using a root-created Amazon ECS Cluster with an attached service-linked role. For regulatory purposes, the solutions architect created a custom SCP that would deny the new account from performing certain actions in relation to using ECS. However, after applying the policy, the new account could still perform the actions that it was supposed to be restricted from doing.</p><p>Which of the following is the most likely reason for this problem?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>The ECS service is being run outside the jurisdiction of the organization. SCPs affect only the principals that are managed by accounts that are part of the organization.</p>"
			},
			{
				"correct": false,
				"answer": "<p>There is an SCP attached to a higher-level OU that permits the actions of the service-linked role. This permission would therefore be inherited by the current OU, and override the SCP placed by the administrator.</p>"
			},
			{
				"correct": true,
				"answer": "<p>SCPs do not affect any service-linked role. Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The default SCP grants all permissions attached to every root, OU, and account. To apply stricter permissions, this policy is required to be modified.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Users and roles must still be granted permissions using IAM permission policies attached to them or to groups. The SCPs filter the permissions granted by such policies, and the user can't perform any actions that the applicable SCPs don't allow. Actions allowed by the SCPs can be used if they are granted to the user or role by one or more IAM permission policies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_how_it_works.png\"></p><p>When you attach SCPs to the root, OUs, or directly to accounts, all policies that affect a given account are evaluated together using the same rules that govern IAM permission policies:</p><p>- Any action that has an explicit <code>Deny</code> in an SCP can't be delegated to users or roles in the affected accounts. An explicit <code>Deny</code> statement overrides any <code>Allow</code> that other SCPs might grant.</p><p>- Any action that has an explicit <code>Allow</code> in an SCP (such as the default \"*\" SCP or by any other SCP that calls out a specific service or action) can be delegated to users and roles in the affected accounts.</p><p>- Any action that isn't explicitly allowed by an SCP is implicitly denied and can't be delegated to users or roles in the affected accounts.</p><p>By default, an SCP named <code>FullAWSAccess</code> is attached to every root, OU, and account. This default SCP allows all actions and all services. So in a new organization, until you start creating or manipulating the SCPs, all of your existing IAM permissions continue to operate as they did. As soon as you apply a new or modified SCP to a root or OU that contains an account, the permissions that your users have in that account become filtered by the SCP. Permissions that used to work might now be denied if they're not allowed by the SCP at every level of the hierarchy down to the specified account.</p><p>As stated in the documentation of AWS Organizations, <strong>SCPs DO NOT affect any service-linked role. Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.</strong></p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_org_attached.png\"></p><p>The option that says: <strong>The default SCP grants all permissions attached to every root, OU, and account. To apply stricter permissions, this policy is required to be modified</strong> is incorrect. The scenario already implied that the administrator created a <strong>Deny</strong> policy. By default, an SCP named <em>FullAWSAccess</em> is attached to every root, OU, and account. This default SCP allows all actions and all services. However, you specify a <strong>Deny</strong> policy if you want to create a blacklist that blocks all access to the specified services and actions. The explicit <strong>Deny</strong> on specific actions in the blacklist policy overrides the <strong>Allow</strong> in any other policy, such as the one in the default SCP.</p><p>The option that says: <strong>There is an SCP attached to a higher-level OU that permits the actions of the service-linked role. This permission would therefore be inherited by the current OU, and override the SCP placed by the administrator</strong> is incorrect because even if a higher-level OU has an SCP attached with an <strong>Allow</strong> policy for the service, the current set up should still have restricted access to the service. Creating and attaching a new <strong>Deny</strong> SCP to the new account's OU will not be affected by the pre-existing Allow policy in the same OU.</p><p>The option that says: <strong>The ECS service is being run outside the jurisdiction of the organization. SCPs affect only the principals that are managed by accounts that are part of the organization </strong>is incorrect because the service-linked role must have been created within the organization, most notably by the root account of the organization. It also does not make sense if we make the assumption that the service is indeed outside of the organization's jurisdiction because the <em>Principal</em> element of a policy specifies which entity will have limited permissions. But the scenario tells us that it should be the new account that is denied certain actions, not the service itself.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>An international foreign exchange company has a serverless forex trading application that was built using AWS SAM and is hosted on AWS Serverless Application Repository. They have millions of users worldwide who use their online portal 24/7 to trade currencies. However, they are receiving a lot of complaints that it takes a few minutes for their users to log in to their portal lately, including occasional HTTP 504 errors. As the Solutions Architect, you are tasked to optimize the system and to significantly reduce the time to log in to improve the customers' satisfaction.</p><p>Which of the following should you implement in order to improve the performance of the application with minimal cost? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use Lambda@Edge to allow your Lambda functions to customize content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Increase the cache hit ratio of your CloudFront distribution by configuring your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code>.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code failure responses.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up multiple and geographically disperse VPCs to various AWS regions then create a transit VPC to connect all of your resources. Deploy the Lambda function in each region using AWS SAM, in order to handle the requests faster.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Lambda@Edge</strong> lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer to the viewer. The functions run in response to CloudFront events, without provisioning or managing servers. You can use Lambda functions to change CloudFront requests and responses at the following points:</p><p>- After CloudFront receives a request from a viewer (viewer request)</p><p>- Before CloudFront forwards the request to the origin (origin request)</p><p>- After CloudFront receives the response from the origin (origin response)</p><p>- Before CloudFront forwards the response to the viewer (viewer response)</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\"></p><p>In the given scenario, you can <strong>use Lambda@Edge to allow your Lambda functions to customize the content that CloudFront delivers and to execute the authentication process in AWS locations closer to the users. </strong>In addition, you can<strong> set up an origin failover by creating an origin group with two origins with one as the primary origin and the other as the second origin which CloudFront automatically switches to when the primary origin fails.</strong> This will alleviate the occasional HTTP 504 errors that users are experiencing.</p><p>The option that says: <strong>Deploy your application to multiple AWS regions to accommodate your users around the world. Set up a Route 53 record with latency routing policy to route incoming traffic to the region that provides the best latency to the user</strong> is incorrect. Although this may resolve the performance issue, this solution entails a significant implementation cost since you have to deploy your application to multiple AWS regions. Remember that the scenario asks for a solution that will improve the performance of the application with <strong>minimal cost</strong>.</p><p>The option that says: <strong>Increase the cache hit ratio of your CloudFront distribution by configuring your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong> </code>is incorrect because improving the cache hit ratio for the CloudFront distribution is irrelevant in this scenario. You can improve your cache performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. However, take note that the problem in the scenario is the sluggish authentication process of your global users and not just the caching of the static objects.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html\">https://docs.aws.amazon.com/lambda/latest/dg/lambda-edge.html</a></p><p><br></p><p><strong>Check out these Amazon CloudFront and AWS Lambda Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p></div>"
	},
	{
		"question": "<p>A multinational consumer goods corporation structured their AWS accounts to use AWS Organizations, which consolidates payment of their multiple AWS accounts for their various Business Units (BU’s) namely Beauty products, Baby products, Health products, and Home Care products unit. One of their Solutions Architects for the Baby products business unit has purchased 10 Reserved Instances for their new Supply Chain application which will go live 3 months from now. However, they do not want their Reserved Instance (RI) discounts to be shared by the other business units. </p><p>Which of the following options is the most suitable solution for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Remove the AWS account of the Baby products business unit out of the AWS Organization."
			},
			{
				"correct": false,
				"answer": "Set the Reserved Instance (RI) sharing to private on the AWS account of the Baby products business unit. "
			},
			{
				"correct": true,
				"answer": "<p>Turn off the Reserved Instance (RI) sharing on the master account for all of the member accounts in the Baby products business unit.</p>"
			},
			{
				"correct": false,
				"answer": "Since the Baby product business unit is part of an AWS Organization, the Reserved Instances will always be shared across other member accounts. There is no way to disable this setting. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For billing purposes, the consolidated billing feature of <strong>AWS Organizations</strong> treats all the accounts in the organization as one account. This means that all accounts in the organization can receive the hourly cost-benefit of Reserved Instances that are purchased by any other account. In the payer account, you can turn off Reserved Instance discount sharing on the Preferences page on the Billing and Cost Management console.</p><p>The master account of an organization can turn off Reserved Instance (RI) sharing for member accounts in that organization. This means that Reserved Instances are not shared between that member account and other member accounts. You can change this preference multiple times. Each estimated bill is computed using the last set of preferences. However, take note that turning off Reserved Instance sharing can result in a higher monthly bill.</p><p><img src=\"https://media.tutorialsdojo.com/sap_consolidated_billing_ri.png\"></p><p>Hence, the correct answer is: <strong>Turn off the Reserved Instance (RI) sharing on the master account for all of the member accounts in the Baby products business unit.</strong></p><p>The option that says: <strong>Set the Reserved Instance (RI) sharing to private on the AWS account of the Baby products business unit</strong> is incorrect because there is no \"private\" option in the RI and Savings Plan discount sharing settings in the Billing Management Console. By default, the member account doesn't have the capability to turn off RI sharing on their account.</p><p>The option that says: <strong>Remove the AWS account of the Baby products business unit out of the AWS Organization</strong> is incorrect because removing the Baby products business unit account from the AWS Organization is not the optimal solution to prevent the other account from sharing its RI discounts. You can simply turn off the Reserved Instance discount sharing in the payer account.</p><p>The option that says: <strong>Since the Baby product business unit is part of an AWS Organization, the Reserved Instances will always be shared across other member accounts. There is no way to disable this setting is</strong> incorrect because this statement is false. There is certainly a way to disable the current setting by simply turning off RI sharing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off.html</a></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off-process.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-turn-off-process.html</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p></div>"
	},
	{
		"question": "<p>A company has production, development, and test environments in its software development department, and each environment contains tens to hundreds of EC2 instances, along with other AWS services. Recently, Ubuntu released a series of security patches for a critical flaw that was detected in their OS. Although this is an urgent matter, there is no guarantee yet that these patches will be bug-free and production-ready hence, the company must immediately patch all of its affected Amazon EC2 instances in all the environments, except for the production environment. The EC2 instances in the production environment will only be patched after it has been verified that the patches work effectively. Each environment also has different baseline patch requirements that needed to be satisfied.</p><p>Using the AWS Systems Manager service, how should you perform this task with the least amount of effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Tag each instance based on its OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and then apply the patches specified in the corresponding patch baseline to each Patch Group. Afterward, verify that the patches have been installed correctly using Patch Compliance. Record the changes to patch and association compliance statuses using AWS Config.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Tag each instance based on its environment and OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and apply the patches specified in the corresponding patch baseline to each Patch Group.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Schedule a maintenance period in AWS Systems Manager Maintenance Windows for each environment, where the period is after business hours so as not to affect daily operations. During the maintenance period, Systems Manager will execute a cron job that will install the required patches for each EC2 instance in each environment. After that, verify in Systems Manager Managed Instances that your environments are fully patched and compliant.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Tag each instance based on its environment and OS. Create various shell scripts for each environment that specifies which patch will serve as its baseline. Using AWS Systems Manager Run Command, place the EC2 instances into Target Groups and execute the script corresponding to each Target Group.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type.</p><p>Patch Manager uses <strong>patch baselines</strong>, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager Maintenance Window task. You can also install patches individually or to large groups of instances by using Amazon EC2 tags. For each auto-approval rule that you create, you can specify an auto-approval delay. This delay is the number of days of wait after the patch was released, before the patch is automatically approved for patching.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\"></p><p>A <strong>patch group</strong> is an optional means of organizing instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: <code><strong>Patch Group</strong></code>. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution.</p><p>Hence, the correct answer is: <strong>Tag each instance based on its environment and OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and apply the patches specified in the corresponding patch baseline to each Patch Group.</strong></p><p>The option that says: <strong>Tag each instance based on its environment and OS. Create various shell scripts for each environment that specifies which patch will serve as its baseline. Using AWS Systems Manager Run Command, place the EC2 instances into Target Groups and execute the script corresponding to each Target Group </strong>is incorrect as this option takes more effort to perform because you are using Systems Manager Run Command instead of Patch Manager. The Run Command service enables you to automate common administrative tasks and perform ad hoc configuration changes at scale, however, it takes a lot of effort to implement this solution. You can use Patch Manager instead to perform the task required by the scenario since you need to perform this task with the least amount of effort.</p><p>The option that says: <strong>Tag each instance based on its OS. Create a patch baseline in AWS Systems Manager Patch Manager for each environment. Categorize EC2 instances based on their tags using Patch Groups and then apply the patches specified in the corresponding patch baseline to each Patch Group. Afterward, verify that the patches have been installed correctly using Patch Compliance. Record the changes to patch and association compliance statuses using AWS Config</strong> is incorrect. You should be tagging instances based on the environment and its OS type in which they belong and not just its OS type. This is because the type of patches that will be applied varies between the different environments. With this option, the Ubuntu EC2 instances in all of your environments, including in production, will automatically be patched.</p><p>The option that says: <strong>Schedule a maintenance period in AWS Systems Manager Maintenance Windows for each environment, where the period is after business hours so as not to affect daily operations. During the maintenance period, Systems Manager will execute a cron job that will install the required patches for each EC2 instance in each environment. After that, verify in Systems Manager Managed Instances that your environments are fully patched and compliant</strong> is incorrect because this is not the simplest way to address the issue using AWS Systems Manager. The AWS Systems Manager Maintenance Windows feature lets you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. Although this solution may work, it entails a lot of configuration and effort to implement.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/\">https://aws.amazon.com/blogs/mt/patching-your-windows-ec2-instances-using-aws-systems-manager-patch-manager/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A company uses Lightweight Directory Access Protocol (LDAP) for its employee authentication and authorization. The company plans to release a mobile app that can be installed on employee’s smartphones. The mobile application will allow users to have federated access to AWS resources. Due to strict security and compliance requirements, the mobile application must use a custom-built solution for user authentication. It must also use IAM roles for granting user permissions to AWS resources. The Solutions Architect was tasked to create a solution that meets these requirements.</p><p>Which of the following options should the Solutions Architect implement to enable authentication and authorization for the application? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Build a custom SAML-compatible solution for user authentication. Leverage AWS Single Sign-On (SSO) for authorizing access to AWS resources.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Build a custom LDAP connector using Amazon API Gateway with AWS Lambda function for user authentication. Use Amazon DynamoDB to store user authorization tokens. Write another Lambda function that will validate user authorization requests based on the token stored on DynamoDB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Build a custom OpenID Connect-compatible solution in combination with AWS Single Sign-On (SSO) to create authentication and authorization functionality for the application.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Build a custom SAML-compatible solution to handle authentication and authorization. Configure the solution to use LDAP for user authentication and use SAML assertion to perform authorization to the IAM identity provider.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Build a custom OpenID Connect-compatible solution for the user authentication functionality. Use Amazon Cognito Identity Pools for authorizing access to AWS resources.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS supports <strong>identity federation with SAML 2.0</strong> (Security Assertion Markup Language 2.0), an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log in to the AWS Management Console or call the AWS API operations without you having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code.</p><p>You can use a role to configure your SAML 2.0-compliant identity provider (IdP) and AWS to permit your federated users to access the AWS Management Console. The role grants the user permissions to carry out tasks in the console. The following diagram illustrates the flow for SAML-enabled single sign-on.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap.png\"></p><p>The diagram illustrates the following steps:</p><p>The user browses your organization's portal and selects the option to go to the AWS Management Console. In your organization, the portal is typically a function of your IdP that handles the exchange of trust between your organization and AWS.</p><p>The portal verifies the user's identity in your organization.</p><p>The portal generates a SAML authentication response that includes assertions that identify the user and include attributes about the user. The portal sends this response to the client browser.</p><p>The client browser is redirected to the AWS single sign-on endpoint and posts the SAML assertion.</p><p>The endpoint requests temporary security credentials on behalf of the user and creates a console sign-in URL that uses those credentials.</p><p>AWS sends the sign-in URL back to the client as a redirect.</p><p>The client browser is redirected to the AWS Management Console. If the SAML authentication response includes attributes that map to multiple IAM roles, the user is first prompted to select the role for accessing the console.</p><p><strong>Amazon Cognito</strong> provides authentication, authorization, and user management for your web and mobile apps. Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google, or Apple. The two main components of Amazon Cognito are user pools and identity pools. User pools are user directories that provide sign-up and sign-in options for your app users. Identity pools enable you to grant your users access to other AWS services. You can use identity pools and user pools separately or together.</p><p><strong>Amazon Cognito identity pools</strong> provide temporary AWS credentials for users who are guests (unauthenticated) and for users who have been authenticated and have received a token.</p><p><strong>OpenID Connect</strong> is an open standard for authentication that is supported by a number of login providers. Amazon Cognito supports the linking of identities with OpenID Connect providers that are configured through AWS Identity and Access Management. Once you've created an OpenID Connect provider in the IAM Console, you can associate it with an identity pool.</p><p>The option that says: <strong>Build a custom SAML-compatible solution to handle authentication and authorization. Configure the solution to use LDAP for user authentication and use SAML assertion to perform authorization to the IAM identity provider</strong> is correct. The requirement is to use a custom-built solution for user authentication and this can use the company LDAP system for authentication. The SAML assertion is also needed to get authorization tokens from the IAM identity provider that will grant IAM roles to users that wish to access AWS resources.</p><p>The option that says: <strong>Build a custom OpenID Connect-compatible solution for the user authentication functionality. Use Amazon Cognito Identity Pools for authorizing access to AWS resources </strong>is correct. The custom OpenID Connect-compatible solution will allow users to log in from their mobile application much like a single sign-on functionality. Amazon Cognito Identity Pool will provide temporary tokens to federated users for accessing AWS resources.</p><p>The option that says: <strong>Build a custom SAML-compatible solution for user authentication. Leverage AWS Single Sign-On (SSO) for authorizing access to AWS resources</strong> is incorrect. The requirement is to grant federated access from the mobile application. AWS SSO supports single sign-on to business applications through web browsers only.</p><p>The option that says: <strong>Build a custom LDAP connector using Amazon API Gateway with AWS Lambda function to user authentication. Use Amazon DynamoDB to store user authorization tokens. Write another Lambda function that will validate user authorization requests based on the token stored on DynamoDB</strong> is incorrect. It is not recommended to store authorization tokens permanently on DynamoDB tables. These tokens should be generated upon user authentication and then temporarily saved on a DynamoDB for a fixed session length.</p><p>The option that says: <strong>Build a custom OpenID Connect-compatible solution in combination with AWS Single Sign-On (SSO) to create authentication and authorization functionality for the application</strong> is incorrect. AWS SSO supports only SAML 2.0–based applications so an OpenID Connect-compatible solution will not work for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/open-id.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/open-id.html</a></p><p><a href=\"https://aws.amazon.com/single-sign-on/faqs/\">https://aws.amazon.com/single-sign-on/faqs/</a></p><p><br></p><p><strong>AWS Identity Services Overview:</strong></p><p><br></p><p><strong>Check out these Amazon Cognito Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/?src=udemy\">https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/</a></p></div>"
	},
	{
		"question": "<p>A company needs a deployment solution for its application that is hosted on the AWS cloud. The company has the following requirements for the application:</p><p>- The instances must have 500GB worth of static dataset that is accessible for the application upon boot up.</p><p>- The instances must be able to scale-out or scale-in depending on the traffic load of the application.</p><p>- The Development team must have a quick and automated way to deploy their code updates several times during the day.</p><p>- Security patches for the vulnerabilities on the operating system (OS) must be installed within 48 hours of release.</p><p>Which of the following solutions should the Solutions Architect implement to meet the company requirements while being cost-effective?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Create a scheduled batch job that will run every night to deploy the new application version and install the OS patches. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Deploy the new version of the application to the instances using AWS CodeDeploy. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Replace the existing instances as soon as AWS releases a new Amazon Linux AMI version. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Deploy the new version of the application to the instances using AWS CodeDeploy.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Use AWS Systems Manager to install the OS patches as soon as they are released. Deploy the new version of the application to the instances using AWS CodeDeploy.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with both security-related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. You can use Patch Manager to install Service Packs on Windows instances and perform minor version upgrades on Linux instances.</p><p>Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. You can install patches on a regular basis by scheduling patching to run as a Systems Manager maintenance window task.</p><p><strong>AWS Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build automations to configure and manage instances and AWS resources.</p><p>- Create custom runbooks or use pre-defined runbooks maintained by AWS.</p><p>- Receive notifications about Automation tasks and runbooks by using Amazon EventBridge.</p><p>- Monitor Automation progress and details by using the AWS Systems Manager console.</p><p>AWS Systems Manager Automation provides several runbooks with pre-defined steps that you can use to perform common tasks like restarting one or more EC2 instances or creating an Amazon Machine Image (AMI). A Systems Manager Automation runbook defines the actions that Systems Manager performs on your managed instances and other AWS resources when an automation runs. A runbook contains one or more steps that run in sequential order. Each step is built around a single action. Output from one step can be used as input in a later step.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_automation.png\"></p><p><strong>Amazon Elastic File System (Amazon EFS)</strong> provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily. With Amazon EFS, you pay only for the storage used by your file system and there is no minimum fee or setup cost. Amazon EFS oﬀers two storage classes, Standard and Infrequent Access. The Standard storage class is used to store frequently accessed files. The Infrequent Access (IA) storage class is a lower-cost storage class that's designed for storing long-lived, infrequently accessed ﬁles cost-eﬀectively.</p><p>Amazon Elastic File System presents a standard file-system interface that supports full file-system access semantics. Using Network File System (NFS) version 4.1 (NFSv4.1), you can mount your Amazon EFS file system on any Amazon Elastic Compute Cloud (Amazon EC2) Linux-based instance. After your system is mounted, you can work with the files and directories just as you do with a local file system.</p><p>Therefore, the correct answer is: <strong>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Deploy the new version of the application to the instances using AWS CodeDeploy. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up.</strong></p><p>The option that says: <strong>Install OS patches and create a new AMI using AWS Systems Manager. Use this new AMI for the Auto Scaling group of EC2 instances and replace the existing instances. Create a scheduled batch job that will run every night to deploy the new application version and install the OS patches. Mount an Amazon EFS volume containing the static dataset on the instances upon boot up</strong> is incorrect. The OS patches can be installed every night, but it is not suitable for the application deployment. A batch job is not suitable for the application deployment as the Developers must deploy several times during the day.</p><p>The option that says: <strong>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Use AWS Systems Manager to install the OS patches as soon as they are released. Deploy the new version of the application to the instances using AWS CodeDeploy</strong> is incorrect. Although Amazon S3 may seem more cost-effective than Amazon EFS in storing static contents, the Amazon EC2 instances will have to download the dataset on its local EBS volume. Attaching 500GB EBS volumes on each of the EC2 instances is more expensive compared to just using a single EFS volume mounted on all EC2 instances at boot up.</p><p>The option that says: <strong>Create an Auto Scaling group of EC2 instances using the Amazon Linux AMI. Install the application on the EC2 instances. Replace the existing instances as soon as AWS releases a new Amazon Linux AMI version. Write a user data script that will download the 500 GB static dataset from an Amazon S3 bucket. Deploy the new version of the application to the instances using AWS CodeDeploy</strong> is incorrect. The Amazon Linux AMI is patched for security vulnerabilities and OS minor versions. However, each new version usually takes weeks or months depending on Amazon’s release cycle.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-documents.html</a></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and Amazon EFS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p></div>"
	},
	{
		"question": "<p>A company develops Docker containers to host web applications on its on-premises data center. The company wants to migrate its workload to the cloud and use AWS Fargate. The solutions architect has created the necessary task definition and service for the Fargate cluster. For security requirements, the cluster is placed on a private subnet in the VPC that has no direct connection outside of the VPC. The following error is received when trying to launch the Fargate task:</p><p><code>CannotPullContainerError: API error (500): Get https://111122223333.dkr.ecr.us-east-1.amazonaws.com/v2/: net/http: request canceled while waiting for connection</code></p><p>Which of the following options should be able to fix this issue?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Update the AWS Fargate task definition and set the auto-assign public IP option to ENABLED. Create a gateway VPC endpoint for Amazon ECR. Update the route table to allow AWS Fargate to pull images on Amazon ECR via the endpoint.</p>"
			},
			{
				"correct": false,
				"answer": "<p>This is a limitation of the “<code>awsvpc</code>” network mode. Update the AWS Fargate definition to use the “<code>bridge</code>” network mode instead to allow connections to the Internet.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the private subnet of the VPC and update the route table of the private subnet to route requests to the Internet.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the public subnet of the VPC and update the route table of the private subnet to route requests to the Internet.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Fargate</strong> is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design.</p><p><strong>Fargate</strong> allocates the right amount of compute resources, eliminating the need to choose instances and scale cluster capacity. You only pay for the resources required to run your containers, so there is no over-provisioning and paying for additional servers. Fargate runs each task or pod in its own kernel providing the tasks and pods their own isolated compute environment.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fargate_overview.png\"></p><p>The <code><strong>CannotPullContainer error (500)</strong></code> is caused by the <code>Connection timed out</code> when connecting to Amazon ECR. This indicates that when creating a task, the container image specified could not be retrieved.</p><p>When a Fargate task is launched, its elastic network interface requires a route to the Internet to pull container images. If you receive an error similar to the following when launching a task, it is because a route to the Internet does not exist:</p><p><code>CannotPullContainerError: API error (500): Get https://111122223333.dkr.ecr.us-east-1.amazonaws.com/v2/: net/http: request canceled while waiting for connection\"</code></p><p>To resolve this issue, you can:</p><p>- For tasks in public subnets, specify <strong>ENABLED</strong> for <strong>Auto-assign public IP</strong> when launching the task.</p><p>- For tasks in private subnets, specify <strong>DISABLED</strong> for <strong>Auto-assign public IP</strong> when launching the task, and configure a NAT gateway in your VPC to route requests to the Internet.</p><p>Therefore, the correct answer is: <strong>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the public subnet of the VPC and update the route table of the private subnet to route requests to the internet</strong>. The NAT gateway in the public subnet should have a public IP address and a route to the Intenet Gateway. The tasks in the private subnet will send Internet traffic to the NAT gateway to be able pull the images on Amazon Elastic Container Registry.</p><p>The option that says: <strong>Update the AWS Fargate task definition and set the auto-assign public IP option to ENABLED. Create a gateway VPC endpoint for Amazon ECR. Update the route table to allow AWS Fargate to pull images on Amazon ECR via the endpoint</strong> is incorrect. Since the Fargate tasks are on private subnet, you don't need to enable the auto-assign public IP option. Additionally, you should interface VPC endpoint, not gateway VPC endpoint.</p><p>The option that says: <strong>Update the AWS Fargate task definition and set the auto-assign public IP option to DISABLED. Launch a NAT gateway on the private subnet of the VPC and update the route table of the private subnet to route requests to the Internet</strong> is incorrect. The NAT gateway should be placed in a public subnet because it needs a Public IP address and a direct route to the Internet Gateway (IGW). If it is placed on a private subnet, it will have the same routing limitation as those resources in the private subnet.</p><p>The option that says: <strong>This is a limitation of the “</strong><code><strong>awsvpc</strong></code><strong>” network mode. Update the AWS Fargate definition to use the “</strong><code><strong>bridge</strong></code><strong>” network mode instead to allow connections to the Internet</strong> is incorrect. AWS Fargate only supports the \"<code>awsvpc</code>\" network mode. Each task is allocated its own elastic network interface (ENI) that is used for communication inside the VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/ecs-pull-container-api-error-ecr/\">https://aws.amazon.com/premiumsupport/knowledge-center/ecs-pull-container-api-error-ecr/</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html\">https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html</a></p><p><br></p><p><strong>Check out this AWS Fargate Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p></div>"
	},
	{
		"question": "<p>A telecommunications company plans to have a public announcement for a new phone offering. It is expected that millions of people will access their website to get the new offer. Their company’s e-commerce platform is running on an Auto Scaling group of On-Demand EC2 instances deployed across multiple Availability Zones. For the database tier, the platform is using an Amazon RDS database in a Multi-AZ deployments configuration. Their e-commerce site performs a high number of small reads and writes per second to handle customer transactions and relies on an eventual consistency model. The Operations team identified that there is read contention on RDS MySQL database after conducting a series of performance tests.</p><p>Which combination of options should you implement to provide a fast, cost-efficient, and scalable solution? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Migrate the database to Amazon Redshift and use its massively parallel query execution capability to improve the read performance of the application.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Modify the Amazon RDS Multi-AZ deployments configuration to launch multiple standby database instances. Distribute the incoming traffic to the standby instances to improve the database performance.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Implement an in-memory cache using Amazon ElastiCache</p>"
			},
			{
				"correct": true,
				"answer": "Set up Read Replicas in each Availability Zone. "
			},
			{
				"correct": false,
				"answer": "Vertically scale your RDS MySQL Instance by upgrading its instance size with provisioned IOPS. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For this scenario, the optimal services to use are <strong>Amazon ElastiCache</strong> and <strong>RDS Read Replicas</strong>. Amazon ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads (such as social networking, gaming, media sharing and Q&amp;A portals) or compute-intensive workloads (such as a recommendation engine) by allowing you to store the objects that are often read in cache. Moreover, with Redis’ support for advanced data structures, you can augment the database tier to provide features (such as leaderboard, counting, session and tracking) that are not easily achievable via databases in a cost-effective way.</p><p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads.</p><p>You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, Oracle, and PostgreSQL as well as Amazon Aurora.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_read_replica.png\"></p><p>You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.</p><p>To further maximize read performance, Amazon RDS for MySQL allows you to add table indexes directly to Read Replicas, without those indexes being present on the master.</p><p>Because read replicas can be promoted to master status, they are useful as part of a sharding implementation. To shard your database, add a read replica and promote it to master status, then, from each of the resulting DB Instances, delete the data that belongs to the other shard.</p><p><strong>Setting up Read Replicas in each Availability Zone</strong> is correct because Read Replicas are used to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads hence, improving the read performance.</p><p><strong>Implementing an in-memory cache using Amazon ElastiCache</strong> is correct because ElastiCache is an in-memory caching solution which reduces the load on the database and improves the read performance.</p><p><strong>Migrating the database to Amazon Redshift and using its massively parallel query execution capability to improve the read performance of the application</strong> is incorrect because Amazon Redshift is more suitable for OLAP-type applications and not for online transaction processing (OLTP). Redshift is also not suitable to host your MySQL database.</p><p><strong>Modifying the Amazon RDS Multi-AZ deployments configuration to launch multiple standby database instances and distributing the incoming traffic to the standby instances to improve the database performance</strong> is incorrect because you cannot distribute the incoming traffic to the standby instances since these are not readable at all. These database instances are primarily used to improve the availability of your database and your application.</p><p><strong>Vertically scaling your RDS MySQL Instance by upgrading its instance size with provisioned IOPS</strong> is incorrect. Although upgrading the instance size may improve the read performance to a certain extent, it is not as scalable compared with Read Replicas or ElastiCache.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><br></p><p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A top university has launched its serverless online portal using Lambda and API Gateway in AWS that enables its students to enroll, manage their class schedules, and see their grades online. After a few weeks, the portal abruptly stopped working and lost all of its data. The university hired an external cybersecurity consultant and based on the investigation, the outage was due to an SQL injection vulnerability on the portal's login page in which the attacker simply injected the malicious SQL code. You also need to track historical changes to the rules and metrics associated with your firewall.</p><p>Which of the following is the most suitable and cost-effective solution to avoid another SQL Injection attack against their infrastructure in AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use AWS WAF to add a web access control list (web ACL) in front of the Lambda functions to block requests that contain malicious SQL code. Use AWS Firewall Manager, to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Block the IP address of the attacker in the Network Access Control List of your VPC and then set up a CloudFront distribution. Set up AWS WAF to add a web access control list (web ACL) in front of the CloudFront distribution to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS WAF to add a web access control list (web ACL) in front of the API Gateway to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new Application Load Balancer (ALB) and set up AWS WAF in the load balancer. Place the API Gateway behind the ALB and configure a web access control list (web ACL) in front of the ALB to block requests that contain malicious SQL code. Use AWS Firewall Manager to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. With AWS Config, you can track changes to WAF web access control lists (web ACLs). For example, you can record the creation and deletion of rules and rule actions, as well as updates to WAF rule configurations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_waf.png\"></p><p><strong>AWS WAF</strong> gives you control over which traffic to allow or block to your web applications by defining customizable web security rules. You can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond quickly to changing traffic patterns. Also, AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of web security rules.</p><p>In this scenario, the best option is to deploy WAF in front of the API Gateway. Hence the correct answer is the option that says: <strong>Use AWS WAF to add a web access control list (web ACL) in front of the API Gateway to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong>.</p><p>The option that says: <strong>Use AWS WAF to add a web access control list (web ACL) in front of the Lambda functions to block requests that contain malicious SQL code. Use AWS Firewall Manager, to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations </strong>is incorrect because you have to use AWS WAF in front of the API Gateway and not directly to the Lambda functions. AWS Firewall Manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations and hence, it is not suitable for tracking changes to WAF web access control lists. You should use AWS Config instead.</p><p>The option that says: <strong>Block the IP address of the attacker in the Network Access Control List of your VPC and then set up a CloudFront distribution. Set up AWS WAF to add a web access control list (web ACL) in front of the CloudFront distribution to block requests that contain malicious SQL code. Use AWS Config to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong> is incorrect. Even though it is valid to use AWS WAF with CloudFront, it entails an additional and unnecessary cost to launch a CloudFront distribution for this scenario. There is no requirement that the serverless online portal should be scalable and be accessible around the globe hence, a CloudFront distribution is not necessary.</p><p>The option that says: <strong>Create a new Application Load Balancer (ALB) and set up AWS WAF in the load balancer. Place the API Gateway behind the ALB and configure a web access control list (web ACL) in front of the ALB to block requests that contain malicious SQL code. Use AWS Firewall Manager to track changes to your web access control lists (web ACLs) such as the creation and deletion of rules including the updates to the WAF rule configurations</strong> is incorrect. Launching a new Application Load Balancer entails additional cost and is not cost-effective. In addition, AWS Firewall manager is primarily used to manage your Firewall across multiple AWS accounts under your AWS Organizations. Using AWS Config is much more suitable for tracking changes to WAF web access control lists.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html\">https://docs.aws.amazon.com/waf/latest/developerguide/what-is-aws-waf.html</a></p><p><br></p><p><strong>Check out this AWS WAF Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><br></p><p><strong>AWS Security Services Overview - WAF, Shield, CloudHSM, KMS:</strong></p><p><a href=\"https://youtu.be/-1S-RdeAmMo\">https://youtu.be/-1S-RdeAmMo</a></p></div>"
	},
	{
		"question": "<p>A fintech startup has developed a cloud-based payment processing system that accepts credit card payments as well as cryptocurrencies such as Bitcoin, Ripple, and the likes. The system is deployed in AWS which uses EC2, DynamoDB, S3, and CloudFront to process the payments. Since they are accepting credit card information from the users, they are required to be compliant with the Payment Card Industry Data Security Standard (PCI DSS). On the recent 3rd-party audit, it was found that the credit card numbers are not properly encrypted and hence, their system failed the PCI DSS compliance test. You were hired by the fintech startup to solve this issue so they can release the product in the market as soon as possible. In addition, you also have to improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content.</p><p>In this scenario, what is the best option to protect and encrypt the sensitive credit card information of the users and to improve the cache hit ratio of your CloudFront distribution?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Add a custom SSL in the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the CloudFront distribution to use Signed URLs. Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure the CloudFront distribution to enforce secure end-to-end connections to origin servers by using HTTPS and field-level encryption. Configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code> to increase your cache hit ratio.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an origin access identity (OAI) and add it to the CloudFront distribution. Configure your origin to add <code>User-Agent</code> and <code>Host</code> headers to your objects to increase your cache hit ratio.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Field-level encryption</strong> adds an additional layer of security, along with HTTPS, that lets you protect specific data throughout system processing so that only certain applications can see it. Field-level encryption allows you to securely upload user-submitted sensitive information to your web servers. The sensitive information provided by your clients is encrypted at the edge closer to the user and remains encrypted throughout your entire application stack, ensuring that only applications that need the data—and have the credentials to decrypt it—are able to do so.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_encryption.png\"></p><p>To use field-level encryption, you configure your CloudFront distribution to specify the set of fields in POST requests that you want to be encrypted, and the public key to use to encrypt them. You can encrypt up to 10 data fields in a request. Hence, the correct answer for this scenario is the option that says: <strong>Configure the CloudFront distribution to enforce secure end-to-end connections to origin servers by using HTTPS and field-level encryption. Configure your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio.</strong></p><p>You can improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content; that is, by improving the cache hit ratio for your distribution. To increase your cache hit ratio, you can configure your origin to add a <code>Cache-Control max-age</code> directive to your objects, and specify the longest practical value for <code>max-age</code>. The shorter the cache duration, the more frequently CloudFront forwards another request to your origin to determine whether the object has changed and, if so, to get the latest version.</p><p>The option that says: <strong>Add a custom SSL in the CloudFront distribution. Configure your origin to add </strong><code><strong>User-Agent</strong></code><strong> and </strong><code><strong>Host</strong></code><strong> headers to your objects to increase your cache hit ratio</strong> is incorrect. Although it provides secure end-to-end connections to origin servers, it is better to add field-level encryption to protect the credit card information.</p><p>The option that says: <strong>Configure the CloudFront distribution to use Signed URLs. Configure your origin to add a </strong><code><strong>Cache-Control max-age</strong></code><strong> directive to your objects, and specify the longest practical value for </strong><code><strong>max-age</strong></code><strong> to increase your cache hit ratio</strong> is incorrect because a Signed URL provides a way to distribute private content but it doesn't encrypt the sensitive credit card information.</p><p>The option that says: <strong>Create an Origin Access Identity (OAI) and add it to the CloudFront distribution. Configure your origin to add </strong><code><strong><em>User-Agent</em></strong></code><strong><em> </em>and </strong><code><strong><em>Host</em></strong></code><strong> headers to your objects to increase your cache hit ratio</strong> is incorrect because OAI is mainly used to restrict access to objects in S3 bucket, but not provide encryption to specific fields.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/field-level-encryption.html#field-level-encryption-setting-up</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company processes several petabytes of images submitted by users on their photo hosting site every month. Each month, the images are processed in its on-premises data center by a High-Performance Computing (HPC) cluster with a capacity of 5,000 cores and 10 petabytes of data. Processing a month’s worth of images by thousands of jobs running in parallel takes about a week and the processed images are stored on a network file server, which also backups the data to a disaster recovery site.</p><p>The current data center is nearing its capacity so the users are forced to spread the jobs within the course of the month. This is not ideal for the requirement of the jobs, so the Solutions Architect was tasked to design a scalable solution that can exceed the current capacity with the least amount of management overhead while maintaining the current level of durability.</p><p>Which of the following solutions will meet the company's requirements while being cost-effective?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon SQS queue and submit the list of jobs to be processed. Create an Auto Scaling Group of Amazon EC2 Spot Instances that will process the jobs from the SQS queue. Share the raw data across all the instances using Amazon EFS. Store the processed images in an Amazon S3 bucket for long term storage.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Package the executable file for the job on a Docker image hosted on Amazon Elastic Container Service. Use the Docker image to run Amazon ECS tasks with a fleet of Spot Instances in an Auto Scaling group. Store the raw data temporarily on Amazon EBS SC1 volumes and then send the images to an Amazon S3 bucket after processing.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Utilize AWS Batch with Managed Compute Environments to create a fleet using Spot Instances. Store the raw data on an Amazon S3 bucket. Create jobs on AWS Batch Job Queues that will pull objects from the Amazon S3 bucket and temporarily store them to the EC2 EBS volumes for processing. Send the processed images back to another Amazon S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Using a combination of On-demand and Reserved Instances as Task Nodes, create an EMR cluster that will use Spark to pull the raw data from an Amazon S3 bucket. List the jobs that need to be processed by the EMR cluster on a DynamoDB table. Store the processed images on a separate Amazon S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Batch</strong> enables developers, scientists, and engineers to easily and efficiently run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. With AWS Batch, there is no need to install and manage batch computing software or server clusters that you use to run your jobs, allowing you to focus on analyzing results and solving problems.</p><p>There is no additional charge for AWS Batch. You only pay for the AWS resources (e.g. EC2 instances or Fargate jobs) you create to store and run your batch jobs. From the AWS Batch use cases page, we can see an example similar to this scenario wherein Digital Media and Entertainment companies require highly scalable batch computing resources to enable accelerated and automated processing of data as well as the compilation and processing of files, graphics, and visual effects for high-resolution video content. Use AWS Batch to accelerate content creation, dynamically scale media packaging, and automate asynchronous media supply chain workflows.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_batch.JPG\"></p><p>In AWS Batch, job queues are mapped to one or more compute environments. Compute environments contain the Amazon ECS container instances that are used to run containerized batch jobs. A specific compute environment can also be mapped to one or many job queues. Within a job queue, the associated compute environments each have an order that's used by the scheduler to determine where jobs that are ready to be run should run.</p><p>Therefore, the correct answer is:<strong> Utilize AWS Batch with Managed Compute Environments to create a fleet using Spot Instances. Store the raw data on an Amazon S3 bucket. Create jobs on AWS Batch Job Queues that will pull objects from the Amazon S3 bucket and temporarily store them to the EC2 EBS volumes for processing. Send the processed images back to another Amazon S3 bucket.</strong></p><p>The option that says: <strong>Package the executable file for the job on a Docker image hosted on Amazon Elastic Container Service. Use the Docker image to run Amazon ECS tasks with a fleet of Spot Instances in an Auto Scaling group. Store the raw data temporarily on Amazon EBS SC1 volumes and then send the images to an Amazon S3 bucket after processing</strong> is incorrect. Although this is possible, managing the ECS cluster adds management overhead and since you can’t quickly increase/decrease SC1 EBS volumes, creating a large volume to handle petabytes of data is not economical.</p><p>The option that says: <strong>Using a combination of On-demand and Reserved Instances as Task Nodes, create an EMR cluster that will use Apache Spark to pull the raw data from an Amazon S3 bucket. List the jobs that need to be processed by the EMR cluster on a DynamoDB table. Store the processed images on a separate Amazon S3 bucket</strong> is incorrect as managing the EMR cluster and Apache Spark adds significant management overhead for this solution. There is also an additional cost for the EC2 instances that are constantly running even if there are only a few jobs that need to be run.</p><p>The option that says: <strong>Create an Amazon SQS queue and submit the list of jobs to be processed. Create an Auto Scaling Group of Amazon EC2 Spot Instances that will process the jobs from the SQS queue. Share the raw data across all the instances using Amazon EFS. Store the processed images in an Amazon S3 bucket for long term storage</strong> is incorrect as Amazon EFS is more expensive than storing the raw data on S3 buckets. This is also not efficient as listing the jobs on SQS Queue can cause some to be processed twice, depending on the state of your Spot instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html\">https://docs.aws.amazon.com/batch/latest/userguide/compute_environments.html</a></p><p><a href=\"https://aws.amazon.com/batch/use-cases/\">https://aws.amazon.com/batch/use-cases/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/\">https://aws.amazon.com/blogs/compute/building-high-throughput-genomic-batch-workflows-on-aws-batch-layer-part-3-of-4/</a></p><p><br></p><p><strong>Check out this AWS Batch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-batch/?src=udemy\">https://tutorialsdojo.com/aws-batch/</a></p></div>"
	},
	{
		"question": "<p>A company has created multiple accounts in AWS to support the rapid growth of its cloud services. The multiple accounts are used to separate their various departments such as finance, human resources, engineering, and many others. Each account is managed by a Systems Administrator which has root access for that specific account only. There is a requirement to centrally manage policies across multiple AWS accounts by allowing or denying particular AWS services for individual accounts, or for groups of accounts.</p><p>Which is the most suitable solution that you should implement with the LEAST amount of complexity?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Provide access to externally authenticated users via Identity Federation. Set up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS Organizations and Organizational Units (OU) to connect all AWS accounts of each department. Create a custom IAM Policy to allow or deny the use of certain AWS services for each account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Connect all departments by setting up cross-account access to each of the AWS accounts of the company. Create and attach IAM policies to your resources based on their respective departments to control access.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Organizations and Service Control Policies to control the list of AWS services that can be used by each member account.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Organizations</strong> offers policy-based management for multiple AWS accounts. With Organizations, you can create groups of accounts, automate account creation, and apply and manage policies for those groups. Organizations enables you to centrally manage policies across multiple accounts, without requiring custom scripts and manual processes. It allows you to create <strong>Service Control Policies (SCPs)</strong> that centrally control AWS service use across multiple AWS accounts.</p><p>Remember that AWS Organizations <strong>does not</strong> replace associating IAM policies with users, groups, and roles within an AWS account. Hence, you still need to set up appropriate IAM policies for your root and member accounts.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_orgganization_nested.JPG\"></p><p>IAM policies let you allow or deny access to AWS services (such as Amazon S3), individual AWS resources (such as a specific S3 bucket), or individual API actions (such as <code>s3:CreateBucket</code>). An IAM policy can be applied only to IAM users, groups, or roles, and it can never restrict the root identity of the AWS account.</p><p>By contrast, AWS Organizations lets you use service control policies (SCPs) to allow or deny access to particular AWS services for individual AWS accounts, or for groups of accounts within an organizational unit (OU). The specified actions from an attached SCP affect all IAM users, groups, and roles for an account, including the root account identity.</p><p>When you apply an SCP to an OU or an individual AWS account, you choose to either <strong>enable </strong>(whitelist), or <strong>disable </strong>(blacklist) the specified AWS service. Access to any service that isn’t explicitly allowed by the SCPs associated with an account, its parent OUs, or the master account is <strong>denied </strong>to the AWS accounts or OUs associated with the SCP. When an SCP is applied to an OU, it is inherited by all of the AWS accounts in that OU.</p><p>Therefore, the correct answer is: <strong>Use AWS Organizations and Service Control Policies to control the list of AWS services that can be used by each member account.</strong></p><p>The option that says:<strong> Setting up AWS Organizations and Organizational Units (OU) to connect all AWS accounts of each department and creating a custom IAM Policy to allow or deny the use of certain AWS services for each account</strong> is incorrect. Although it is correct to use AWS Organizations, this option is incorrect about IAM Policy. It is the Service Control Policy (SCP) which enables you to allow or deny the use of certain AWS services for each account, and not the IAM Policy.</p><p>The option that says:<strong> Connecting all departments by setting up cross-account access to each of the AWS accounts of the company, then creating and attaching IAM policies to your resources based on their respective departments to control access</strong> is incorrect. Although you can set up cross-account access to each department, this entails a lot of configuration compared with using AWS Organizations and Service Control Policies (SCPs). Cross-account access would be a more suitable choice if you only have two accounts to manage, but not for multiple accounts.</p><p>The option that says: <strong>Providing access to externally authenticated users via Identity Federation and setting up an IAM role to specify permissions for users from each department whose identity is federated from your organization or a third-party identity provider</strong> is incorrect. This option is focused on the Identity Federation authentication set up for your AWS accounts but not the IAM policy management for multiple AWS accounts. A combination of AWS Organizations and Service Control Policies (SCPs) is a better choice compared to this option.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p></div>"
	},
	{
		"question": "<p>A company has several virtual machines on its on-premises data center hosting its three-tier web application. The company wants to migrate the application to AWS to take advantage of the benefits of cloud computing. The following are the company requirements for the migration process:</p><p>- The virtual machine images from the on-premises data center must be imported to AWS.</p><p>- The changes on the on-premises servers must be synchronized to the AWS servers until the production cutover is completed.</p><p>- Have minimal downtime during the production cutover.</p><p>- The root volumes and data volumes (containing Terabytes of data) of the VMs must be migrated to AWS.</p><p>- The migration solution must have minimal operational overhead.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a job on AWS Server Migration Service (SMS) to migrate the root volumes of the virtual machines to AWS. Import the data volumes using the AWS CLI import-snapshot command. Launch Amazon EC2 instances based on the images created from AWS SMS and attach the imported data volumes. After successful testing, perform a final replication before the cutover. Launch new instances based on the updated AMIs and attach the corresponding data volumes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Leverage both AWS Application Discovery Service and AWS Migration Hub to group the on-premises VMs as an application. Write an AWS CLI script that uses VM Import/Export to import the VMs as AMIs. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, perform a final virtual machine import before the cutover. Launch new instances based on the updated AMIs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write an AWS CLI script that uses VM Import/Export to migrate the virtual machines. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, re-run the script to perform a final replication before the cutover. Launch new instances based on the updated AMIs.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a job on AWS Server Migration Service (SMS) to migrate the virtual machines to AWS. Create a replication job for each application tier to sync the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from AWS SMS. After successful testing, perform a final replication before the cutover and launch new instances based on the updated AMIs.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Server Migration Service</strong> automates the migration of your on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud. AWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2. Working with AMIs, you can easily test and update your cloud-based images before deploying them in production.</p><p>By using AWS SMS to manage your server migrations, you can:</p><p><strong>- Simplify the cloud migration process.</strong> You can begin migrating a group of servers with just a few clicks in the AWS Management Console. After the migration has initiated, AWS SMS manages all the complexities of the migration process, including automatically replicating volumes of live servers to AWS and creating new AMIs periodically. You can quickly launch EC2 instances from AMIs in the console.</p><p><strong>- Orchestrate multi-server migrations.</strong> AWS SMS orchestrates server migrations by allowing you to schedule replications and track the progress of a group of servers that constitutes an application. You can schedule initial replications, configure replication intervals, and track progress for each server using the console. When you launch a migrated application, you can apply customized configuration scripts that run during startup.</p><p><strong>- Test server migrations incrementally.</strong> With support for incremental replication, AWS SMS allows fast, scalable testing of migrated servers. Because AWS SMS replicates incremental changes to your on-premises servers and transfers only the delta to the cloud, you can test small changes iteratively and save on network bandwidth.</p><p><strong>- Support the most widely used operating systems.</strong> AWS SMS supports the replication of operating system images containing Windows, as well as several major Linux distributions.</p><p><strong>- Minimize downtime.</strong> Incremental AWS SMS replication minimizes the business impact associated with application downtime during the final cutover.</p><p>AWS Server Migration Service is designed to simplify the end-to-end server migration process. AWS SMS currently supports the migration of on-premises virtual machines (VMs) as an agentless service using a virtual appliance. AWS SMS is an ideal solution to use when you are planning a scaled migration from VMware environments to AWS where the downtime, agentless tools, incremental replication, and testing the application before the cutover are critical considerations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_sms.png\"></p><p>Therefore, the correct answer is: <strong>Create a job on AWS Server Migration Service (SMS) to migrate the virtual machines to AWS. Create a replication job for each application tier to sync the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from AWS SMS. After successful testing, perform a final replication before the cutover and launch new instances based on the updated AMIs.</strong></p><p>The option that says: <strong>Write an AWS CLI script that uses VM Import/Export to migrate the virtual machines. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, re-run the script to perform a final replication before the cutover. Launch new instances based on the updated AMIs</strong> is incorrect. AWS VM Import/Export does not support synching incremental changes from the on-premises environment to AWS. You will need to import the VM again as a whole after you make changes to the on-premises environment. This requires a lot of time and adds more operational overhead.</p><p>The option that says: <strong>Create a job on AWS Server Migration Service (SMS) to migrate the root volumes of the virtual machines to AWS. Import the data volumes using the AWS CLI import-snapshot command. Launch Amazon EC2 instances based on the images created from AWS SMS and attach the imported data volumes. After successful testing, perform a final replication before the cutover. Launch new instances based on the updated AMIs and attach the corresponding data volumes</strong> is incorrect. This may be possible but creating manual snapshots of the data volumes requires more operational overhead. AWS SMS supports up to 16TB volumes so you can use it to migrate the data volumes as well.</p><p>The option that says: <strong>Leverage both AWS Application Discovery Service and AWS Migration Hub to group the on-premises VMs as an application. Write an AWS CLI script that uses VM Import/Export to import the VMs as AMIs. Schedule the script to run at regular intervals to synchronize the changes from the on-premises environment to AWS. Launch Amazon EC2 instances based on the images created from VM Import/Export. After successful testing, perform a final virtual machine import before the cutover. Launch new instances based on the updated AMIs</strong> is incorrect. The AWS Application Discovery Service plans migration projects by gathering information about the on-premises data center and all discovered data are stored in your AWS Migration Hub. This is similar to the other option for VM Import/Export as you will need to import the VM again as a whole after you make changes on the on-premises environment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html\">https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html</a></p><p><a href=\"https://aws.amazon.com/blogs/apn/aws-server-migration-service-server-migration-to-the-cloud-made-easy/\">https://aws.amazon.com/blogs/apn/aws-server-migration-service-server-migration-to-the-cloud-made-easy/</a></p><p><a href=\"https://docs.aws.amazon.com/server-migration-service/latest/userguide/application-migration.html\">https://docs.aws.amazon.com/server-migration-service/latest/userguide/application-migration.html</a></p><p><br></p><p><strong>Check out this AWS Server Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-server-migration-service-sms/?src=udemy\">https://tutorialsdojo.com/aws-server-migration-service-sms/</a></p><p><br></p><p><strong>AWS Migration Services Overview:</strong></p><p><a href=\"https://youtu.be/yqNBkFMnsL8\">https://youtu.be/yqNBkFMnsL8</a></p></div>"
	},
	{
		"question": "<p>Four large banks in the country have collaborated to create a secure, simple-to-use, mobile payment app that enables users to easily transfer money and pay bills without much hassle. With the new mobile payment app, anyone can easily pay another person, split the bill with their friends, or pay for their coffee in an instant with just a few taps in the app. The payment app is available on both Android and iOS devices, including a web portal that is deployed in AWS using OpsWorks Stacks and EC2 instances. It was a big success with over 5 million users nationwide and has over 1000 transactions every hour. After one year, a new feature that will enable the users to store their credit card information in the app is ready to be added to the existing web portal. However, due to PCI-DSS compliance, the new version of the APIs and web portal cannot be deployed to the existing application stack.</p><p>How would the solutions architect deploy the new web portal for the mobile app without having any impact on 5 million users?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Deploy the new web portal using a Blue/Green deployment strategy with AWS CodeDeploy and Lambda in which the green environment represents the current web portal version serving production traffic while the blue environment is staged in running a different version of the web portal."
			},
			{
				"correct": false,
				"answer": "Create a new stack that contains the latest version of the web portal. Using Route 53 service, direct all the incoming traffic to the new stack at once so that all the customers get to access new features."
			},
			{
				"correct": false,
				"answer": "Forcibly upgrade the existing application stack in Production to be PCI-DSS compliant. Once done, deploy the new version of the web portal on the existing application stack."
			},
			{
				"correct": true,
				"answer": "Deploy a new OpsWorks stack that contains a new layer with the latest web portal version. Shift traffic between existing stack and new stack, running different versions of the web portal using Blue/Green deployment strategy by using Route53. Route only a small portion of incoming production traffic to use the new application stack while maintaining the old application stack. Check the features of the new portal; once it's 100% validated, slowly increase incoming production traffic to the new stack. If there are issues on the new stack, change Route53 to revert to old stack."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Blue/green deployments</strong> provide near zero-downtime release and rollback capabilities. The fundamental idea behind blue/green deployment is to shift traffic between two identical environments that are running different versions of your application. The blue environment represents the current application version serving production traffic. In parallel, the green environment is staged running a different version of your application. After the green environment is ready and tested, production traffic is redirected from blue to green. If any problems are identified, you can roll back by reverting traffic back to the blue environment.</p><p><strong>AWS OpsWorks</strong> has the concept of stacks, which are logical groupings of AWS resources (EC2 instances, Amazon RDS, Elastic Load Balancing, and so on) that have a common purpose and should be logically managed together. Stacks are made of one or more layers. A layer represents a set of EC2 instances that serve a particular purpose, such as serving applications or hosting a database server. When a data store is part of the stack, you should be aware of certain data management challenges.</p><p>Next, create the green environment/stack with the newer version of the application. At this point, the green environment is not receiving any traffic. If Elastic Load Balancing needs to be pre-warmed, you can do it at this time.</p><p>When it’s time to promote the green environment/stack into production, update DNS records to point to the green environment/stack’s load balancer. You can also do this DNS flip gradually by using the Amazon Route 53 weighted routing policy.</p><p>To implement this technique in AWS OpsWorks, bring up the blue environment/stack with the current version of the application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_opsworks_blue_green.png\"></p><p>Therefore, the correct answer is: <strong>Deploy a new OpsWorks stack that contains a new layer with the latest web portal version. Shift traffic between existing stack and new stack, running different versions of the web portal using Blue/Green deployment strategy by using Route53. Route only a small portion of incoming production traffic to use the new application stack while maintaining the old application stack. Check the features of the new portal; once it's 100% validated, slowly increase incoming production traffic to the new stack. If there are issues on the new stack, change Route53 to revert to the old stack.</strong></p><p>The option that says: <strong>Forcibly upgrade the existing application stack in Production to be PCI-DSS compliant. Once done, deploy the new version of the web portal on the existing application stack</strong> is incorrect because if you forcibly deploy the new web portal to the existing application stack and there is an issue on the deployment, then there would be some inevitable downtime in order for you to fix the system and revert back to the original version. It is better to use blue/green deployments instead.</p><p>The option that says:<strong> Create a new stack that contains the latest version of the web portal. Using Route 53 service, direct all the incoming traffic to the new stack at once so that all the customers get to access new features</strong> is incorrect. Although the current application stack can still be used if something goes wrong, the risk of service disruption is quite high when you direct all incoming traffic to the new portal. If something did go wrong, there would be a downtime in order for you to switch back to the old stack.</p><p>The option that says: <strong>Deploy the new web portal using a Blue/Green deployment strategy with AWS CodeDeploy and Lambda in which the green environment represents the current web portal version serving production traffic while the blue environment is staged in running a different version of the web portal </strong>is incorrect. Although it mentioned Blue/Green deployment, the application stack doesn't mention about serverless computing at all. Hence, Lambda is irrelevant in this situation and in addition, the description of the blue and green environments are actually switched. The blue environment represents the current application while the green represents the new one.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\">https://d0.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/clone-a-stack-in-aws-opsworks-and-update-dns.html\">https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/clone-a-stack-in-aws-opsworks-and-update-dns.html</a></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html</a></p><p><br></p><p><strong>Check out this AWS OpsWOrks Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-opsworks/?src=udemy\">https://tutorialsdojo.com/aws-opsworks/</a></p></div>"
	},
	{
		"question": "<p>A media company hosts its entire infrastructure on the AWS cloud. There is a requirement to copy information to or from the shared resources from another AWS account. The solutions architect has to provide the other account access to several AWS resources such as Amazon S3, AWS KMS, and Amazon ES in the form of a list of AWS account ID numbers. In addition, the user in the other account should still work in the trusted account and there is no need to give up his or her user permissions in place of the role permissions. The solutions architect must also set up a solution that continuously assesses, audits, and monitors the policy configurations.</p><p>Which of the following is the MOST suitable type of policy that you should use in this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up a service-linked role with an identity-based policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up cross-account access with a resource-based Policy. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a service-linked role with a service control policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up cross-account access with a user-based policy configuration. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For some AWS services, you can grant cross-account access to your resources. To do this, you attach a policy directly to the resource that you want to share, instead of using a role as a proxy. The resource that you want to share must support resource-based policies. Unlike a user-based policy, a resource-based policy specifies who (in the form of a list of AWS account ID numbers) can access that resource.</p><p>Cross-account access with a resource-based policy has some advantages over a role. With a resource that is accessed through a resource-based policy, the user still works in the trusted account and does not have to give up his or her user permissions in place of the role permissions. In other words, the user continues to have access to resources in the trusted account at the same time as he or she has access to the resource in the trusting account. This is useful for tasks such as copying information to or from the shared resource in the other account.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_config_timeline.png\"></p><p>AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.</p><p>Hence, the option that says: <strong>Set up cross-account access with a resource-based Policy. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is correct.</p><p>The option that says: <strong>Set up cross-account access with a user-based policy. configuration. Use AWS Config rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is incorrect because a user-based policy maps the access to a certain IAM user and not to a certain AWS resource.</p><p>The option that says: <strong>Set up a service-linked role with an identity-based policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is incorrect because a service-linked role is just a unique type of IAM role that is linked directly to an AWS service. In addition, it is the AWS Config service, and not the AWS Systems Manager, that enables you to assess, audit, and evaluate the configurations of your AWS resources.</p><p>The option that says: <strong>Set up a service-linked role with a service control policy. Use AWS Systems Manager rules to periodically audit changes to the IAM policy and monitor the compliance of the configuration</strong> is incorrect because a service control policy is primarily used in AWS Organizations and not for cross-account access. Service-linked roles are predefined by the service and include all the permissions that the service requires to call other AWS services on your behalf. This is not suitable for providing access to your resources to other AWS accounts, unlike cross-account access. You should also use AWS Config, and not AWS Systems Manager, to periodically audit changes to the IAM policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html</a></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p></div>"
	},
	{
		"question": "<p>A company wants to host its internal web application in AWS. The front-end uses Docker containers and it connects to a MySQL instance as the backend database. The company plans to use Amazon ECS to reduce the overhead in managing the servers. The application should allow employees to access company documents, which are accessed frequently for the first 3 months and then rarely after that. As part of the company policy, these documents must be retained for at least five years. Because this is an internal web application, the company wants to have the lowest possible cost.</p><p>Which of the following implementations is the most cost-effective solution?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use Amazon EC2 Spot instances for the ECS cluster. Ensure that Spot Instance draining is enabled on the ECS agent config. Use On-Demand instances for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months and will delete objects older than five years.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon EC2 On-Demand instances for the ECS cluster. Use On-Demand instances as well for the Amazon RDS database and its read replicas. Create an Amazon EFS volume that is mounted on the EC2 instances to store the company documents. Create a cron job that will copy the documents to Amazon S3 Glacier after three months and then create a bucket lifecycle policy that will delete objects older than five years.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use Amazon EC2 Spot instances for the ECS cluster. Ensure that Spot Instance draining is enabled on the ECS agent config. Use Reserved instance for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months and will delete objects older than five years.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon EC2 On-Demand instances for the ECS cluster. Use Spot instances for the Amazon RDS database and its read replicas. Create an encrypted ECS volume on the EC2 hosts that is shared to the containers to store the company documents. Set up a cron job that will delete the files after five years.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>Spot Instance</strong> is an unused Amazon EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2, and adjusted gradually based on the long-term supply of and demand for Spot Instances. You can register Spot Instances to your Amazon ECS clusters. Amazon EC2 terminates, stops, or hibernates your Spot Instance when the Spot price exceeds the maximum price for your request or capacity is no longer available. Amazon EC2 provides a Spot Instance interruption notice, which gives the instance a two-minute warning before it is interrupted. If Amazon ECS Spot Instance draining is enabled on the instance, ECS receives the Spot Instance interruption notice and places the instance in DRAINING status.</p><p>When a container instance is set to <code>DRAINING</code>, Amazon ECS prevents new tasks from being scheduled for placement on the container instance. Service tasks on the draining container instance that are in the PENDING state are stopped immediately. If there are container instances in the cluster that are available, replacement service tasks are started on them. Spot Instance draining is disabled by default and must be manually enabled by adding the line <code>ECS_ENABLE_SPOT_INSTANCE_DRAINING=true</code> on your <code>/etc/ecs/ecs.config</code> file.</p><p>Within the Spot provisioning model, you can provide an allocation strategy of either “Diversified” or “Lowest Price” which will define how the EC2 Spot Instances are provisioned. The recommended best practice is to select the “<strong>Diversified</strong>” strategy, to maximize provisioning choices, while reducing the costs. When this is combined with Spot Instance draining, you can allow your Spot instances to drain connections gracefully while having enough time for the cluster to spawn other Spot instance types to handle the load. When configured correctly, you can significantly reduce downtime of Spot instances or eliminate downtime entirely.</p><p><img src=\"https://media.tutorialsdojo.com/sap_asg_mixed_instances.png\"></p><p>Therefore, the correct answer is: <strong>Use Amazon EC2 Spot instances for the ECS cluster. Ensure that Spot Instance draining is enabled on the ECS agent config. Use Reserved instance for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months, and will delete objects older than five years.</strong> With a diversified Spot instance type and Spot instance draining, you can allow your ECS cluster to spawn other EC2 instance types automatically to handle the load at a very low cost. Reserved instances are recommended cost-saving to RDS instances that will be running continuously for years.</p><p>The option that says: <strong>Use Amazon EC2 On-Demand instances for the ECS cluster. Use Spot instances for the Amazon RDS database and its read replicas. Create an encrypted ECS volume on the EC2 hosts that is shared to the containers to store the company documents. Set up a cron job that will delete the files after five years</strong> is incorrect. Storing company documents on the EC2 instances will require more disk space on instances, which is unnecessary and expensive. Using Spot instances for RDS instances is not recommended as this will cause major downtime or data loss in case AWS terminates your spot instance.</p><p>The option that says: <strong>Use Amazon EC2 On-Demand instances for the ECS cluster. Use On-Demand instances as well for the Amazon RDS database and its read replicas. Create an Amazon EFS volume that is mounted on the EC2 instances to store the company documents. Create a cron job that will copy the documents to Amazon S3 Glacier after three months and then create a bucket lifecycle policy that will delete objects older than five years</strong> is incorrect. This is possible, however, using EFS volumes is more expensive than just storing the files on Amazon S3 in the first place.</p><p>The option that says: <strong>Use Amazon EC2 Spot instances for the ECS cluster. Ensure that Spot Instance draining is enabled on the ECS agent config. Use On-Demand instances for the Amazon RDS database and its read replicas. Create an encrypted Amazon S3 bucket to store the company documents. Create a bucket lifecycle policy that will move the documents to Amazon S3 Glacier after three months, and will delete objects older than five years</strong> is incorrect. This option is also possible, however, using on-demand instances for continuously running RDS instances is expensive. You can save costs by using Reserved instances for Amazon RDS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/container-instance-spot.html</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/containers-for-less/get-started/\">https://aws.amazon.com/ec2/spot/containers-for-less/get-started/</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/getting-started/\">https://aws.amazon.com/ec2/spot/getting-started/</a></p><p><br></p><p><strong>Check out this Amazon ECS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-container-service-amazon-ecs/</a></p></div>"
	},
	{
		"question": "<p>A startup in the fashion industry is building a mobile app that showcases its latest fashion accessories and gadgets. The marketing manager hired a famous model with millions of Instagram followers to promote their new products and hence, it is expected that the app will be a huge hit once it is launched in the market. It must have the ability to automatically scale to handle millions of views of its static contents and to allow users to store their own photos of themselves wearing fashionable accessories with a maximum of 100 characters for captions.</p><p>In this scenario, which of the following solutions would fulfill this requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Use Cognito to handle user authentication and management.</p><p>\n2. Use an RDS database to store user data.</p><p>\n3. Create an S3 bucket to store all of the user photos and other static files.</p><p>\n4. Distribute the static contents using CloudFront to improve scalability.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Use Cognito to handle user authentication and management.</p><p>\n2. Launch a DynamoDB table to store user data.</p><p>\n3. Create an S3 bucket to store all of the user photos and other static files.</p><p>\n4. Distribute the static contents using CloudFront to improve scalability.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Configure an on-premises Active Directory (AD) server utilizing SAML 2.0 to manage the application users inside of the on-premises AD server.</p><p>\n2. Develop a custom code that authenticates against the LDAP server. </p><p>\n3. Use DynamoDB as the main database of the app and S3 as the scalable object storage.</p><p>\n4. Grant an IAM role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table.</p><p>\n5. Distribute the static contents using CloudFront to improve scalability.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Set up a SAML 2.0-based Federation that lets the users sign into the app using a third-party identity provider such as Amazon, Google, or Facebook.</p><p>\n2. Set up an RDS database and an S3 bucket to store the photos.</p><p>\n3. Use the AssumeRoleWithWebIdentity API call to assume the IAM role containing the proper permissions to communicate with the RDS database. </p><p>\n4. Distribute the static contents using S3 to improve scalability.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Cognito</strong> scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0. <strong>Amazon S3</strong> provides a scalable object storage solution. <strong>Amazon CloudFront</strong> is a Content Delivery Network that helps your system to handle the millions of user views and finally, <strong>DynamoDB</strong> is a scalable database that can handle millions of records.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_dynamodb_cloudfront.png\"></p><p>In this scenario, the top priority is scalability to meet the upcoming surge of users of your mobile app. Since all of these services can provide the scalability that your mobile app needs, the best option that you can choose is the following:</p><p><strong>1. Use Cognito to handle user authentication and management.</strong></p><p><strong>2. Launch a DynamoDB table to store user data.</strong></p><p><strong>3. Create an S3 bucket to store all of the user photos and other static files.</strong></p><p><strong>4. Distribute the static contents using CloudFront to improve scalability.</strong></p><p>The following option is incorrect because a SAML 2.0-based federation doesn't use social media logins and it is better to use CloudFront to distribute static contents compared to an S3 bucket:</p><p><strong>1. Set up a SAML 2.0-based Federation that lets the users sign into the app using a third party identity provider such as Amazon, Google or Facebook.</strong></p><p><strong>2. Set up an RDS database and an S3 bucket to store the photos.</strong></p><p><strong>3. Use the AssumeRoleWithWebIdentity API call to assume the IAM role containing the proper permissions to communicate with the RDS database.</strong></p><p><strong>4. Distribute the static contents using S3 to improve scalability.</strong></p><p>The following option is incorrect because using an on-premise Active Directory server is not a suitable solution for this scenario:</p><p><strong>1. Configure an on-premises Active Directory (AD) server utilizing SAML 2.0 to manage the application users inside of the on-premises AD server.</strong></p><p><strong>2. Develop a custom code that authenticates against the LDAP server.</strong></p><p><strong>3. Use DynamoDB as the main database of the app and S3 as the scalable object storage.</strong></p><p><strong>4. Grant an IAM role assigned to the STS token to allow the end-user to access the required data in the DynamoDB table.</strong></p><p><strong>5. Distribute the static contents using CloudFront to improve scalability.</strong></p><p>Remember that this is a public mobile app and hence, it is better to set up a Web Identity Federation instead</p><p>The following option is incorrect because RDS is not as scalable as DynamoDB:</p><p><strong>1. Use Cognito to handle user authentication and management.</strong></p><p><strong>2. Use an RDS database to store user data.</strong></p><p><strong>3. Create an S3 bucket to store all of the user photos and other static files.</strong></p><p><strong>4. Distribute the static contents using CloudFront to improve scalability.</strong></p><p>Based on the type of data you will be storing, a relational database like RDS is not suitable to store simple data sets such as a 100-character caption which does not need multiple tables or relational data models.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/protect-public-clients-for-amazon-cognito-by-using-an-amazon-cloudfront-proxy/\">https://aws.amazon.com/blogs/security/protect-public-clients-for-amazon-cognito-by-using-an-amazon-cloudfront-proxy/</a></p><p><a href=\"https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/\">https://aws.amazon.com/blogs/mobile/building-fine-grained-authorization-using-amazon-cognito-user-pools-groups/</a></p><p><br></p><p><strong>Check out these Amazon Cognito and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A logistics company plans to host its web application on AWS to allow customers to track their shipping worldwide. The web application will have a multi-tier setup – Amazon EC2 instances for running the web and application layer, Amazon S3 bucket for hosting the static content, and a NoSQL database. The company plans to provision the resources in the us-east-1 region. The company also wants to have a second site hosted on us-west-1 region for disaster recovery. The second site must have the same copy of data from the primary site and the failover should be as quick as possible when the primary region becomes unavailable. Failing back to the primary region should be done automatically once it becomes available again.</p><p>Which of the following solutions should the Solutions Architect implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create a DynamoDB global table spanning both regions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create an Amazon CloudFront distribution. Set the S3 bucket as the origin for static files and multi-origins for the web and application tiers. For the database tier, create an Amazon DynamoDB table in each region and regularly backup to an Amazon S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Provision the same Auto Scaling group of EC2 instances for web and application tiers in both regions using AWS Service Catalog. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Ensure that Amazon Route 53 health check is enabled on the primary region and update the public DNS zone entry with the secondary region in case of an outage. For the database tier, create an Amazon RDS for MySQL and enable cross-region replication to create a read-replica on the secondary region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create an Amazon Aurora global database spanning the two regions.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudFormation</strong> helps AWS customers implement an Infrastructure as Code model. Instead of setting up their environments and applications by hand, they build a template and use it to create all of the necessary resources, collectively known as a CloudFormation stack. This model removes opportunities for manual error, increases efficiency, and ensures consistent configurations over time.</p><p>With <strong>Amazon CloudFormation StackSets</strong> you can define an AWS resource configuration in a CloudFormation template and then roll it out across multiple AWS accounts and/or Regions with a couple of clicks. You can use this to set up a baseline level of AWS functionality that addresses the cross-account and cross-region scenarios. Once you have set this up, you can easily expand coverage to additional accounts and regions.</p><p>Amazon S3 Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same AWS account or by different accounts. The objects may be replicated to a single destination bucket or multiple destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source bucket. <strong>Amazon S3 Cross-Region Replication (CRR)</strong> is used to copy objects across Amazon S3 buckets in different AWS Regions. CRR is helpful if you want to meet compliance requirements such as the need to have a copy of your data on another location.</p><p><strong>Amazon DynamoDB global tables</strong> provide you with a fully managed, multi-region and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application's business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\"></p><p>Therefore, the correct answer is: <strong>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create a DynamoDB global table spanning both regions.</strong></p><p>The option that says: <strong>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create Amazon Route 53 DNS zone entries with a failover routing policy and set the us-west-1 region as the secondary site. For the database tier, create an Amazon Aurora global database spanning the two regions</strong> is incorrect. The application is designed for a NoSQL database so a DynamoDB global table is recommended for this, not an Amazon Aurora global database.</p><p>The option that says: <strong>Provision the same Auto Scaling group of EC2 instances for web and application tiers in both regions using AWS Service Catalog. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Ensure that Amazon Route 53 health check is enabled on the primary region and update the public DNS zone entry with the secondary region in case of an outage. For the database tier, create an Amazon RDS for MySQL and enable cross-region replication to create a read-replica on the secondary region</strong> is incorrect. You don’t have to manually update the public DNS zone entry with the secondary region, you just have to configure the failover routing policy in Route 53 to automatically failover to the secondary site and vice versa. MySQL is not recommended as the application is designed for a NoSQL database.</p><p>The option that says: <strong>Create the same resources of Auto Scaling group of EC2 instances for web and application tiers on both regions using AWS CloudFormation StackSets. Enable Amazon S3 cross-Region on the S3 bucket to asynchronously replicate the contents to the secondary region. Create an Amazon CloudFront distribution. Set the S3 bucket as the origin for static files and multi-origins for the web and application tiers. For the database tier, create an Amazon DynamoDB table in each region and regularly backup to an Amazon S3 bucket</strong> is incorrect. You can’t reliably sync DynamoDB tables on two regions with just backups from S3. There will be a delay on the backup and restore. You should use DynamoDB global tables instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/\">https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resources-across-multiple-aws-accounts-and-regions/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><br></p><p><strong>Check out these Amazon S3, Amazon DynamoDB, and CloudFormation StackSet Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation-stacksets-and-nested-stacks/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation-stacksets-and-nested-stacks/</a></p></div>"
	},
	{
		"question": "<p>A leading media company has a hybrid architecture where its on-premises data center is connected to AWS via a Direct Connect connection. They also have a repository of over 50-TB digital videos and media files. These files are stored on their on-premises tape library and are used by their Media Asset Management (MAM) system. Due to the sheer size of their data, they want to implement an automated catalog system that will enable them to search their files using facial recognition. A catalog will store the faces of the people who are present in these videos including a still image of each person. Eventually, the media company would like to migrate these media files to AWS including the MAM video contents. </p><p>Which of the following options provides a solution which uses the LEAST amount of ongoing management overhead and will cause MINIMAL disruption to the existing system?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Integrate the file system of your local data center to AWS Storage Gateway by setting up a file gateway appliance on-premises. Utilize the MAM solution to extract the media files from the current data store and send them into the file gateway. Build a collection using Amazon Rekognition by populating a catalog of faces from the processed media files. Use an AWS Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media file from the S3 bucket which is backing the file gateway, retrieve the needed metadata, and finally, persist the information into the MAM solution.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate all of the media files from the on-premises library into an EBS volume mounted on a large EC2 instance. Install an open-source facial recognition tool in the instance like OpenFace or OpenCV. Process the media files to retrieve the metadata and push this information into the MAM solution. Lastly, copy the media files to an S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a tape gateway appliance on-premises and connect it to your AWS Storage Gateway. Configure the MAM solution to fetch the media files from the current archive and push them into the tape gateway to be stored in Amazon Glacier. Using Amazon Rekognition, build a collection from the catalog of faces. Utilize a Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time, retrieve the required metadata, and push the metadata into the MAM solution.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Kinesis Video Streams to set up a video ingestion stream and with Amazon Rekognition, build a collection of faces. Stream the media files from the MAM solution into Kinesis Video Streams and configure the Amazon Rekognition to process the streamed files. Launch a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Finally, configure the stream to store the files in an S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Rekognition</strong> can store information about detected faces in server-side containers known as collections. You can use the facial information that's stored in a collection to search for known faces in images, stored videos, and streaming videos. Amazon Rekognition supports the <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_IndexFaces.html\">IndexFaces</a> operation. You can use this operation to detect faces in an image and persist information about facial features that are detected into a collection. This is an example of a <em>storage-based</em> API operation because the service persists information on the server.</p><p>To store facial information, you must first create (<a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateCollection.html\">CreateCollection</a>) a face collection in one of the AWS Regions in your account. You specify this face collection when you call the <code>IndexFaces</code> operation. After you create a face collection and store facial feature information for all faces, you can search the collection for face matches. To search for faces in an image, call <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_SearchFacesByImage.html\">SearchFacesByImage</a>. To search for faces in a stored video, call <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_StartFaceSearch.html\">StartFaceSearch</a>. To search for faces in a streaming video, call <a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/API_CreateStreamProcessor.html\">CreateStreamProcessor</a>.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rekognition_screen.jpg\"></p><p>AWS Storage Gateway offers file-based, volume-based, and tape-based storage solutions. With a tape gateway, you can cost-effectively and durably archive backup data in GLACIER or DEEP_ARCHIVE. A tape gateway provides a virtual tape infrastructure that scales seamlessly with your business needs and eliminates the operational burden of provisioning, scaling, and maintaining a physical tape infrastructure.</p><p>You can run AWS Storage Gateway either on-premises as a VM appliance, as a hardware appliance, or in AWS as an Amazon Elastic Compute Cloud (Amazon EC2) instance. You deploy your gateway on an EC2 instance to provision iSCSI storage volumes in AWS. You can use gateways hosted on EC2 instances for disaster recovery, data mirroring, and providing storage for applications hosted on Amazon EC2.</p><p>Hence, the correct answer is: <strong>Integrate the file system of your local data center to AWS Storage Gateway by setting up a file gateway appliance on-premises. Utilize the MAM solution to extract the media files from the current data store and send them into the file gateway. Build a collection using Amazon Rekognition by populating a catalog of faces from the processed media files. Use an AWS Lambda function to invoke Amazon Rekognition Javascript SDK to have it fetch the media file from the S3 bucket which is backing the file gateway, retrieve the needed metadata, and finally, persist the information into the MAM solution.</strong></p><p>The option that says: <strong>Migrate all of the media files from the on-premises library into an EBS volume mounted on a large EC2 instance. Install an open-source facial recognition tool in the instance like OpenFace or OpenCV. Process the media files to retrieve the metadata and push this information into the MAM solution. Lastly, copy the media files to an S3 bucket</strong> is incorrect. This entails a lot of ongoing management overhead instead of just using Amazon Rekognition. Moreover, it is more suitable to use the AWS Storage Gateway service rather than an EBS Volume.</p><p>The option that says: <strong>Set up a tape gateway appliance on-premises and connect it to your AWS Storage Gateway. Configure the MAM solution to fetch the media files from the current archive and push them into the tape gateway to be stored in Amazon Glacier. Using Amazon Rekognition, build a collection from the catalog of faces. Utilize a Lambda function which invokes the Rekognition Javascript SDK to have Amazon Rekognition process the video directly from the tape gateway in real-time, retrieve the required metadata, and push the metadata into the MAM solution</strong> is incorrect. Although this solution uses the right combination of AWS Storage Gateway and Amazon Rekognition, take note that you can't directly fetch the media files from your tape gateway in real-time since this is backed up using Glacier. Although the on-premises data center is using a tape gateway, you can still set up a solution to use a file gateway in order to properly process the videos using Amazon Rekognition. Keep in mind that the tape gateway in AWS Storage Gateway service is primarily used as an archive solution.</p><p>The option that says: <strong>Use Amazon Kinesis Video Streams to set up a video ingestion stream and with Amazon Rekognition, build a collection of faces. Stream the media files from the MAM solution into Kinesis Video Streams and configure the Amazon Rekognition to process the streamed files. Launch a stream consumer to retrieve the required metadata, and push the metadata into the MAM solution. Finally, configure the stream to store the files in an S3 bucket </strong>is incorrect. You won't be able to connect your tape gateway directly to your Kinesis Video Streams service. You need to use AWS Storage Gateway first.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/collections.html\">https://docs.aws.amazon.com/rekognition/latest/dg/collections.html</a></p><p><a href=\"https://aws.amazon.com/storagegateway/file/\">https://aws.amazon.com/storagegateway/file/</a></p><p><br></p><p><strong>Check out this Amazon Rekognition Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A startup is building a web app that lets users post photos of good deeds in their neighborhood with a 143-character caption/article. The developers decided to write the application in ReactJS, a popular javascript framework so that it would run on the broadest range of browsers, mobile phones, and tablets. The app should provide access to Amazon DynamoDB to store the caption. The initial prototype shows that there aren't large spikes in usage.</p><p>Which option provides the most cost-effective and scalable architecture for this application?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or from any other popular social sites and use the <code>AssumeRoleWithWebIdentity</code> API of STS to generate temporary credentials. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in Amazon S3 and DynamoDB. Serve your web app out of an S3 bucket enabled as a website.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials to an IAM user. This will allow GET and PUT operations to DynamoDB. Serve your web application from an NGINX server hosted in a fleet of EC2 instances that are load-balanced and auto-scaled. Your EC2 instances are configured with an IAM role that allows GET and PUT operations in DynamoDB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or any other popular social site. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in DynamoDB. Serve your web application from an NGINX server hosted on a fleet of EC2 instances, with a load balancer and auto-scaling. Add an IAM role to the EC2 instance to allow GET and PUT operations to DynamoDB tables.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance. This will provide signed credentials to an IAM user allowing GET and PUT operations in the DynamoDB table and the S3 bucket. You serve your mobile application out of an S3 bucket enabled as a website.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>ReactJS</strong> is a modern framework for creating front-ends for your web applications. From the users perspective, this website is dynamic since it asks for users to login and users can upload images to it. But since it is written on ReactJS framework, it is basically just a bunch of static files and the web browser renders the page dynamically.</p><p>We can have full-blown web pages like this and be hosted on Amazon S3 (with enabling static website hosting) with no problems. From Amazon S3's perspective, it is hosting a static website, since all files that are on the bucket are static files only. But ReactJS allows the client’s web browser to interpret these static files and render the webpage dynamically for the user.</p><p><strong>Amazon Cognito</strong> follows the OIDC specification to authenticate users of web and mobile apps. Users can sign in directly through the Amazon Cognito hosted UI or through a federated identity provider, such as Amazon, Facebook, Apple, or Google. The hosted UI workflows include sign-in and sign-up, password reset, and MFA. Since not all customer workflows are the same, you can customize Amazon Cognito workflows at key points with AWS Lambda functions, allowing you to run code without provisioning or managing servers. After a user authenticates, Amazon Cognito returns standard OIDC tokens. You can use the user profile information in the ID token to grant your users access to your own resources or you can use the tokens to grant access to APIs hosted by Amazon API Gateway. You can also exchange the tokens for temporary AWS credentials to access other AWS services.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_web_token.jpg\"></p><p>If you don't use Amazon Cognito, then you choose to write a custom code or app that interacts with a web IdP (Login with Amazon, Facebook, Google, or any other OIDC-compatible IdP) and then call the <code>AssumeRoleWithWebIdentity</code> API to trade the authentication token you get from those IdPs for AWS temporary security credentials. If you have already used this approach for existing apps, you can continue to use it. You can also deploy your app in the S3 bucket.</p><p>The option that says: <strong>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or any other popular social sites and use the </strong><code><strong>AssumeRoleWithWebIdentity</strong></code><strong> API of STS to generate temporary credentials. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in Amazon S3 and DynamoDB. Serve your web app out of an S3 bucket enabled as a website</strong> is correct because it authenticates the application via a federated identity provider such as Google, Facebook, Amazon, or other social sites. It sets up proper permission for DynamoDB access and hosts the website in S3. Plus, it also uses STS and <code>AssumeRoleWithWebIdentity</code> API which provides a better authentication.</p><p>The option that says: <strong>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) on an EC2 instance. This will provide signed credentials to an IAM user allowing GET and PUT operations in the DynamoDB table and the S3 bucket. You serve your mobile application out of an S3 bucket enabled as a website</strong> is incorrect. The Token Vending Machine (STS Service) is implemented on just a single EC2 instance. This poses a single point of failure which means that the architecture is not highly available and not fault-tolerant. This is not a scalable solution either as the instance can become the performance bottleneck.</p><p>The option that says: <strong>Configure the ReactJS client with temporary credentials from the Security Token Service using a Token Vending Machine (TVM) to provide signed credentials to an IAM user. This will allow GET and PUT operations to DynamoDB. Serve your web application from an NGINX server hosted in a fleet of EC2 instances that are load-balanced and auto-scaled. Your EC2 instances are configured with an IAM role that allows GET and PUT operations in DynamoDB</strong> is incorrect. Although this solution is highly-available and scalable, deploying EC2 instances in an auto-scaled environment is not as a cost-effective solution as the S3 website.</p><p>The option that says: <strong>Register the web application with a Web Identity Provider such as Google, Facebook, Amazon, or from any other popular social site. Create an IAM role for that web provider and set up permissions for the IAM role to allow GET and PUT operations in DynamoDB. Serve your web application from an NGINX server hosted on a fleet of EC2 instances, with a load balancer and auto-scaling. Add an IAM role to the EC2 instance to allow GET and PUT operations to DynamoDB tables</strong> is incorrect because it does not mention any security token service that generates temporary credentials. Furthermore, deploying EC2 instances in an auto-scaled environment, albeit scalable, is not as cost-effective as the S3 website.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-add-authentication-single-page-web-application-with-amazon-cognito-oauth2-implementation/\">https://aws.amazon.com/blogs/security/how-to-add-authentication-single-page-web-application-with-amazon-cognito-oauth2-implementation/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-integrate-apps.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-integrate-apps.html</a></p><p><a href=\"https://aws.amazon.com/blogs/mobile/deploy-a-react-app-to-s3-and-cloudfront-with-aws-mobile-hub/\">https://aws.amazon.com/blogs/mobile/deploy-a-react-app-to-s3-and-cloudfront-with-aws-mobile-hub/</a></p><p><br></p><p><strong>Check out these Amazon S3 and Amazon Cognito Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p></div>"
	},
	{
		"question": "<p>A company has a hybrid set up for its mobile application. The on-premises data center hosts a 3TB MySQL database server that handles the write-intensive requests from the application. The on-premises network is connected to the AWS VPC with a VPN. On AWS, the serverless application runs on AWS Lambda and API Gateway with an Amazon DynamoDB table used for saving user preferences. The application scales well as more users are using the mobile app. The user traffic is unpredictable but there is an average increase of about 20% each month. A few months into operation, the company noticed the exponential increase of costs for AWS Lambda. The Solutions Architect noticed that the Lambda execution time averages 4.5 minutes and most of that is wait time due to latency when calling the on-premises data MySQL server.</p><p>Which of the following solutions should the Solutions Architect implement to reduce the overall cost?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>1. Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</p><p>2. Configure API caching on Amazon API Gateway to reduce the overall number of invocations to the Lambda functions.</p><p>3. Gradually lower the timeout and memory properties of the Lamdba functions without increasing the execution time.</p><p>4. Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity based on user traffic.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Migrate the on-premises MySQL database server to Amazon RDS for<br>&nbsp; &nbsp; MySQL. Enable Multi-AZ to ensure high availability.</p><p>2. Create a CloudFront distribution with the API Gateway as the origin to<br>&nbsp; &nbsp; cache the API responses and reduce the Lambda invocations.</p><p>3. Gradually lower the timeout and memory properties of the Lamdba<br>&nbsp; &nbsp; functions without increasing the execution time.</p><p>4. Configure Auto Scaling on Amazon DynamoDB to automatically adjust<br>&nbsp; &nbsp; the capacity with user traffic and enable DynamoDB Accelerator to cache<br>&nbsp; &nbsp; frequently accessed records.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Provision an AWS Direct Connect connection from the on-premises data<br>&nbsp; &nbsp; center to Amazon VPC instead of a VPN to significantly reduce the<br>&nbsp; &nbsp; network latency to the MySQL server.</p><p>2. Configure caching on the mobile application to reduce the overall AWS<br>&nbsp; &nbsp; Lambda function calls.</p><p>3. Gradually lower the timeout and memory properties of the Lamdba<br>&nbsp; &nbsp; functions without increasing the execution time.</p><p>4. Add an Amazon Elasticache cluster in front of DynamoDB to cache the<br>&nbsp; &nbsp; frequently accessed records.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Provision an AWS Direct Connect connection from the on-premises data<br>&nbsp; &nbsp; center to Amazon VPC instead of a VPN to significantly reduce the<br>&nbsp; &nbsp; network latency to the MySQL server.</p><p>2. Create a CloudFront distribution with the API Gateway as the origin to<br>&nbsp; &nbsp; cache the API responses and reduce the Lambda invocations.</p><p>3. Convert the Lambda functions to run them on Amazon EC2 Reserved<br>&nbsp; &nbsp; Instances. Use Auto Scaling on peak time with a combination of Spot<br>&nbsp; &nbsp; instances to further reduce costs.</p><p>4. Configure Auto Scaling on Amazon DynamoDB to automatically adjust<br>&nbsp; &nbsp; the capacity with user traffic.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon API Gateway</strong> is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. API developers can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud. As an API Gateway API developer, you can create APIs for use in your own client applications.</p><p>You can enable API caching in Amazon API Gateway to cache your endpoint's responses. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.</p><p>When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. The default TTL value for API caching is 300 seconds. The maximum TTL value is 3600 seconds. TTL=0 means caching is disabled.</p><p>With <strong>AWS Lambda</strong>, you pay only for what you use. You are charged based on the number of requests for your functions and the duration, the time it takes for your code to execute. Lambda counts a request each time it starts executing in response to an event notification or invoke call, including test invokes from the console.</p><p>Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 1ms*. The price depends on the amount of memory you allocate to your function. In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function.</p><p><strong>Auto Scaling for DynamoDB</strong> helps automate capacity management for your tables and global secondary indexes. You simply specify the desired target utilization and provide upper and lower bounds for read and write capacity. DynamoDB will then monitor throughput consumption using Amazon CloudWatch alarms and then will adjust provisioned capacity up or down as needed.</p><p>Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. When the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity.</p><p>Therefore, the following option is correct:</p><p><strong>- Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</strong></p><p><strong>- Configure API caching on Amazon API Gateway to reduce the overall number of invocations to the Lambda functions.</strong></p><p><strong>- Gradually lower the timeout and memory properties of the Lamdba functions without increasing the execution time.</strong></p><p><strong>- Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity based on user traffic.</strong></p><p>Migrating the on-premises MySQL server to Amazon RDS provides the best latency for the Lambda functions which will significantly reduce the cost for execution time. API Gateway can cache the API request to reduce the Lambda invocation which can reduce the cost further. Auto Scaling for DynamoDB also reduces the cost by provisioning capacity depending on the current user traffic.</p><p>The following option is incorrect:</p><p><strong>- Provision an AWS Direct Connect connection from the on-premises data center to Amazon VPC instead of a VPN to significantly reduce the network latency to the MySQL server.</strong></p><p><strong>- Configure caching on the mobile application to reduce the overall AWS Lambda function calls.</strong></p><p><strong>- Gradually lower the timeout and memory properties of the Lamdba functions without increasing the execution time.</strong></p><p><strong>- Add an Amazon Elasticache cluster in front of DynamoDB to cache the frequently accessed records.</strong></p><p>Although the Direct Connect connection can reduce the network latency compared to a VPN connection, provisioning a Direct Connection just for a single application is not economical. Having the MySQL server hosted in AWS offers even far better network latency. Provisioning an Elasticache cluster also increases the cost. Caching the API requests should be done on the API Gateway, not on the mobile app itself.</p><p>The following option is incorrect:</p><p><strong>- Provision an AWS Direct Connect connection from the on-premises data center to Amazon VPC instead of a VPN to significantly reduce the network latency to the MySQL server.</strong></p><p><strong>- Create a CloudFront distribution with the API Gateway as the origin to cache the API responses and reduce the Lambda invocations.</strong></p><p><strong>- Convert the Lambda functions to run them on Amazon EC2 Reserved Instances. Use Auto Scaling on peak time with a combination of Spot instances to further reduce costs.</strong></p><p><strong>- Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity with user traffic.</strong></p><p>Provisioning a Direct Connection just for the application is not economical even if it offers better latency than a VPN connection. Caching the API requests should be done on the API Gateway, and not on CloudFront. EC2 Reserve instances could be more expensive than Lambda functions when application traffic is low.</p><p>The following option is incorrect:</p><p><strong>- Migrate the on-premises MySQL database server to Amazon RDS for MySQL. Enable Multi-AZ to ensure high availability.</strong></p><p><strong>- Create a CloudFront distribution with the API Gateway as the origin to cache the API responses and reduce the Lambda invocations.</strong></p><p><strong>- Gradually lower the timeout and memory properties of the Lamdba functions without increasing the execution time.</strong></p><p><strong>- Configure Auto Scaling on Amazon DynamoDB to automatically adjust the capacity with user traffic and enable DynamoDB Accelerator to cache frequently accessed records.</strong></p><p>Caching the API requests should be done on the API Gateway, and not on CloudFront. DynamoDB Accelerator is used for caching requests if you need response times in microseconds. This is very expensive.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html</a></p><p><a href=\"https://aws.amazon.com/api-gateway/faqs/\">https://aws.amazon.com/api-gateway/faqs/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/\">https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/</a></p><p><a href=\"https://aws.amazon.com/lambda/pricing/\">https://aws.amazon.com/lambda/pricing/</a></p><p><br></p><p><strong>Check out these Amazon API Gateway, Amazon DynamoDB, and AWS Lambda Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p></div>"
	},
	{
		"question": "<p>A data analytics startup has been chosen to develop a data analytics system that will track all statistics in the Fédération Internationale de Football Association (FIFA) World Cup, which will also be used by other 3rd-party analytics sites. The system will record, store and provide statistical data reports about the top scorers, goal scores for each team, average goals, average passes, average yellow/red cards per match, and many other details. FIFA fans all over the world will frequently access the statistics reports every day and thus, it should be durably stored, highly available, and highly scalable. In addition, the data analytics system will allow the users to vote for the best male and female FIFA player as well as the best male and female coach. Due to the popularity of the FIFA World Cup event, it is projected that there will be over 10 million queries on game day and could spike to 30 million queries over the course of time.</p><p>Which of the following is the most cost-effective solution that will meet these requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Launch a MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.</p><p>\n2. Generate the FIFA reports by querying the Read Replica.</p><p>\n3. Configure a daily job that performs a daily table cleanup.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Generate the FIFA reports from MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.<br><br> 2. Set up a batch job that puts reports in an S3 bucket.</p><p>\n3. Launch a CloudFront distribution to cache the content with a TTL set to expire objects daily.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Launch a Multi-AZ MySQL RDS instance. </p><p>\n2. Query the RDS instance and store the results in a DynamoDB table. </p><p>\n3. Generate reports from DynamoDB table.<br><br>4. Delete the old DynamoDB tables every day.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Launch a MySQL database in Multi-AZ RDS deployments configuration. </p><p>\n2. Configure the application to generate reports from ElastiCache to improve the read performance of the system. </p><p>\n3. Utilize the default expire parameter for items in ElastiCache.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, you are required to have the following:</p><p>A durable storage for the generated reports.</p><p>A database that is highly available and can scale to handle millions of queries.</p><p>A Content Delivery Network that can distribute the report files to users all over the world.</p><p><strong>Amazon S3</strong> is object storage built to store and retrieve any amount of data from anywhere. It’s a simple storage service that offers industry leading durability, availability, performance, security, and virtually unlimited scalability at very low costs.</p><p><strong>Amazon RDS</strong> provides high availability and failover support for DB instances using Multi-AZ deployments. In a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. The primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.</p><p>Amazon RDS uses the MariaDB, Microsoft SQL Server, MySQL, Oracle, and PostgreSQL DB engines' built-in replication functionality to create a special type of DB instance called a <strong>read replica</strong> from a source DB instance. The source DB instance becomes the primary DB instance. Updates made to the primary DB instance are asynchronously copied to the read replica.</p><p><img src=\"https://media.tutorialsdojo.com/sap_RDSreadreplica_async.jpg\"></p><p><strong>Amazon CloudFront</strong> is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p>Hence, the following option is the best solution that satisfies all of these requirements:</p><p><strong>1. Generate the FIFA reports from MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.</strong></p><p><strong>2. Set up a batch job that puts reports in an S3 bucket.</strong></p><p><strong>3. Launch a CloudFront distribution to cache the content with a TTL set to expire objects daily.</strong></p><p>In the above, S3 provides durable storage; Multi-AZ RDS with Read Replicas provide a scalable and highly available database and CloudFront provides the CDN.</p><p>The following option is incorrect:</p><p><strong>1. Launch a MySQL database in Multi-AZ RDS deployments configuration with Read Replicas.</strong></p><p><strong>2. Generate the FIFA reports by querying the Read Replica.</strong></p><p><strong>3. Configure a daily job that performs a daily table cleanup.</strong></p><p>Although the database is scalable and highly available, it neither has any durable data storage nor a CDN.</p><p>The following option is incorrect:</p><p><strong>1. Launch a MySQL database in Multi-AZ RDS deployments configuration.</strong></p><p><strong>2. Configure the application to generate reports from ElastiCache to improve the read performance of the system.</strong></p><p><strong>3. Utilize the default expire parameter for items in ElastiCache.</strong></p><p>Although this option handles and provides a better read capability for the system, it is still lacking a durable storage and a CDN.</p><p>The following option is incorrect:</p><p><strong>1. Launch a Multi-AZ MySQL RDS instance.</strong></p><p><strong>2. Query the RDS instance and store the results in a DynamoDB table.</strong></p><p><strong>3. Generate reports from DynamoDB table.</strong></p><p><strong>4. Delete the old DynamoDB tables every day.</strong></p><p>The above is not a cost-effective solution to maintain both RDS and a DynamoDB instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/multi-az/\">https://aws.amazon.com/rds/details/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>The department of education just recently decided to leverage on AWS cloud infrastructure to supplement their current on-premises network. They are building a new learning portal that teaches kids basic computer science concepts and provides innovative gamified courses for teenagers where they can gain higher rankings, power-ups and badges. A Solutions Architect is instructed to build a highly available cloud infrastructure in AWS with multiple Availability Zones. The department wants to increase the application’s reliability and gain actionable insights using application logs. A Solutions Architect needs to aggregate logs, automate log analysis for errors and immediately notify the IT Operations team when errors breached a certain threshold. </p><p>Which of the following is the MOST suitable solution that the Architect should implement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon CloudWatch Events. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon Athena to monitor the metric filter and immediately notify the IT Operations team for any issues.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Create a CloudWatch Alarm that monitors the metric filter and immediately notify the IT Operations team for any issues.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Download and install the AWS X-Ray agent in the on-premises servers and send the logs to AWS Lambda to turn log data into numerical metrics that identify and measure application errors. Store the metrics data in Systems Manager Parameter Store. Create a CloudWatch Alarm that monitors the metric and immediately notify the IT Operations team for any issues.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Download and install the Amazon Kinesis agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon QuickSight to monitor the metric filter in CloudWatch and immediately notify the IT Operations team for any issues.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudWatch</strong> monitors your Amazon Web Services (AWS) resources and the applications you run on AWS in real-time. You can use CloudWatch to collect and track metrics, which are variables you can measure for your resources and applications. The CloudWatch home page automatically displays metrics about every AWS service you use. You can additionally create custom dashboards to display metrics about your custom applications, and display custom collections of metrics that you choose.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudwatch_overview.png\"></p><p>After the <strong>CloudWatch Logs agent</strong> begins publishing log data to Amazon CloudWatch, you can begin searching and filtering the log data by creating one or more metric filters. Metric filters define the terms and patterns to look for in log data as it is sent to CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into numerical CloudWatch metrics that you can graph or set an alarm on. You can use any type of CloudWatch statistic, including percentile statistics, when viewing these metrics or setting alarms.</p><p>You can create alarms which watch metrics and send notifications or automatically make changes to the resources you are monitoring when a threshold is breached. For example, you can monitor the CPU usage and disk reads and writes of your Amazon EC2 instances and then use this data to determine whether you should launch additional instances to handle increased load. You can also use this data to stop under-used instances to save money. With CloudWatch, you gain system-wide visibility into resource utilization, application performance, and operational health.</p><p>Hence, the correct answer is: <strong>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Create a CloudWatch Alarm that monitors the metric filter and immediately notify the IT Operations team for any issues.</strong></p><p>The option that says:<strong><em> </em>Download and install the Amazon Kinesis agent in the on-premises servers and send the logs to Amazon CloudWatch Logs. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon QuickSight to monitor the metric filter in CloudWatch and immediately notify the IT Operations team for any issues</strong> is incorrect. You have to use Amazon CloudWatch agent instead of an Amazon Kinesis agent to send the logs to Amazon CloudWatch Logs. It is also better to use CloudWatch Alarms to monitor the metric filter than to use Amazon QuickSight.</p><p>The option that says:<strong> Download and install the AWS X-Ray agent in the on-premises servers and send the logs to AWS Lambda to turn log data into numerical metrics that identify and measure application errors. Store the metrics data in Systems Manager Parameter Store. Create a CloudWatch Alarm that monitors the metric and immediately notify the IT Operations team for any issues </strong>is incorrect. You have to use Amazon CloudWatch agent instead of an AWS X-Ray agent to send the logs to Amazon CloudWatch Logs. AWS X-Ray is primarily used for debugging applications and not for monitoring. It is not suitable to use a Lambda function to track and process the metrics and use the Systems Manager Parameter Store to keep the metrics data. These operations can simply be done by CloudWatch.</p><p>The option that says:<strong><em> </em>Download and install the Amazon CloudWatch agent in the on-premises servers and send the logs to Amazon CloudWatch Events. Create a metric filter in CloudWatch to turn log data into numerical metrics to identify and measure application errors. Use Amazon Athena to monitor the metric filter and immediately notify the IT Operations team for any issues</strong> is incorrect. You have to send the logs to CloudWatch Logs and not CloudWatch Events. It is also better to use CloudWatch Alarm to monitor the metric filter and immediately notify the IT Operations team for any issues.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringPolicyExamples.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p></div>"
	},
	{
		"question": "<p>There was a major incident that occurred in your company wherein the web application that you are supporting unexpectedly went down in the production environment. Upon investigation, it was found that a junior DevOps engineer terminated the EC2 instance in production which caused the disruption of service. Only the Solutions Architects should be allowed to stop or terminate instances in the production environment. You also found out that there are a lot of developers who have full access to your production AWS account.</p><p>Which of the following options will fix this security vulnerability in your cloud architecture and prevent this kind of failure from happening again? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Modify the IAM policy of the developers to require MFA before deleting EC2 instances."
			},
			{
				"correct": true,
				"answer": "Add tags to the EC2 instances in the production environment and add resource-level permissions to the developers with an explicit deny on terminating the instance which contains the tag. "
			},
			{
				"correct": false,
				"answer": "<p>Attach a <code>PowerUserAccess</code> AWS managed policy to the developers.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Modify the associated IAM Role assigned to the developers by removing the policy that allows them to terminate EC2 instances in production.</p>"
			},
			{
				"correct": false,
				"answer": "Replace the Security Group of all of the EC2 instances in Production to prevent developers from accessing it."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>To help you manage your instances, images, and other Amazon EC2 resources, you can optionally assign your own metadata to each resource in the form of tags. Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment. This is useful when you have many resources of the same type—you can quickly identify a specific resource based on the tags you've assigned to it. For example, you could define a set of tags for your account's Amazon EC2 instances that help you track each instance's owner and stack level.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iam_identity_vs_resource_policy.png\"></p><p>Take note that MFA is just an additional layer of security but it won't totally prevent the users from terminating the EC2 instances.</p><p><strong>Adding tags to the EC2 instances in the production environment and adding resource-level permissions to the developers with an explicit deny on terminating the instance which contains the tag</strong> is correct because it identifies the instances based on its environment using a tag which creates a resource level permission that explicitly denies anyone from terminating certain instances hosted in production.</p><p><strong>Modifying the associated IAM Role assigned to the developers by removing the policy that allows them to terminate EC2 instances in production</strong> is correct because changing the IAM Role assigned to the developers to revoke their privilege of terminating EC2 instances will certainly prevent the issue from happening again.</p><p><strong>Modifying the IAM policy of the developers to require MFA before deleting EC2 instances</strong> is incorrect because MFA is just an additional layer of security given to the users when logging into AWS and accessing the resources. However, an MFA alone cannot prevent the users from performing resource level actions, such as terminating the instance.</p><p><strong>Replacing the Security Group of all of the EC2 instances in Production to prevent developers from accessing it</strong> is incorrect because a security group is mainly used to secure and control the traffic coming in and out of the EC2 instances. In this scenario, it is best to modify the IAM policy of all of the developers and add tags to the instances in the production environment.</p><p><strong>Attaching a </strong><code><strong>PowerUserAccess</strong></code><strong> AWS managed policy to the developers</strong> is incorrect because it will only provide more permissive access to terminate EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html</a></p><p><a href=\"https://aws.amazon.com/iam/details/mfa/\">https://aws.amazon.com/iam/details/mfa/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_tags.html</a></p><p><br></p><p><strong>Check out these Amazon EC2 and AWS IAM Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company runs a popular photo-sharing site hosted on the AWS cloud. There are user complaints about the frequent downtime of the site considering the hefty price for using their service. The company is using a MySQL RDS instance to record user details and other data analytics. A standard S3 storage class bucket is used to store the photos and user metadata, which are frequently accessed only in the first month. The website is also capable of immediately retrieving the images no matter how long they were stored. The RDS instance is always affected and sometimes goes down when there is a problem in the Availability Zone. The solutions architect was tasked to analyze the current architecture and to solve the user complaints about the website. In addition, the solutions architect should also implement a system that automatically discovers, classifies, and protects personally identifiable information (PII) data in the Amazon S3 bucket.</p><p>Which of the following options offers the BEST solution for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use Amazon Inspector to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Replace the S3 Standard bucket with an Infrequent Access storage class. Re-configure the existing database to use RDS Read Replicas.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Inspector to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Use a lifecycle policy in S3 to move the old photos to Amazon S3 Glacier Deep Archive after a month. Re-configure the existing database to use RDS Multi-AZ Deployments.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Macie to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Replace the S3 bucket with EBS Volumes and use Redshift instead of RDS.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use Amazon Macie to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Use a lifecycle policy in S3 to move the old photos to Infrequent Access storage class after a month. Re-configure the existing database to use RDS Multi-AZ Deployments.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Macie</strong> is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved. The fully managed service continuously monitors data access activity for anomalies and generates detailed alerts when it detects the risk of unauthorized access or inadvertent data leaks. Today, Amazon Macie is available to protect data stored in Amazon S3, with support for additional AWS data stores soon.</p><p><img src=\"https://media.tutorialsdojo.com/sap_macie_dashboard.jpg\"></p><p>In this scenario, the best way to solve the issue is to: <strong>Use Amazon Macie to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Use a lifecycle policy in S3 to move the old photos to Infrequent Access storage class after a month. Re-configure the existing database to use RDS Multi-AZ Deployments</strong>. Keep in mind that the website should be able to immediately retrieve the images no matter how long they were stored. This means that you should not archive the images.</p><p>The option that says: <strong>Use Amazon Macie to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Replace the S3 bucket with EBS Volumes and use Redshift instead of RDS</strong> is incorrect because EBS Volumes are not as scalable as S3 and Redshift is not a suitable storage option as it is mainly used as a petabyte-scale data warehouse service.</p><p>The option that says: <strong>Use Amazon Inspector to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Use a lifecycle policy in S3 to move the old photos to Amazon S3 Glacier Deep Archive after a month. Re-configure the existing database to use RDS Multi-AZ Deployments</strong> is incorrect because although it is right to use Amazon RDS Multi-AZ deployments configuration, the use of Glacier Deep Archive is not preferred for this scenario. Take note that Glacier is primarily used for archiving and the retrieval times are much slower compared to S3. Implementing this solution means that the users will have to wait a long time to retrieve their photos. In addition, Amazon Inspector is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS, especially in Amazon EC2 instances. You have to use Amazon Macie instead.</p><p>The option that says: <strong>Use Amazon Inspector to automatically discover, classify, and protect personally identifiable information (PII) data in the Amazon S3 bucket. Replace the S3 Standard bucket with an Infrequent Access storage class. Re-configure the existing database to use RDS Read Replicas</strong> is incorrect because although it is right to use Infrequent Access storage class, the use of Read Replica is not suitable for this scenario. Although it may improve availability, you are only limited to use read operations when you use Read Replicas. This means when the primary database is down, the Read Replica would not be available to register new users since read operations are only allowed. Moreover, you have to use Amazon Macie and not Amazon Inspector, since this is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/s3/storage-classes/\">https://aws.amazon.com/s3/storage-classes/</a></p><p><a href=\"https://aws.amazon.com/macie/\">https://aws.amazon.com/macie/</a></p><p><a href=\"https://aws.amazon.com/macie/details/\">https://aws.amazon.com/macie/details/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-macie/?src=udemy\">https://tutorialsdojo.com/amazon-macie/</a></p></div>"
	},
	{
		"question": "<p>A company is implementing cloud best practices for its infrastructure. The Solutions Architect is using AWS CloudFormation templates for infrastructure-as-code of its two-tier web application. The application frontend is hosted on an Auto Scaling group of Amazon EC2 instances while the database is an Amazon RDS for MySQL instance. For security purposes, the database password must be rotated every 60 days.</p><p>Which of the following solutions is the MOST secure way to store and retrieve the database password for the web application?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>On the CloudFormation template, create a database password parameter. Add a <code>UserData</code> property to reference the password parameter in the initialization script of the Auto Scaling group’s launch configuration using the <code>Ref</code> intrinsic function. Save the password inside the EC2 instance upon its launch. Use the <code>Ref</code> intrinsic function to reference the parameter as the value of the <code>MasterUserPassword</code> property in the <code>AWS::RDS::DBInstance</code> resource.</p>"
			},
			{
				"correct": true,
				"answer": "<p>On the CloudFormation template, create an AWS Secrets Manager secret resource for the database password. Modify the application to retrieve the database password from Secrets Manager when it launches. Use a dynamic reference for the secret resource to be placed as the value of the <code>MasterUserPassword</code> property of the <code>AWS::RDS::DBInstance</code> resource.</p>"
			},
			{
				"correct": false,
				"answer": "<p>On the CloudFormation template, create an AWS Secrets Manager secret resource for the database password. Add a <code>UserData</code> property to reference the secret resource in the initialization script of the Auto Scaling group’s launch configuration using the <code>Ref</code> intrinsic function. Use the <code>Ref</code> intrinsic function to reference the secret resource as the value of the <code>MasterUserPassword</code> property in the <code>AWS::RDS::DBInstance</code> resource.</p>"
			},
			{
				"correct": false,
				"answer": "<p>On the CloudFormation template, create an encrypted parameter using AWS Systems Manager Parameter Store for the database password. Add a <code>UserData</code> property to reference the encrypted parameter in the initialization script of the Auto Scaling group’s launch configuration. Use the <code>Fn::GetAtt</code> intrinsic function to reference the encrypted parameter as the value of the <code>MasterUserPassword/ property in the AWS::RDS::DBInstanc resource.</code></p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Secrets Manager</strong> integrates with <strong>AWS CloudFormation</strong> so you can create and retrieve secrets securely using CloudFormation. This integration makes it easier to automate provisioning your AWS infrastructure. For example, without any code changes, you can generate unique secrets for your resources with every execution of your CloudFormation template. This also improves the security of your infrastructure by storing secrets securely, encrypting automatically, and enabling rotation more easily. Secrets Manager helps you protect the secrets needed to access your applications, services, and IT resources.</p><p><strong>CloudFormation</strong> helps you model your AWS resources as templates and execute these templates to provision AWS resources at scale. Some AWS resources require secrets as part of the provisioning process. For example, to provision a MySQL database, you must provide the credentials for the database superuser. You can use Secrets Manager, the AWS dedicated secrets management service, to create and manage such secrets.</p><p><strong>Secrets Manager</strong> makes it easier to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. You can reference Secrets Manager in your CloudFormation templates to create unique secrets with every invocation of your template. By default, Secrets Manager encrypts these secrets with encryption keys that you own and control. Secrets Manager ensures the secret isn’t logged or persisted by CloudFormation by using a dynamic reference to the secret. You can configure Secrets Manager to rotate your secrets automatically without disrupting your applications. Secrets Manager offers built-in integrations for rotating credentials for all Amazon RDS databases and supports extensibility with AWS Lambda so you can meet your custom rotation requirements.</p><p><strong>Dynamic references</strong> provide a compact, powerful way for you to specify external values that are stored and managed in other services, such as the Systems Manager Parameter Store, in your stack templates. When you use a dynamic reference, CloudFormation retrieves the value of the specified reference when necessary during stack and change set operations.</p><p>Secrets Manager resource types supported in CloudFormation:</p><p><code>AWS::SecretsManager::Secret</code> — Create a secret and store it in Secrets Manager.</p><p><code>AWS::SecretsManager::ResourcePolicy</code> — Create a resource-based policy and attach it to a secret. Resource-based policies enable you to control access to secrets.</p><p><code>AWS::SecretsManager::SecretTargetAttachment</code> — Configure Secrets Manager to rotate the secret automatically.</p><p><code>AWS::SecretsManager::RotationSchedule</code> — Define the Lambda function that will be used to rotate the secret.</p><p>Therefore, the correct answer is: <strong>On the CloudFormation template, create an AWS Secrets Manager secret resource for the database password. Modify the application to retrieve the database password from Secrets Manager when it launches. Use a dynamic reference for the secret resource to be placed as the value of the </strong><code><strong>MasterUserPassword</strong></code><strong> property of the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource. </strong>You can use dynamic references in CloudFormation to specify external values that are stored and managed in other services, such as AWS SSM Parameter Store or AWS Secrets Manager. Your application can then retrieve the database password resource on AWS Secrets Manager when it needs to.</p><p>The option that says: <strong>On the CloudFormation template, create a database password parameter. Add a </strong><code><strong>UserData</strong></code><strong> property to reference the password parameter in the initialization script of the Auto Scaling group’s launch configuration using the </strong><code><strong>Ref</strong></code><strong> intrinsic function. Save the password inside the EC2 instance upon its launch. Use the </strong><code><strong>Ref</strong></code><strong> intrinsic function to reference the parameter as the value of the </strong><code><strong>MasterUserPassword</strong></code><strong> property in the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource</strong> is incorrect. Using a normal parameter in CloudFormation is not secure. This will require you to either store the password on the template itself or input it on the AWS web console.</p><p>The option that says: <strong>On the CloudFormation template, create an AWS Secrets Manager secret resource for the database password. Add a </strong><code><strong>UserData</strong></code><strong> property to reference the secret resource in the initialization script of the Auto Scaling group’s launch configuration using the </strong><code><strong>Ref</strong></code><strong> intrinsic function. Use the </strong><code><strong>Ref</strong></code><strong> intrinsic function to reference the secret resource as the value of the </strong><code><strong>MasterUserPassword</strong></code><strong> property in the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource</strong> is incorrect. Using the user data scripts to retrieve the database password may expose the password to the environment of the operating system of the EC2 instance. It is more secure to configure the application to retrieve the secret resource when the application launches so it is not exposed to anything outside the application.</p><p>The option that says: <strong>On the CloudFormation template, create an encrypted parameter using AWS Systems Manager Parameter Store for the database password. Add a </strong><code><strong>UserData</strong></code><strong> property to reference the encrypted parameter in the initialization script of the Auto Scaling group’s launch configuration. Use the </strong><code><strong>Fn::GetAtt</strong></code><strong> intrinsic function to reference the encrypted parameter as the value of the </strong><code><strong>MasterUserPassword</strong></code><strong> property in the </strong><code><strong>AWS::RDS::DBInstance</strong></code><strong> resource</strong> is incorrect. AWS Systems Manager Parameter can store an encrypted password, however, it can generate a password that can be referenced in AWS CloudFormation. It also doesn't support automatic rotation of the parameter.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_cloudformation.html\">https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_cloudformation.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-create-and-retrieve-secrets-managed-in-aws-secrets-manager-using-aws-cloudformation-template/\">https://aws.amazon.com/blogs/security/how-to-create-and-retrieve-secrets-managed-in-aws-secrets-manager-using-aws-cloudformation-template/</a></p><p><br></p><p><strong>Check out these AWS Secrets Manager and AWS CloudFormation Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p></div>"
	},
	{
		"question": "<p>An insurance company collects contributions from its clients and invests them in the stock market. Using the on-premises data center, the company ingests raw data feeds from the stock market, transforms it, and sends it to the internal Apache Kafka cluster for processing. The management wants to send the cluster’s output to Amazon Web Services by building a scalable and near-real-time solution that will provide the stock market data to its web application. The application is a critical production component so the solution needs to have a consistent high-performance network.</p><p>Which of the following actions should the solutions architect implement to fulfill the requirements? (Select THREE.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Write a Lambda function to process the Amazon Kinesis data stream and create a WebSocket API in Amazon API Gateway to invoke the function. Send the callback messages to connected clients by using the <code>@connections</code> command for the API.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write a Lambda function to process the Amazon Kinesis data stream and write a GraphQL API in AWS AppSync to invoke the function. Send the callback messages to connected clients by using the <code>@connections</code> command for the API.</p>"
			},
			{
				"correct": false,
				"answer": "<p>To have a consistent performance while being cost-effective, configure a Site-to-Site VPN from the on-premises data center to the AWS VPC.</p>"
			},
			{
				"correct": true,
				"answer": "<p>To have consistent performance, request for an AWS Direct Connect connection from the on-premises data center to the AWS VPC.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Pull the messages from the on-premises Apache Kafka cluster by using a fleet of Amazon EC2 instances in an Auto Scaling Group. Send the data into an Amazon Kinesis Data Stream by using Amazon Kinesis Producer Library.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Fetch the messages from the on-premises Apache Kafka cluster by using a fleet of EC2 instances in an Auto Scaling Group. Send the data into an Amazon Kinesis Data Stream by using Amazon Kinesis Consumer Library.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Direct Connect</strong> makes it easy to establish a dedicated connection from an on-premises network to one or more VPCs in the same region. Using private VIF on AWS Direct Connect, you can establish private connectivity between AWS and your data center, office, or colocation environment, as shown in the following figure. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p><p><img src=\"https://media.tutorialsdojo.com/sap_awsDirectConnect.jpg\"></p><p>A producer puts data records into Amazon Kinesis data streams. For example, a web server sending log data to a Kinesis data stream is a producer. A consumer processes the data records from a stream. To put data into the stream, you must specify the name of the stream, a partition key, and the data blob to be added to the stream. The partition key is used to determine which shard in the stream the data record is added to. All the data in the shard is sent to the same worker that is processing the shard.</p><p>A <strong>WebSocket API</strong> in API Gateway is a collection of WebSocket routes that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can use API Gateway features to help you with all aspects of the API lifecycle, from creation through monitoring your production APIs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_webSockets.jpg\"></p><p>API Gateway WebSocket APIs are bidirectional. A client can send messages to a specific service, and services can independently send messages to clients. This bidirectional behavior enables richer client/service interactions because services can push data to clients without requiring clients to make an explicit request. WebSocket APIs are often used in real-time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms.</p><p>You can use the <code>@connections</code> API from your backend service to send a callback message to a connected client, get connection information or disconnect from the client.</p><p>Therefore, the correct answers are:</p><p><strong>- To have consistent performance, request for an AWS Direct Connection from the on-premises data center to the AWS VPC.</strong></p><p><strong>- Pull the messages from the on-premises Apache Kafka cluster by using a fleet of Amazon EC2 instances in an Auto Scaling Group. Send the data into an Amazon Kinesis Data Stream by using Amazon Kinesis Producer Library.</strong></p><p><strong>- Write a Lambda function to process the Amazon Kinesis data stream and create a WebSocket API in Amazon API Gateway to invoke the function. Send the callback messages to connected clients by using the </strong><code><strong>@connections</strong></code><strong> command for the API.</strong></p><p>The option that says: <strong>Fetch the messages from the on-premises Apache Kafka cluster by using a fleet of EC2 instances in an Auto Scaling Group. Send the data into an Amazon Kinesis Data Stream by using Amazon Kinesis Consumer Library </strong>is incorrect because you should use Amazon Kinesis Producer Library, not Consumer Library.</p><p>The option that says: <strong>Write a Lambda function to process the Amazon Kinesis data stream and write a GraphQL API in AWS AppSync to invoke the function. Send the callback messages to connected clients by using the </strong><code><strong>@connections</strong></code><strong> command for the API</strong> is incorrect because using <code>@connections</code> to have the backend service connect back to the clients is not a feature of the GraphQL API when using AWS AppSync.</p><p>The option that says: <strong>To have a consistent performance while being cost-effective, configure a Site-to-Site VPN from the on-premises data center to the AWS VPC</strong> is incorrect because a Site-to-Site VPN does not provide a reliable and high-performance network connection between the on-premises data center and Amazon VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-how-to-call-websocket-api-connections.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-how-to-call-websocket-api-connections.html</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-producers.html\">https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-producers.html</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-consumers.html\">https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-consumers.html</a></p><p><br></p><p><strong>Check out these AWS Direct Connect and Amazon Kinesis Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
	},
	{
		"question": "<p>A company that manages hundreds of AWS client accounts has created a central logging service running on an Auto Scaling group of Amazon EC2 instances. The logging service receives logs from the client AWS accounts through the connectivity provided by AWS PrivateLink. The interface endpoint for this is available on each of the client AWS accounts. The EC2 instances hosting the logging service are spread on multiple subnets with a Network Load Balancer in front to spread the incoming load. Upon testing, the clients are unable to submit logs through the VPC endpoint.</p><p>Which of the following solutions will most likely resolve the issue? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Ensure that the security group attached to the EC2 instances hosting the logging service allows inbound traffic from the NLB subnet IPs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ensure that the security group attached to the EC2 instances hosting the logging service allows inbound traffic from the IP address block of the clients.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Ensure that the NACL associated with the logging service subnet allows communication to and from the NLB subnets. Ensure that the NACL associated with the NLB subnets allows communication to and from the EC2 instances subnets running the logging service.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ensure that the security group attached to the NLB allows inbound traffic from the interface endpoint subnet.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ensure that the NACL associated with the logging service subnets allows communication to and from the interface endpoint. Ensure that the NACL associated with the interface endpoint subnet allows communication to and from the EC2 instances running the logging service.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you create an Amazon VPC endpoint interface with <strong>AWS PrivateLink</strong>, an Elastic Network Interface is created inside of the subnet that you specify. This interface VPC endpoint (interface endpoint) inherits the network ACL of the associated subnet. You must associate a security group with the interface endpoint to protect incoming and outgoing requests.</p><p><img src=\"https://media.tutorialsdojo.com/sap_privatelink_vpc_endpoint.png\"></p><p>When you associate a Network Load Balancer with an endpoint service, the Network Load Balancer forwards requests to the registered target as if the target was registered by IP address. In this case, the source IP addresses are the private IP addresses of the load balancer nodes. If you have access to the Amazon VPC endpoint service, you must verify that the security group rules and the rules within the network ACL associated with the Network Load Balancer’s targets:</p><p>- Allow communication from the private IP address of the Network Load Balancer.</p><p>- Don't allow communication from the IP address of the client or the interface endpoint.</p><p>To allow communication between clients and the Amazon VPC endpoint, you must create rules within the network ACL associated with the client’s subnet and the subnet associated with the interface endpoint. Be aware of the following limits:</p><p>- Network Load Balancers do not have associated security groups. Therefore, the security groups for your targets must use IP addresses to allow traffic from the load balancer.</p><p>- You cannot use the security groups for clients as a source in the security groups for the targets. Instead, use the client CIDR blocks as sources in the target security groups.</p><p>If you register targets by IP address and do not want to grant access to the entire VPC CIDR, you can grant access to the private IP addresses used by the load balancer nodes. There is one IP address per load balancer subnet.</p><p>Therefore, the correct answer are:</p><p><strong>- Ensure that the NACL associated with the logging service subnet allows communication to and from the NLB subnets. Ensure that the NACL associated with the NLB subnets allows communication to and from the EC2 instances subnets running the logging service.</strong></p><p><strong>- Ensure that the security group attached to the EC2 instances hosting the logging service allows inbound traffic from the NLB subnet IPs.</strong></p><p>The option that says: <strong>Ensure that the NACL associated with the logging service subnets allows communication to and from the interface endpoint. Ensure that the NACL associated with the interface endpoint subnet allows communication to and from the EC2 instances running the logging service</strong> is incorrect because the rules within the network ACL associated with the Network Load Balancer’s targets should not allow direct communication from the IP address of the client or the interface endpoint. A better approach is to ensure that the NACL associated with the NLB subnets allows communication to and from the EC2 instances subnets running the logging service.</p><p>The option that says: <strong>Ensure that the security group attached to the EC2 instances hosting the logging service allows inbound traffic from the IP address block of the clients</strong> is incorrect because the security group attached to the EC2 instances must permit the inbound traffic from the NLB subnet IPs and not the IP address block of the clients. The security group rules associated with the Network Load Balancer’s targets should not allow direct access from the IP address of the client or the interface endpoint.</p><p>The option that says: <strong>Ensure that the security group attached to the NLB allows inbound traffic from the interface endpoint subnet</strong> is incorrect because Network Load Balancers do not have associated security groups. The security groups for your targets must use IP addresses to allow traffic from the load balancer.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-troubleshooting.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/security-network-acl-vpc-endpoint/\">https://aws.amazon.com/premiumsupport/knowledge-center/security-network-acl-vpc-endpoint/</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/security-group-load-balancer/\">https://aws.amazon.com/premiumsupport/knowledge-center/security-group-load-balancer/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#target-security-groups\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#target-security-groups</a></p><p><br></p><p><strong>Application Load Balancer vs Network Load Balancer vs Classic Load Balancer:</strong></p><p><a href=\"https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/?src=udemy\">https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</a></p></div>"
	},
	{
		"question": "A global real estate startup is looking for an option of adding a cost-effective location-based alert to their iOS and Android mobile apps. Their users will receive alerts on their mobile device regarding real estate offers in proximity to their current location and the delivery time for the push notifications should be less than a minute. The existing mobile app has an initial 2 million users worldwide and is rapidly growing. \n\nWhat is the most suitable architecture to use in this scenario?",
		"answers": [
			{
				"correct": false,
				"answer": "Set up an architecture where there is an Auto Scaling group of On-Demand EC2 instances behind an API Gateway that retrieve the relevant offers from RDS. Once the data has been processed, use AWS AppSync to send out the offers to the mobile app."
			},
			{
				"correct": true,
				"answer": "<p>Set up an architecture where the mobile app will send the user's location to an SQS queue and a fleet of On-Demand EC2 instances will retrieve the relevant offers from a DynamoDB table. Once the data has been processed, use AWS SNS Mobile Push to send out the offers to the mobile app.</p>"
			},
			{
				"correct": false,
				"answer": "Set up an architecture where the mobile app will send the user's location to an API Gateway with Lambda functions which process and retrieve the relevant offers from an RDS database. Once the data has been processed, use Amazon Pinpoint to send out the offers to the mobile app. "
			},
			{
				"correct": false,
				"answer": "Set up an architecture where the mobile app will send the user's location to an SQS queue and a fleet of On-Demand EC2 instances will retrieve the relevant offers from an Amazon Aurora database. Once the data has been processed, use AWS Device Farm to send out the offers to the mobile app. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With <strong>Amazon SNS Mobile Push Notifications</strong>, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sns_flow.png\"></p><p>The option that says:<strong> Set up an architecture where the mobile app will send the user's location to an SQS queue and a fleet of On-Demand EC2 instances will retrieve the relevant offers from a DynamoDB table. Once the data has been processed, use AWS SNS Mobile Push to send out the offers to the mobile app</strong> is correct because SQS is a highly scalable, cost-effective solution for carrying out utility tasks such as holding the location of millions of users. In addition, it uses a highly scalable DynamoDB table and a cost-effective AWS SNS Mobile Push service to send push notification messages directly to the mobile apps.</p><p>The option that says: <strong>Set up an architecture where there is an Auto Scaling group of On-Demand EC2 instances behind an API Gateway that retrieve the relevant offers from RDS. Once the data has been processed, use AWS AppSync to send out the offers to the mobile app</strong> is incorrect. Although a combination of On-Demand EC2 instances and API Gateway can provide a scalable computing system, it is wrong to use AWS AppSync for push notification to mobile devices. You should use AWS SNS Mobile Push service instead.</p><p>The option that says: <strong>Set up an architecture where the mobile app will send the user's location to an API Gateway with Lambda functions which process and retrieve the relevant offers from an RDS database. Once the data has been processed, use Amazon Pinpoint to send out the offers to the mobile app</strong> is incorrect. Although it is correct to use Amazon Pinpoint to send push notifications, RDS is not a suitable database for the mobile app because it is not as scalable enough when processing data from various users around the globe, compared with DynamoDB. Usually, mobile applications do not have complicated table relationships hence, it is recommended to use a NoSQL database like DynamoDB.</p><p>The option that says: <strong>Set up an architecture where the mobile app will send the user's location to an SQS queue and a fleet of On-Demand EC2 instances will retrieve the relevant offers from an Amazon Aurora database. Once the data has been processed, use AWS Device Farm to send out the offers to the mobile app</strong> is incorrect because AWS Device Farm is an app testing service and is not used to push notifications to various mobile devices. It only lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real-time.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html\">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/mobile-push-pseudo.html\">https://docs.aws.amazon.com/sns/latest/dg/mobile-push-pseudo.html</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/sns-mobile-application-as-subscriber.html\">https://docs.aws.amazon.com/sns/latest/dg/sns-mobile-application-as-subscriber.html</a></p><p><br></p><p><strong>Check out this Amazon SNS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sns/?src=udemy\">https://tutorialsdojo.com/amazon-sns/</a></p></div>"
	},
	{
		"question": "<p>As part of the Corporate Social Responsibility of the tech company, the development team created an online learning system for a public university. The application architecture uses an Application Load Balancer in front of two On-Demand EC2 instances located in two Availability Zones. The only remaining requirement is to secure the new website with an HTTPS connection.</p><p>Which of the following option is the most cost-effective and easiest way to complete the online learning system?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Generate a Public Certificate in ACM. Configure the Application Load Balancer to use the Public Certificate to handle HTTPS requests."
			},
			{
				"correct": false,
				"answer": "Generate a Private Certificate in ACM. Configure the two EC2 instances to use the Private Certificate to handle HTTPS requests."
			},
			{
				"correct": false,
				"answer": "Generate a Public Certificate in ACM. Configure the two EC2 instances to use the Public Certificate to handle HTTPS requests."
			},
			{
				"correct": false,
				"answer": "Generate a Private Certificate in ACM. Configure the Application Load Balancer to use the Private Certificate to handle HTTPS requests."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With <strong>AWS Certificate Manager</strong>, you can generate public or private SSL/TLS certificates that you can use to secure your site. Public SSL/TLS certificates provisioned through AWS Certificate Manager are free. You pay only for the AWS resources that you create to run your application. For private certificates, the ACM Private Certificate Authority (CA) is priced along two dimensions: (1) You pay a monthly fee for the operation of each private CA until you delete it and (2) you pay for the private certificates you issue each month.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_acm_certs.png\"></p><p>Public certificates generated from ACM can be used on Amazon CloudFront, Elastic Load Balancing, or Amazon API Gateway but not directly on EC2 instances, unlike private certificates.</p><p>Hence, <strong>generating a Public Certificate in ACM and configuring the Application Load Balancer to use the Public Certificate to handle HTTPS requests</strong> is correct in this scenario as a public certificate does not cost anything and you can configure this certificate with the Application Load Balancer.</p><p>The option that says: <strong>Generate a Private Certificate in ACM. Configure the Application Load Balancer to use the Private Certificate to handle HTTPS requests</strong> is incorrect because this solution entails an additional cost. Remember that you have to pay a monthly fee for the operation of each private CA until you delete it. A more cost-effective solution is to use a public certificate instead since this is free of charge.</p><p>The option that says: <strong>Generate a Public Certificate in ACM. Configure the two EC2 instances to use the Public Certificate to handle HTTPS requests</strong> is incorrect because you cannot export public certificates from ACM and use them with your EC2 instances. You can only use the public certificate from ACM in Amazon CloudFront, Elastic Load Balancing, and Amazon API Gateway.</p><p>The option that says: <strong>Generate a Private Certificate in ACM. Configure the two EC2 instances to use the Private Certificate to handle HTTPS requests</strong> is incorrect. Although you can export private certificates from ACM and use them with EC2 instances, using this type of certificate still costs more than a public certificate.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/certificate-manager/pricing/\">https://aws.amazon.com/certificate-manager/pricing/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://aws.amazon.com/certificate-manager/faqs/</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p><p><br></p><p><strong>How can I add certificates for websites to the ELB using Amazon Certificate Manager?</strong></p><p><a href=\"https://youtu.be/pTQrAZbHmRU\">https://youtu.be/pTQrAZbHmRU</a></p></div>"
	},
	{
		"question": "<p>A company runs a cryptocurrency analytics website and uses a CloudFront distribution with a custom domain name (tutorialsdojo.com) to speed up the loading time of the site. Since the data being distributed are quite confidential, the management instructed the solutions architect to require HTTPS communication between the viewers (web visitors) and the CloudFront distribution. Additionally, it is required to improve the performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers.</p><p>Which of the following are the recommended actions to accomplish the above requirement? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Configure your origin to add a <code>Cache-Control max-age directive</code> to your objects and specify the longest practical value for <code>max-age</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Integrate your CloudFront web distribution with Amazon Elasticsearch (ES) and Kibana to improve the performance of your origin servers and to visualize the cache data in real time.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Import an SSL/TLS certificate from a third-party certificate authority into a private S3 bucket with versioning and MFA enabled.</p>"
			},
			{
				"correct": true,
				"answer": "Use an SSL/TLS certificate provided by AWS Certificate Manager (ACM)."
			},
			{
				"correct": false,
				"answer": "<p>Associate your CloudFront web distribution with Lambda@Edge which provides automatic scalability from a few requests per day to thousands of requests per second.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can configure one or more cache behaviors in your CloudFront distribution to require HTTPS for communication between viewers and CloudFront. You also can configure one or more cache behaviors to allow both HTTP and HTTPS, so that CloudFront requires HTTPS for some objects but not for others. The configuration steps depend on which domain name you're using in object URLs:</p><p>- If you're using the domain name that CloudFront assigned to your distribution, such as d111111abcdef8.cloudfront.net, you change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication. In that configuration, CloudFront provides the SSL/TLS certificate.</p><p>- If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\"></p><p>You can improve performance by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content; that is, by improving the cache hit ratio for your distribution. To increase your cache hit ratio, you can configure your origin to add a <code><strong>Cache-Control max-age</strong></code><strong> </strong>directive to your objects, and specify the longest practical value for <code><strong>max-age</strong></code>. The shorter the cache duration, the more frequently CloudFront forwards another request to your origin to determine whether the object has changed and, if so, to get the latest version.</p><p>In this scenario, the correct answers are <strong>using an SSL/TLS certificate provided by AWS Certificate Manager (ACM)</strong> and <strong>configuring your origin to add a </strong><code><strong>Cache-Control max-age directive</strong></code><strong> to your objects and specifying the longest practical value for </strong><code><strong>max-age</strong></code> as these two can meet the requirement as explained above.</p><p><strong>Associating your CloudFront web distribution with Lambda@Edge which provides automatic scalability from a few requests per day to thousands of requests per second</strong> is incorrect because Lambda@Edge is an extension of AWS Lambda which lets you execute functions that customize the content that CloudFront delivers. This feature does not improve the cache hit ratio of your CloudFront distribution.</p><p><strong>Integrating your CloudFront web distribution with Amazon Elasticsearch (ES) and Kibana to improve the performance of your origin servers and to visualize the cache data in real time</strong> is incorrect because the Amazon Elasticsearch Service is just a fully managed service that makes it easier to deploy, secure, and operate Elasticsearch in AWS. This service will not improve the cache hit ratio of your CloudFront distribution. The built-in Kibana support only provides a way to get faster and better insights into your data and thus, not related to this scenario.</p><p><strong>Importing an SSL/TLS certificate from a third-party certificate authority into a private S3 bucket with versioning and MFA enabled</strong> is incorrect because an S3 bucket, whether it is public or private, is not suitable to store the SSL certificate.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A company has an Oracle Real Application Clusters (RAC) database on their on-premises data center which they want to migrate to AWS. The Chief Information Security Officer (CISO) instructed the solutions architects to automate the patch management process of the operating system in which the database runs, as well as to set up scheduled backups to comply with the company's disaster recovery plan.</p><p>Which of the following should the solutions architect implement to meet the company requirements with the least amount of effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Migrate the database to Amazon Aurora and enable automated backups for your Aurora RAC cluster. Patching is automatically handled in Aurora during the system maintenance window.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch a Lambda function that would automate the creation of snapshots of the database in the EC2 instance. Use the CodeDeploy and CodePipeline service to automate the patch management process of the database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the database to Amazon RDS which provides a multi-AZ failover feature for your RAC cluster. This will also reduce the RPO and RTO in the event of system failure since RDS offers features such as patch management and maintenance of the underlying host.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Migrate the database to a cluster of EBS-backed Amazon EC2 instances across multiple AZs. Automate the creation of EBS snapshots from EBS volumes of the EC2 instance by using Amazon Data Lifecycle Manager. Install the SSM Agent to the EC2 instance and automate the patch management process using AWS Systems Manager Patch Manager.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon RDS does not support certain features in Oracle such as Multitenant Database, Real Application Clusters (RAC), Unified Auditing, Database Vault, and many more.</p><p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), CentOS, Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\"></p><p>Hence, the option that says: <strong>Migrate the database to a cluster of EBS-backed Amazon EC2 instances across multiple AZs. Automate the creation of EBS snapshots from EBS volumes of the EC2 instance by using Amazon Data Lifecycle Manager. Install the SSM Agent to the EC2 instance and automate the patch management process using AWS Systems Manager Patch Manager </strong>is correct. Oracle RAC is supported via the deployment using Amazon EC2. AWS Systems Manager Patch Manager automates the process of patching managed instances with security-related updates.</p><p>The following options are both incorrect because an Amazon RDS database does not support Oracle RAC:</p><p><strong>1. Migrate the database to Amazon RDS which provides a multi-AZ failover feature for your RAC cluster. This will also reduce the RPO and RTO in the event of system failure since RDS offers features such as patch management and maintenance of the underlying host.</strong></p><p><strong>2. Migrate the database to Amazon Aurora and enable automated backups for your Aurora RAC cluster. Patching is automatically handled in Aurora during the system maintenance window</strong></p><p>The option that says: <strong>Launch a Lambda function that would automate the creation of snapshots of the database in the EC2 instance. Use the CodeDeploy and CodePipeline service to automate the patch management process of the database</strong> is incorrect because CodeDeploy and CodePipeline are CI/CD services and are not suitable for patch management. You should use AWS Systems Manager Patch Manager instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/rds/oracle/faqs/\">https://aws.amazon.com/rds/oracle/faqs/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A popular news website that uses an Oracle database is currently deployed in the company's on-premises network. Due to its growing number of readers, the company decided to move its infrastructure to AWS where they can further improve the performance of the website. The company earns from the advertisements placed on the website so you were instructed to ensure that the website remains available in case of database server failures. Their team of content writers constantly upload new articles every day including the wee hours of the morning to cover breaking news.</p><p>In this scenario, how can you implement a highly available architecture to meet the requirement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Create an Oracle database in RDS with Multi-AZ deployments."
			},
			{
				"correct": false,
				"answer": "Create an Oracle Real Application Clusters (RAC) in RDS which provides a shared cache architecture that overcomes the limitations of traditional shared-nothing and shared-disk approaches to provide highly scalable and available database solutions for the news website."
			},
			{
				"correct": false,
				"answer": "Create an Oracle database in RDS with Read Replicas."
			},
			{
				"correct": false,
				"answer": "<p>Create an Oracle database instance in RDS with Recovery Manager (RMAN) which performs backup and recovery tasks on your database and automates the administration of your backup strategies.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon RDS Multi-AZ</strong> deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.</p><p>In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operations without the need for manual administrative intervention.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_multiaz.png\"></p><p>Therefore, the correct answer is: <strong>Create an Oracle database in RDS with Multi-AZ deployments.</strong> This ensures high availability even if the primary database instance goes down.</p><p>The option that says: <strong>Create an Oracle database in RDS with Read Replicas </strong>is incorrect because the content writers won't be able to upload their articles to the Read Replicas in the event that the primary database goes down.</p><p>The following options are incorrect because Oracle RMAN and RAC are not supported in RDS:</p><p><strong>- Create an Oracle database instance in RDS with Recovery Manager (RMAN) which performs backup and recovery tasks on your database and automates the administration of your backup strategies</strong></p><p>- <strong>Create an Oracle Real Application Clusters (RAC) in RDS which provides a shared cache architecture that overcomes the limitations of traditional shared-nothing and shared-disk approaches to provide highly scalable and available database solutions for the news website</strong>.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/multi-az/\">https://aws.amazon.com/rds/details/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A multinational healthcare company plans to launch a new MedTech information website. The solutions architect decided to use Amazon CloudFormation to deploy a three-tier web application that consists of a web tier, an application tier, and a database tier that will utilize Amazon DynamoDB for storage. The solutions architect must secure any credentials that are used to access the database tier.</p><p>Which of the following options will allow the application instances access to the DynamoDB tables without exposing API credentials?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Have the user enter the access and secret keys of an existing IAM User that has permissions to read and write from the DynamoDB table instead of using the Parameter section in the CloudFormation template.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an IAM Role that grants access to the DynamoDB table. Use the <code>AWS::SSM::Parameter</code> resource that creates an SSM parameter in AWS Systems Manager Parameter Store containing the Amazon Resource Name of the IAM role. Have the instance profile property of the application instance reference the role.</p>"
			},
			{
				"correct": false,
				"answer": "Create an IAM User in the CloudFormation template and assign permissions to read and write from the DynamoDB table. Then retrieve the values of the access and secret keys using CloudFormation's GetAtt function, and pass them to the application instance through user-data. "
			},
			{
				"correct": true,
				"answer": "Create an IAM Role and assign the required permissions to read and write from the DynamoDB table. Have the instance profile property of the application instance reference the role."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could have your developers store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But developers would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work.</p><p>Instead, you can and should use an <strong>IAM role</strong> to manage <em>temporary</em> credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a user name and password or access keys) to an EC2 instance. Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests.</p><p>An <strong>IAM role</strong> is similar to a user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials (password or access keys) associated with it. Instead, if a user assumes a role, temporary security credentials are created dynamically and provided to the user. The scenario requires the instance to have access to DynamoDB tables without having to use the API credentials. In such scenarios, always think of creating IAM Roles rather than IAM Users.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role.png\"></p><p>Therefore, the correct answer is: <strong>Create an IAM Role and assign the required permissions to read and write from the DynamoDB table. Have the instance profile property of the application instance reference the role.</strong> It uses IAM Role with the appropriate permissions to access the resource, and it references that Role in the instance profile property of the application instance.</p><p>The option that says: <strong>Create an IAM User in the CloudFormation template and assign permissions to read and write from the DynamoDB table. Then retrieve the values of the access and secret keys using CloudFormation's GetAtt function, and pass them to the application instance through user-data</strong> is incorrect because you should never expose the Access and Secret Keys while accessing AWS resources, and using IAM Role is a more secure way of accessing the resources than using IAM Users with security credentials.</p><p>The option that says: <strong>Create an IAM Role that grants access to the DynamoDB table. Use the </strong><code><strong>AWS::SSM::Parameter</strong></code><strong> resource that creates an SSM parameter in AWS Systems Manager Parameter Store containing the Amazon Resource Name of the IAM role. Have the instance profile property of the application instance reference the role</strong> is incorrect because storing the ARN of the IAM Role in the AWS Systems Manager Parameter Store is not the proper way to attach the role to the application instance. You have to use the instance profile property (<code>AWS::IAM::InstanceProfile</code>) instead.</p><p>The option that says: <strong>Have the user enter the access and secret keys of an existing IAM User that has permissions to read and write from the DynamoDB table instead of using the Parameter section in the CloudFormation template</strong> is incorrect because you should never expose the Access and Secret Keys while accessing the AWS resources, and using IAM Role is a more secure way of accessing the resources than using IAM Users with security credentials.<br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-role.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A leading media company is building a collaborative news website that is expected to have over 5 million readers per month globally. Each article contains a cover image and has at least 200 words. Based on the trend of their other websites, the new articles are highly browsed in the first 2 months and the authors tend to frequently update the articles on the first month after its publication. The readership is also expected to drop on the 3rd month and the articles are usually rarely accessed after a year. The readers are also leaving a lot of comments within the first 3 months of publishing.</p><p>In this scenario, which of the following items can you use to build a durable, highly available, and scalable architecture for the news website? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use Lambda with Auto-Healing enabled.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use Amazon RDS Multi-AZ deployments with Read Replicas. Use S3 to store the static data such as the cover images and other media.</p>"
			},
			{
				"correct": false,
				"answer": "Launch an RDS Oracle Real Application Clusters (RAC) with Read Replicas."
			},
			{
				"correct": true,
				"answer": "Use CloudFront as a Content Delivery Network to load the articles much faster anywhere in the globe."
			},
			{
				"correct": false,
				"answer": "Use EBS Volumes in RAID 0 configuration to store the static data such as the cover images and other media."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the main objective is to provide a durable, highly-available and scalable architecture for the website. You can use CloudFront as a CDN, then Amazon RDS with Multi-AZ deployments and Read Replicas to provide scalability and high-availability for the millions of incoming traffic every month. Lastly, you can use an S3 bucket to durably store the images and other static media content of the website.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\"></p><p>Therefore, the correct answer is: <strong>Use CloudFront as a Content Delivery Network to load the articles much faster anywhere in the globe </strong>and <strong>Use Amazon RDS Multi-AZ deployments with Read Replicas. Use S3 to store the static data such as the cover images and other media</strong>.</p><p>The option that says: <strong>Launch an RDS Oracle Real Application Clusters (RAC) with Read Replicas</strong> is incorrect because Oracle RAC is not supported in RDS.</p><p>The option that says: <strong>Use Lambda with Auto-Healing enabled </strong>is incorrect. AWS Lambda is serverless and it does not have Auto-Healing option since functions are event-driven.</p><p>The option that says: <strong>Use EBS Volumes in RAID 0 configuration to store the static data such as the cover images and other media </strong>is incorrect because EBS Volumes are not as durable and not as scalable compared with S3, even with a RAID 1 (mirroring) configuration.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><a href=\"https://aws.amazon.com/s3/\">https://aws.amazon.com/s3/</a></p><p><br></p><p><strong>Check out these Amazon RDS and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>An electronics and communications company in Japan has several VPCs in the AWS cloud. It uses NAT instances to allow multiple EC2 instances from the private subnet to initiate connections to the Internet while also restricting any requests coming from the outside network. However, there are numerous incidents where the NAT instance is not available, which affects the batch processing of critical applications.</p><p>Which is the most suitable solution that provides better availability and bandwidth to the current infrastructure with minimal administrative effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Launch a larger NAT instance with the enhanced networking feature enabled to improve the availability and performance of the NAT device.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch two large NAT instances in two separate public subnets and add a route from the private subnet to each NAT instance to make it more fault-tolerant and highly available.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a NAT gateway then specify its corresponding subnet and Elastic IP address. Update the route tables of the private subnet to point the Internet traffic to the NAT gateway.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an egress-only Internet gateway. Update the route tables of the private subnet to point the Internet traffic to the egress-only Internet gateway.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use a <strong>NAT</strong> device to enable instances in a private subnet to connect to the internet (for example, for software updates) or other AWS services, but prevent the internet from initiating connections with the instances. A NAT device forwards traffic from the instances in the private subnet to the internet or other AWS services, and then sends the response back to the instances. When traffic goes to the internet, the source IPv4 address is replaced with the NAT device’s address and similarly, when the response traffic goes to those instances, the NAT device translates the address back to those instances’ private IPv4 addresses.</p><p><img src=\"https://media.tutorialsdojo.com/sap_nat_gateway.png\"></p><p>AWS offers two kinds of NAT devices—a <strong><em>NAT gateway</em></strong> or a <strong><em>NAT instance</em></strong>. It is recommended to use NAT gateways, as they provide better availability and bandwidth over NAT instances. The NAT Gateway service is also a managed service that does not require your administration efforts.</p><p>A NAT instance is launched from a NAT AMI and you can choose to use a NAT instance for special purposes. However, this type of NAT device is limited and is not highly available compared with a NAT Gateway.</p><p>Therefore, the correct answer is: <strong>Create a NAT gateway then specify its corresponding subnet and Elastic IP address. Update the route tables of the private subnet to point the Internet traffic to the NAT gateway.</strong></p><p>The option that says: <strong>Launch a larger NAT instance with the enhanced networking feature enabled to improve the availability and performance of the NAT device</strong> is incorrect. Even if you upgrade your NAT device to a larger instance, it would still be a single component. This means that if your NAT instance goes down, there would be no other instance to handle the requests, which means that your architecture is not highly available. It is better to use a NAT Gateway to provide better availability and bandwidth for your infrastructure.</p><p>The option that says: <strong>Launch two large NAT instances in two separate public subnets and add a route from the private subnet to each NAT instance to make it more fault-tolerant and highly available</strong> is incorrect. Although this solution is indeed highly available and fault-tolerant, this entails a lot of administrative effort to manage those two NAT instances. Hence, it is still better to use the NAT Gateway service since it is a managed service that does not require administrative effort.</p><p>The option that says: <strong>Create an egress-only Internet gateway. Update the route tables of the private subnet to point the Internet traffic to the egress-only Internet gateway </strong>is incorrect because an egress-only Internet gateway is primarily used to handle IPv6 traffic, which is not mentioned in this scenario.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company hosts its main web application on the AWS cloud which is composed of web servers and database servers. To ensure high availability, the web servers are deployed on an Auto Scaling group of Amazon EC2 instances across multiple Availability Zones with an Application Load Balancer in front. For the database, it is deployed on a Multi-Availability Zone configuration in Amazon RDS. During the RDS maintenance window, the operating system of the primary DB instance undergoes software patching that triggers the failover process.</p><p>What would happen to the database during failover?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "The canonical name record (CNAME) is changed from the primary database to standby database."
			},
			{
				"correct": false,
				"answer": "A new DB instance will be created and immediately replace the primary database."
			},
			{
				"correct": false,
				"answer": "The IP address of the primary DB instance is switched to the standby DB instance."
			},
			{
				"correct": false,
				"answer": "The RDS DB instance will automatically reboot."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon RDS Multi-AZ</strong> deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora), so that you can resume database operations as soon as the failover is complete.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_multiaz.png\"></p><p>When automatic failover occurs, your application can remain unaware of what's happening behind the scenes. The CNAME record for your DB instance will be altered to point to the newly promoted standby.</p><p>Therefore, the correct answer is: <strong>The canonical name record (CNAME) is changed from the primary database to standby database.</strong></p><p>The option that says: <strong>The IP address of the primary DB instance is switched to the standby DB instance</strong> is incorrect because the Canonical Name Record (CNAME) will be changed and not the IP address.</p><p>The option that says: <strong>The RDS DB instance will automatically reboot</strong> is incorrect because the RDS instance will not automatically reboot.</p><p>The option that says:<strong> A new DB instance will be created and immediately replace the primary database</strong> is incorrect because there is no new DB instance that will be created.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/multi-az/\">https://aws.amazon.com/rds/details/multi-az/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/amazon-rds-multi-az-deployment/\">https://aws.amazon.com/blogs/aws/amazon-rds-multi-az-deployment/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company has a hybrid cloud architecture where their on-premises data center and VPC are connected via multiple AWS Direct Connect ports in a single Link Aggregation Group (LAG). They have an on-premises patch management system that automatically applies the patches to the operating systems of their servers and file systems. You were given a task to synchronize the patch baselines being used on-premises to all of the EC2 instances in your VPC, as well as to automate the patching schedule. </p><p>Which of the following methods should you implement to meet the above requirement with the LEAST amount of effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use the AWS Systems Manager State Manager to automate the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define, which includes the OS patches that should be applied in each EC2 instance. Automate the patching schedule by using AWS Systems Manager Distributor, to package and distribute the required patches to your instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Systems Manager Session Manager to manage and deploy the security patches of your EC2 instances based on the patch baselines from your on-premises data center. Automate the patching schedule by using the AWS Systems Manager Maintenance Windows.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Systems Manager Patch Manager to manage and deploy the security patches of your EC2 instances based on the patch baselines from your on-premises data center. Automate the patching schedule by setting up scheduled jobs using AWS Lambda and AWS Systems Manager Run Command.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Systems Manager Patch Manager to manage and deploy the security patches of your EC2 instances based on the patch baselines from your on-premises data center. Install the SSM Agent to all of your instances and automate the patching schedule by using AWS Systems Manager Maintenance Windows.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Patch Manager </strong>automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_patch_manager.jpg\"></p><p><strong>AWS Systems Manager Maintenance Windows</strong> let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Each Maintenance Window has a schedule, a maximum duration, a set of registered targets (the instances that are acted upon), and a set of registered tasks. You can also specify dates that a Maintenance Window should not run before or after, and you can specify the international time zone on which to base the Maintenance Window schedule.</p><p>Therefore, the correct answer is: <strong>Use AWS Systems Manager Patch Manager to manage and deploy the security patches of your EC2 instances based on the patch baselines from your on-premises data center. Install the SSM Agent to all of your instances and automate the patching schedule by using AWS Systems Manager Maintenance Windows.</strong></p><p>The option that says: <strong>Use AWS Systems Manager Session Manager to manage and deploy the security patches of your EC2 instances based on the patch baselines from your on-premises data center. Automate the patching schedule by using the AWS Systems Manager Maintenance Windows</strong> is incorrect because the Session Manager is primarily used to comply with corporate policies that require controlled access to instances, strict security practices, and fully auditable logs with instance access details, but not for applying OS patches. Using the AWS Systems Manager Patch Manager is a more appropriate solution to implement.</p><p>The option that says: <strong>Use AWS Systems Manager Patch Manager to manage and deploy the security patches of your EC2 instances based on the patch baselines from your on-premises data center. Automate the patching schedule by setting up scheduled jobs using AWS Lambda and AWS Systems Manager Run Command</strong> is incorrect. Although it properly uses AWS Systems Manager Patch Manager, it is still better to use AWS Systems Manager Maintenance Windows instead of manually creating scheduled jobs using AWS Lambda and AWS Systems Manager Run Command. Take note that the scenario specifies that you have to meet the requirement with the LEAST amount of effort, which can be met by using the AWS Systems Manager Maintenance Windows feature. In addition, installing the SSM Agent to all of your instances is also required when using the AWS Systems Manager Patch Manager, which is not mentioned in this option.</p><p>The option that says: <strong>Use the AWS Systems Manager State Manager to automate the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define, which includes the OS patches that should be applied in each EC2 instance. Automate the patching schedule by using AWS Systems Manager Distributor, to package and distribute the required patches to your instances</strong> is incorrect because the AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This does not handle patch management, unlike AWS Systems Manager Patch Manager. With the State Manager, you can configure your instances to boot with a specific software at start-up; download and update agents on a defined schedule; configure network settings and many others, but not the patching of your EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A media company in South Korea offers high-quality wildlife photos to its clients. Its photographers upload a large number of photographs to the company’s Amazon S3 bucket. Currently, the company is using a dedicated group of on-premises servers to process the photos and uses an open-source messaging system to deliver job information to the servers. After processing, the data would go to a tape library and be stored for long-term archival. The company decided to shift everything to AWS Cloud, and the solutions architect was tasked to implement the same existing infrastructure design and leverage AWS tools such as storage and messaging services to minimize cost.</p><p>Which of the following options is the recommended solution that will meet the requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "SQS will handle the job messages, while CloudWatch alarms will terminate any idle EC2 worker instances. After the data has been processed, change the storage class of your S3 objects to S3 IA-Standard."
			},
			{
				"correct": false,
				"answer": "SNS will handle the passing of job messages, while CloudWatch alarms will terminate any idle spot worker instances. After the data has been processed, transfer your S3 objects to Amazon Glacier."
			},
			{
				"correct": true,
				"answer": "Create an Auto-scaling group of spot instance workers that scale according to the queue depth in SQS to process job messages. After the data has been processed, transfer your S3 objects to Amazon Glacier."
			},
			{
				"correct": false,
				"answer": "<p>Initially change the storage class of the S3 objects to S3 IA-Standard. Then create an Auto-scaling group of spot instance workers that scale according to the queue depth in SQS to process job messages. After the data has been processed, transfer your S3 objects to Amazon S3-IA.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>There are some scenarios where you might think about scaling in response to activity in an <strong>Amazon SQS queue</strong>. For example, suppose that you have a web app that lets users upload images and use them online. In this scenario, each image requires resizing and encoding before it can be published. The app runs on EC2 instances in an Auto Scaling group, and it's configured to handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance levels at all times. The app places the raw bitmap data of the images in an SQS queue for processing. It processes the images and then publishes the processed images where they can be viewed by users. The architecture for this scenario works well if the number of image uploads doesn't vary over time. But if the number of uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling group.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_cloudwatch_metric.png\"></p><p>In this scenario, the best option is the combination of SQS and Glacier as a storage option.</p><p>Therefore, the correct answer is: <strong>Create an Auto-scaling group of spot instance workers that scale according to the queue depth in SQS to process job messages. After the data has been processed, transfer your S3 objects to Amazon Glacier. </strong>It uses SQS to process the messages, and it uses Glacier as the archival storage solution which is the cheapest storage option.</p><p>The option that says: <strong>SQS will handle the job messages, while CloudWatch alarms will terminate any idle EC2 worker instances. After the data has been processed, change the storage class of your S3 objects to S3 IA-Standard</strong> is incorrect. S3 IA is not an archival storage option, and since auto scaling is not mentioned, you cannot use CloudWatch alarms to terminate the idle EC2 instances.</p><p>The option that says: <strong>Initially change the storage class of the S3 objects to S3 IA-Standard. Then create an Auto-scaling group of spot instance workers that scale according to the queue depth in SQS to process job messages. After the data has been processed, transfer your S3 objects to Amazon S3-IA</strong> is incorrect. S3 IA is not an archival storage option.</p><p>The option that says: <strong>SNS will handle the passing of job messages, while CloudWatch alarms will terminate any idle spot worker instances. After the data has been processed, transfer your S3 objects to Amazon Glacier</strong> is incorrect. Amazon Simple Notification Service cannot be used to process the messages.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><a href=\"https://aws.amazon.com/glacier\">https://aws.amazon.com/glacier</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><br></p><p><strong>Check out these Amazon SQS and Amazon S3 Glacier Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-glacier/?src=udemy\">https://tutorialsdojo.com/amazon-glacier/</a></p></div>"
	},
	{
		"question": "<p>A company is migrating a legacy Oracle database from their on-premises data center to AWS. It will be deployed in an existing EBS-backed EC2 instance with multiple EBS volumes attached. For the migration, a new volume must be created for the Oracle database and then attached to the instance. This will be used by a financial web application and will primarily store historical financial data that are infrequently accessed.</p><p>Which of the following is the MOST cost-effective and throughput-oriented solution that the solutions architect should implement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Migrate the database using the AWS Database Migration Service and use a Provisioned IOPS (io1) EBS volume.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Migrate the database using the AWS Database Migration Service and use a Cold HDD (sc1) EBS volume.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the database using the AWS Server Migration Service and use a General Purpose (gp2) EBS Volume.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the database using the AWS Server Migration Service and use a Throughput Optimized (st1) EBS volume.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Database Migration Service</strong> helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.</p><p>The <strong>AWS Database Migration Service</strong> can migrate your data to and from the most widely used commercial and open-source databases. AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle or Microsoft SQL Server to Amazon Aurora. With AWS Database Migration Service, you can continuously replicate your data with high availability and consolidate databases into a petabyte-scale data warehouse by streaming data to Amazon Redshift and Amazon S3.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dms_prep.png\"></p><p>Cold HDD volumes provide low-cost magnetic storage that defines performance in terms of throughput rather than IOPS. With a lower throughput limit than Throughput Optimized HDD, this is a good fit ideal for large, sequential cold-data workloads. If you require infrequent access to your data and are looking to save costs, Cold HDD provides inexpensive block storage. Take note that bootable Cold HDD volumes are not supported.</p><p>Cold HDD provides the lowest cost HDD volume and is designed for less frequently accessed workloads. Therefore, the correct answer is:<strong><em> </em>Migrate the database using the AWS Database Migration Service and use a Cold HDD (sc1) EBS volume.</strong></p><p>The option that says: <strong>Migrate the database using the AWS Database Migration Service and use a Provisioned IOPS (io1) EBS volume</strong> is incorrect because it costs more than the Cold HDD and thus, not cost-effective for this scenario. It provides the highest performance SSD volume for mission-critical low-latency or high-throughput workloads, which is not needed in the scenario.</p><p>The option that says: <strong>Migrate the database using the AWS Server Migration Service and use a Throughput Optimized (st1) EBS volume<em> </em></strong>is incorrect because although it is cheaper than SSD, it is primarily designed and used for frequently accessed throughput-intensive workloads. Cold HDD perfectly fits the description as it is used for their infrequently accessed data and provides the lowest cost, unlike Throughput Optimized HDD. In addition, the AWS Server Migration Service (SMS) is primarily used to migrate on-premises virtual machines from VMware vSphere, Windows Hyper-V, or Microsoft Azure only. You have to use the AWS Database Migration Service instead in this situation.</p><p>The option that says: <strong>Migrate the database using the AWS Server Migration Service and use a General Purpose (gp2) EBS Volume<em> </em></strong>is incorrect because a General purpose SSD volume costs more and it is mainly used for a wide variety of workloads. It is recommended to be used as system boot volumes, virtual desktops, low-latency interactive apps, and many more. Moreover, you have to use the AWS Database Migration Service instead in this scenario and not AWS Server Migration Service (SMS).</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ebs/details/\">https://aws.amazon.com/ebs/details/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a></p><p><a href=\"https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html\">https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.html</a></p><p><br></p><p><strong>Check out this Amazon EBS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p></div>"
	},
	{
		"question": "<p>An electric utility company deploys smart meters for its customers to easily track their electricity usage. Each smart meter sends data every five minutes to an Amazon API Gateway which is then processed by several AWS Lambda functions before storing to an Amazon DynamoDB table. The Lambda functions take about 5 to 10 seconds to process the data based on the initial deployment testing. As the company’s customer base grew, the solutions architect noticed that the Lambda functions are now taking 60 to 90 seconds to complete the processing. New metrics are also collected from the smart meters which further increased the processing time. Errors began showing when running the Lambda function such as <code>TooManyRequestsException</code> and <code>ProvisionedThroughputExceededException</code> error when performing PUT operation on the DynamoDB table.</p><p>Which combination of the following actions will resolve these issues? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Since the Lambda functions are being overwhelmed with too many requests, increase the payload size from the meters but send the data less frequently to avoid reaching the concurrency limit.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The new metrics being collected requires more processing power from the Lambda functions. Adjust the memory allocation for the Lambda function to accommodate the surge.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Process the data in batches to avoid reaching the write limits to the DynamoDB table. Group the requests from API Gateway by streaming the data into an Amazon Kinesis data stream.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up an Amazon SQS FIFO queue to handle the burst of the data stream from the smart metrics. Trigger the Lambda function to run whenever a message is received on the queue.</p>"
			},
			{
				"correct": true,
				"answer": "<p>As more customers are sending data, adjust the Write Capacity Unit (WCU) of the DynamoDB table to be able to accommodate all the write requests being processed by the Lambda functions.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In <strong>Amazon DynamoDB</strong>, the ProvisionedThroughputExceededException error means that you exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes. This means that your request rate is too high. The AWS SDKs for DynamoDB automatically retries requests that receive this exception. Your request is eventually successful unless your retry queue is too large to finish.</p><p>To solve this, you can increase the write capacity unit (WCU) of your DynamoDB table. Every PutItem request consumes a write capacity unit. A write capacity unit represents one write per second, for an item up to 1 KB in size. For example, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in size per second.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamoDB_auto_scaling.png\"></p><p>In <strong>AWS Lambda,</strong> the first time you invoke your function, AWS Lambda creates an instance of the function and runs its handler method to process the event. When the function returns a response, it stays active and waits to process additional events. If you invoke the function again while the first event is being processed, Lambda initializes another instance, and the function processes the two events concurrently. Your functions' concurrency is the number of instances that serve requests at a given time. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region.</p><p>When seeing the TooManyRequestsException in AWS Lambda, it is possible that the throttles that you're seeing aren't on your Lambda function. Throttles can also occur on API calls during your function's invocation or on concurrency limits.</p><p>With <strong>API Gateway</strong>, you can send the stream to an Amazon Kinesis data stream on which you can group requests in batches so there will be a decrease in requests in Lambda.</p><p>Therefore, the correct answers are:</p><p><strong>- As more customers are sending data, adjust the Write Capacity Unit (WCU) of the DynamoDB table to be able to accommodate all the write requests being processed by the Lambda functions.</strong></p><p><strong>- Process the data in batches to avoid reaching the write limits to the DynamoDB table. Group the requests from API Gateway by streaming the data into an Amazon Kinesis data stream.</strong></p><p>The option that says: <strong>The new metrics being collected requires more processing power from the Lambda functions. Adjust the memory allocation for the Lambda function to accommodate the surge</strong> is incorrect. Although this can improve the processing power of the Lambda functions, this will not solve the TooManyRequestsException error which is due to reaching the AWS Lambda concurrency execution limits.</p><p>The option that says: <strong>Since the Lambda functions are being overwhelmed with too many requests, increase the payload size from the meters but send the data less frequently to avoid reaching the concurrency limit</strong> is incorrect. Although this will solve the TooManyRequestsException for the Lambda function, you may reach the 10MB payload limit on the API gateway if you aggregate too much data before sending it to API Gateway.</p><p>The option that says: <strong>Set up an Amazon SQS FIFO queue to handle the burst of the data stream from the smart metrics. Trigger the Lambda function to run whenever a message is received on the queue</strong> is incorrect. This action is not recommended because an SQS FIFO queue can only handle 3000 messages per second. The customer base is constantly growing so it is recommended to use Amazon Kinesis to scale beyond this.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/\">https://aws.amazon.com/premiumsupport/knowledge-center/lambda-troubleshoot-throttling/</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html#concurrent-execution-safety-limit\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html#concurrent-execution-safety-limit</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/integrating-api-with-aws-services-kinesis.html</a></p><p><br></p><p><strong>Check out these AWS Lambda and Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A company uses a CloudFormation script to deploy an online voting application. The app is used for a Nature Photography Contest that accepts high-resolution images, stores them in an S3 bucket, and records a 100-character summary about the image in RDS. The Solutions Architect must ensure that the same online voting application can be deployed once again using the same CloudFormation template for succeeding contests in the future. The photography contest will run for just a month and once it has been concluded, there would be nobody using the online voting application anymore until the next contest. As preparation for the upcoming events next year, the 100-character summaries should be kept and the S3 bucket, which contains the high-resolution photos, should remain.</p><p>Which of the following options is the recommended action to meet the above requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>For both the RDS and S3 resource types on the CloudFormation template, set the DeletionPolicy to <code>Retain</code>.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Set the DeletionPolicy on the S3 resource declaration in the CloudFormation template to <code>Retain</code>.\n2. Set the RDS resource declaration DeletionPolicy to <code>Snapshot</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Enable Cross-Region Replication (CRR) in the S3 bucket to maintain a copy of all the S3 objects.\n2. Set the DeletionPolicy for the RDS instance to <code>Snapshot</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Set the DeletionPolicy on the S3 resource to <code>Snapshot</code>.&nbsp; \n2. Set the DeletionPolicy on the RDS resource to <code>Snapshot</code>.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With the <strong>DeletionPolicy attribute</strong>, you can preserve or (in some cases) back up a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_deletionpolicy-2.JPG\"></p><p>Note that this capability also applies to stack update operations that lead to resources being deleted from stacks, for example, if you remove the resource from the stack template and then update the stack with the template. This capability does not apply to resources whose physical instance is replaced during stack update operations. For example, if you edit a resource's properties such that AWS CloudFormation replaces that resource during a stack update.</p><p>In this scenario, you need to keep the data on your S3 bucket and RDS which can be achieved by <strong>setting the DeletionPolicy of S3 to </strong><code><strong>Retain</strong></code><strong> and for RDS to use </strong><code><strong>Snapshot</strong></code>.</p><p><strong>Setting the DeletionPolicy on the S3 resource to </strong><code><strong>Snapshot</strong></code><strong> and for RDS resource to use </strong><code><strong>Snapshot</strong></code> is incorrect because S3 does not support snapshots. It should be set to <code>Retain</code>.</p><p><strong>For both the RDS and S3 resource types on the CloudFormation template, setting the DeletionPolicy to </strong><code><strong>Retain</strong></code> is incorrect. Although you can set <code>Retain</code> on both the DeletionPolicy of the S3 bucket and the RDS database, this entails an unnecessary operating cost since the RDS database will still be running even if it is not used. Take note that the application is meant to run for just a month and once it has been concluded, nobody would be using it until the next contest next year. The RDS should be set to <code>Snapshot</code>.</p><p><strong>Enabling Cross-Region Replication (CRR) in the S3 bucket to maintain a copy of all the S3 objects and setting the DeletionPolicy for the RDS instance to </strong><code><strong>Snapshot</strong></code><strong> </strong>is incorrect because even though your data will still be available in the other region because of the CRR, the current S3 bucket will still be deleted.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/\">https://aws.amazon.com/premiumsupport/knowledge-center/delete-cf-stack-retain-resources/</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p></div>"
	},
	{
		"question": "<p>A company has an on-premises identity provider (IdP) used for authenticating employees. The Solutions Architect has created a SAML 2.0 based federated identity solution that integrates with the company IdP. This solution is used to authenticate users’ access to the AWS environment. Upon initial testing, the Solutions Architect has been successfully granted access to the AWS environment through the federated identity web portal. However, other test users who tried to authenticate through the federated identity web portal are not given access to the AWS environment.</p><p>Which of the following options must be checked to ensure the proper configuration of identity federation? (Select THREE.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Ensure that the ARN of the SAML provider, the ARN of the created IAM role, and SAML assertion from the IdP are all included when the federated identity web portal calls the AWS STS <code>AssumeRoleWithSAML</code> API.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Ensure that the appropriate IAM roles are mapped to company users and groups in the IdP’s SAML assertions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ensure that the IAM policy for that user has “Allow” permissions to use SAML federation.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Ensure that the trust policy of the IAM roles created for the federated users or groups has set the SAML provider as principal.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Check the company’s IdP to ensure that the users are all part of the default <code>AWSFederatedUser</code> IAM group which is readily available in AWS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ensure that the resources on the AWS environment VPC can reach the on-premises IdP using its DNS hostname.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS supports identity federation with SAML 2.0 (Security Assertion Markup Language 2.0), an open standard that many identity providers (IdPs) use. This feature enables federated single sign-on (SSO), so users can log into the AWS Management Console or call the AWS API operations without having to create an IAM user for everyone in your organization. By using SAML, you can simplify the process of configuring federation with AWS, because you can use the IdP's service instead of writing custom identity proxy code.</p><p>IAM federation supports these use cases:</p><p>- Federated access to allow a user or application in your organization to call AWS API operations. You use a SAML assertion (as part of the authentication response) that is generated in your organization to get temporary security credentials.</p><p>- Web-based single sign-on (SSO) to the AWS Management Console from your organization. Users can sign in to a portal in your organization hosted by a SAML 2.0–compatible IdP, select an option to go to AWS, and be redirected to the console without having to provide additional sign-in information. You can use a third-party SAML IdP to establish SSO access to the console or you can create a custom IdP to enable console access for your external users.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap.png\"></p><p>The diagram illustrates the following steps:</p><p>The user browses to your organization's portal and selects the option to go to the AWS Management Console. In your organization, the portal is typically a function of your IdP that handles the exchange of trust between your organization and AWS.</p><p>The portal verifies the user's identity in your organization.</p><p>The portal generates a SAML authentication response that includes assertions that identify the user and include attributes about the user. The portal sends this response to the client browser.</p><p>The client browser is redirected to the AWS single sign-on endpoint and posts the SAML assertion.</p><p>The endpoint requests temporary security credentials on behalf of the user and creates a console sign-in URL that uses those credentials.</p><p>AWS sends the sign-in URL back to the client as a redirect.</p><p>The client browser is redirected to the AWS Management Console. If the SAML authentication response includes attributes that map to multiple IAM roles, the user is first prompted to select the role for accessing the console.</p><p>Before you can use SAML 2.0-based federation, you must configure your organization's IdP and your AWS account to trust each other. Inside your organization, you must have an IdP that supports SAML 2.0, like Microsoft Active Directory Federation Service (AD FS, part of Windows Server), Shibboleth, or another compatible SAML 2.0 provider. In your organization's IdP, you define assertions that map users or groups in your organization to the IAM roles. Note that different users and groups in your organization might map to different IAM roles. The exact steps for performing the mapping depend on what IdP you're using.</p><p>The role or roles that you create in IAM define what federated users from your organization are allowed to do in AWS. When you create the trust policy for the role, you specify the SAML provider that you created earlier as the <code>Principal</code>. You can additionally scope the trust policy with a <code>Condition</code> element to allow only users that match certain SAML attributes to access the role.</p><p>The option that says: <strong>Ensure that the trust policy of the IAM roles created for the federated users or groups has set the SAML provider as principal</strong> is correct. In IAM, you create one or more IAM roles for the federated users. In the role's trust policy, you need to set the SAML provider as the principal, which establishes a trust relationship between your organization and AWS.</p><p>The option that says: <strong>Ensure that the ARN of the SAML provider, the ARN of the created IAM role, and SAMS assertion from the IdP are all included when the federated identity web portal calls the AWS STS </strong><code><strong>AssumeRoleWithSAML</strong></code><strong> API</strong> is correct. These items should all be passed by the client calling the AWS STS <code>AssumeRoleWithSAML</code> API.</p><p>The option that says: <strong>Ensure that the appropriate IAM roles are mapped to company users and groups in the IdP’s SAML assertions</strong> is correct. In your organization's IdP, you should define assertions that map users or groups in your organization to the IAM roles.</p><p>The option that says: <strong>Ensure that the IAM policy for that user has “Allow” permissions to use SAML federation </strong>is incorrect because test user's permissions are mapped to IAM roles, so we need to take a look at the IAM role policies and not the individual user IAM permission policies.</p><p>The option that says: <strong>Check the company’s IdP to ensure that the users are all part of the default </strong><code><strong>AWSFederatedUser</strong></code><strong> IAM group which is readily available in AWS</strong> is incorrect because there is no such thing as a default <code>AWSFederatedUser</code> IAM group. You only need to define assertions in your organization's IdP that map users or groups in your organization to the IAM roles.</p><p>The option that says: <strong>Ensure that the resources on the AWS environment VPC can reach the on-premises IdP using its DNS hostname</strong> is incorrect as this is not a requirement for the identity federation to work.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-saml.html</a></p><p><br></p><p><strong>Check out these AWS Security and Identity Services Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheets-security-identity-services/?src=udemy\">https://tutorialsdojo.com/aws-cheat-sheets-security-identity-services/</a></p></div>"
	},
	{
		"question": "<p>A retail company is planning to deploy its business analytics application on AWS to gather insights on customer behaviors and preferences. With a large amount of data that needs to be processed, the application requires 10,000 hours of computing time every month. Since this is for analytics, the company is flexible when it comes to the availability of compute resources and wants the solution to be as cost-effective as possible. Supplementing the analytics application, a reporting service needs to run continuously to distribute analytical reports.</p><p>Which of the following solutions will satisfy the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Deploy the analytics service on a combination of Amazon EC2 On-Demand instances and Reserved Instances with a 3-year term. Configure a custom metric in Auto Scaling to scale the fleet to meet the needed capacity. Create a container for the reporting service and run it on Amazon ECS with AWS Fargate.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Deploy the analytics service on a fleet of Amazon EC2 Spot instances in an Auto Scaling group. Configure a custom metric to scale the Spot fleet to meet the needed capacity. Create a container for the reporting service and run it on Amazon ECS with AWS Fargate.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a container for the analytics application and run it on Amazon ECS with AWS Fargate. Configure a custom metric with Service Auto Scaling to scale the analytics application and meet the needed capacity. Deploy the reporting service on a fleet of Amazon EC2 Spot instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a container for the analytics application and run it on Amazon Batch with AWS Fargate. Configure a custom metric with Service Auto Scaling to scale the analytics application to meet the needed capacity. Deploy the reporting service on an Amazon EC2 On-Demand instance.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon EC2 Spot Instances</strong> let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices. You can use Spot Instances for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, high-performance computing (HPC), and test &amp; development workloads. Because Spot Instances are tightly integrated with AWS services such as Auto Scaling, EMR, ECS, CloudFormation, Data Pipeline, and AWS Batch, you can choose how to launch and maintain your applications running on Spot Instances.</p><p>A Spot Instance is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. For example, Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks.</p><p><img src=\"https://media.tutorialsdojo.com/sap_spot_instances_overview.png\"></p><p>AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate makes it easy for you to focus on building your applications.</p><p><strong>Amazon Elastic Container Service (Amazon ECS)</strong> is a shared state, optimistic concurrency system that provides flexible scheduling capabilities for your tasks and containers. Each task that uses the <strong>Fargate launch type</strong> has its own isolation boundary and does not share the underlying kernel, CPU resources, memory resources, or elastic network interface with another task.</p><p>Amazon ECS provides a service scheduler (for long-running tasks and applications) and the ability to run tasks manually (for batch jobs or single run tasks), with Amazon ECS placing tasks on your cluster for you. You can specify task placement strategies and constraints that allow you to run tasks in the configuration you choose, such as spread out across Availability Zones.</p><p>Therefore, the correct answer is: <strong>Deploy the analytics service on a fleet of Amazon EC2 Spot instances in an Auto Scaling group. Configure a custom metric to scale the Spot fleet to meet the needed capacity. Create a container for the reporting service and run it on Amazon ECS with AWS Fargate. </strong>With a flexible workload, you can save a lot of costs by using Spot Instances. AWS Fargate is suitable for running a container continuously. AWS Fargate will automatically replace the container if it becomes unhealthy.</p><p>The option that says: <strong>Create a container for the analytics application and run it on Amazon ECS with AWS Fargate. Configure a custom metric with Service Auto Scaling to scale the analytics application and meet the needed capacity. Deploy the reporting service on a fleet of Amazon EC2 Spot instances</strong> is incorrect. The reporting service needs to run continuously so it is not recommended to run it on EC2 Spot instances which can be reclaimed by AWS anytime. Running thousands of hours of workload on an AWS Fargate cluster is more expensive compared to using Spot instances.</p><p>The option that says: <strong>Create a container for the analytics application and run it on Amazon Batch with AWS Fargate. Configure a custom metric with Service Auto Scaling to scale the analytics application to meet the needed capacity. Deploy the reporting service on an Amazon EC2 On-Demand instance</strong> is incorrect. The reporting service needs to run continuously so it is not cost-effective to run it on an On-Demand instance. AWS Batch is used in running batch workloads, and not analytical workloads that will run for thousands of hours.</p><p>The option that says: <strong>Deploy the analytics service on a combination of Amazon EC2 On-Demand instances and Reserved Instances with a 3-year term. Configure a custom metric in Auto Scaling to scale the fleet to meet the needed capacity. Create a container for the reporting service and run it on Amazon ECS with AWS Fargate</strong> is incorrect. Running the reporting service on AWS Fargate is a good choice. However, the On-Demand instances and Reserved instances are not cost-effective in running the analytics application. Since the workload can be flexible and only run once a month, Spot instances are recommended for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/details/\">https://aws.amazon.com/ec2/spot/details/</a></p><p><a href=\"https://aws.amazon.com/fargate/faqs/?nc=sn&amp;loc=4\">https://aws.amazon.com/fargate/faqs/?nc=sn&amp;loc=4</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/scheduling_tasks.html</a></p><p><br></p><p><strong>Check out these AWS Fargate and Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A leading commercial bank has multiple AWS accounts that are consolidated using AWS Organizations. They are building an online portal for foreclosed real estate properties that they own. The online portal is designed to use SSL for better security. The bank would like to implement a separation of responsibilities between the DevOps team and their cybersecurity team. The DevOps team is entitled to manage and log in to the EC2 instances while the cybersecurity team has exclusive access to the application's X.509 certificate, which contains the private key and is stored in AWS Certificate Manager (ACM).</p><p>Which of the following options would satisfy the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up a Service Control Policy (SCP) that authorizes access to the certificate store only for the cybersecurity team and then add a configuration to terminate the SSL on the ELB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Config service to configure the EC2 instances to retrieve the X.509 certificate upon boot from a CloudHSM that is managed by the cybersecurity team.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Upload the X.509 certificate to an S3 bucket owned by the cybersecurity team and accessible only by the IAM role of the EC2 instances. Use the Systems Manager Session Manager as the HTTPS session manager for the application.</p>"
			},
			{
				"correct": true,
				"answer": "Configure an IAM policy that authorizes access to the certificate store only for the cybersecurity team and then add a configuration to terminate the SSL on the ELB."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the best solution is to set the appropriate IAM policy to both the DevOps and cybersecurity teams and then add a configuration to terminate the SSL on the ELB.</p><p>Take note that you can either terminate the SSL on the ELB side or on the EC2 instance. If you choose the former, the X.509 certificate will only be present in the ELB and if you choose the latter, the X.509 certificate will be stored inside the EC2 instance.</p><p>Since we don't want the DevOps team to have access to the certificate, it is best to terminate the SSL on the ELB level rather than the EC2.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_ssl_cert.png\"></p><p>Therefore, the correct answer is: <strong>Configure an IAM policy that authorizes access to the certificate store only for the cybersecurity team and then adding a configuration to terminate the SSL on the ELB.</strong></p><p>The option that says: <strong>Use the AWS Config service to configure the EC2 instances to retrieve the X.509 certificate upon boot from a CloudHSM that is managed by the cybersecurity team</strong> is incorrect. The AWS Config service simply enables you to assess, audit, and evaluate the configurations of your AWS resources. It does not grant any permission or access. In addition, CloudHSM is a managed hardware security module (HSM) in the AWS Cloud that handles encryption keys and not SSL certificates.</p><p>The option that says: <strong>Upload the X.509 certificate to an S3 bucket owned by the cybersecurity team and accessible only by the IAM role of the EC2 instances and using the Systems Manager Session Manager as the HTTPS session manager for the application</strong> is incorrect because the Systems Manager Session Manager service simply provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys. This service does not handle SSL connections. It is also a security risk to store X.509 certificates in an S3 bucket. It should be stored in the AWS Certificate Manager.</p><p>The option that says: <strong>Set up a Service Control Policy (SCP) that authorizes access to the certificate store only for the cybersecurity team and then adding a configuration to terminate the SSL on the ELB</strong> is incorrect. A service control policy (SCP) simply determines what services and actions can be delegated by administrators to the users and roles in the accounts that the SCP is applied to. It does not grant any permissions, unlike an IAM Policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/APIReference/API_UploadServerCertificate.html\">https://docs.aws.amazon.com/IAM/latest/APIReference/API_UploadServerCertificate.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/\">https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-ssl-termination/</a></p><p><br></p><p><strong>Check out these AWS Elastic Load Balancing (ELB) and IAM Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p></div>"
	},
	{
		"question": "<p>A shipping firm runs its web applications on its on-premises data center. The servers have a dependency on non-x86 hardware and the management plans to use AWS to scale its on-premises data storage. However, the backup application is only able to write to POSIX-compatible block-based storage. There is a total of 1,000 TB of data files that need to be mounted to a single folder on the file server. Existing users must also be able to access portions of this data while the backups are taking place.</p><p>Which of the following backup solutions would be most appropriate to meet the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use Amazon Glacier as the target for your data backups."
			},
			{
				"correct": false,
				"answer": "Provision Gateway Stored Volumes from AWS Storage Gateway."
			},
			{
				"correct": false,
				"answer": "Use Amazon S3 as the target for your data backups."
			},
			{
				"correct": true,
				"answer": "Provision Gateway Cached Volumes from AWS Storage Gateway."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Storage Gateway</strong> connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the AWS Cloud for scalable and cost-effective storage that helps maintain data security. Gateway-Cached volumes can support volumes of 1,024TB in size, whereas Gateway-stored volume supports volumes of 512 TB size.</p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_gateway_stored.png\"></p><p>Therefore, the correct answer is: <strong>Provision Gateway Cached Volumes from AWS Storage Gateway</strong> is correct because it supports volumes of up to 1,024 TB in size, and the frequently accessed data is stored on the on-premises server while the entire data is backed up over AWS.</p><p>The option that says: <strong>Use Amazon Glacier as the target for your data backups</strong> is incorrect. The data stored in Amazon Glacier is not available immediately. Retrieval jobs typically require 3-5 hours to complete.</p><p>The option that says: <strong>Provision Gateway Stored Volumes from AWS Storage Gateway</strong> is incorrect. Gateway stored volumes can only store up to 512 TB worth of data.</p><p>The option that says: <strong>Use Amazon S3 as the target for your data backups</strong> is incorrect. Amazon S3 is designed for object storage and not ideal for POSIX compliant data.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#resource-volume-limits\">https://docs.aws.amazon.com/storagegateway/latest/userguide/resource-gateway-limits.html#resource-volume-limits</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company has launched a web service in the cloud that analyzes tweets filtered by keywords. This service is hosted on a fleet of on-demand EC2 instances running in multiple Availability Zones with Auto Scaling, and are load-balanced by an application load balancer. After checking the load balancer logs, the solutions architect noticed that on-demand EC2 instances in one of the AZ's are not receiving requests.</p><p>Which of the following option is the most likely cause of this issue?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Amazon EC2 Auto scaling does not span multiple availability zones.</p>"
			},
			{
				"correct": false,
				"answer": "You have to manually add instances in each AZ for them to receive traffic."
			},
			{
				"correct": false,
				"answer": "<p>Multi-AZ autoscaling only works in the North Virginia region.</p>"
			},
			{
				"correct": true,
				"answer": "<p>The availability zone that is not receiving traffic was not associated with the application load balancer.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can set up your load balancer in EC2-Classic to distribute incoming requests across EC2 instances in a single Availability Zone or multiple Availability Zones. First, launch EC2 instances in all the Availability Zones that you plan to use. Next, register these instances with your load balancer. Finally, add the Availability Zones to your load balancer. After you add an Availability Zone, the load balancer starts routing requests to the registered instances in that Availability Zone. Note that you can modify the Availability Zones for your load balancer at any time. By default, the load balancer routes requests evenly across its Availability Zones. To route requests evenly across the registered instances in the Availability Zones, enable cross-zone load balancing.</p><p>If cross-zone load balancing is disabled:</p><p>- Each of the two targets in Availability Zone A receives 25% of the traffic.</p><p>- Each of the eight targets in Availability Zone B receives 6.25% of the traffic.</p><p>This is because each load balancer node can route 50% of its client traffic only to targets in its Availability Zone.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cross_zone_load_balancing_disabled.png\"></p><p>If cross-zone load balancing is enabled, each of the 10 targets receives 10% of the traffic. This is because each load balancer node can route 50% of its client traffic to all 10 targets.</p><p>Therefore, the correct answer is: <strong>The availability zone that is not receiving traffic was not associated with the application load balancer.</strong> Most likely, the reason is that the specific AZ is not added to the ELB.</p><p>The option that says:<strong> Amazon EC2 Auto Scaling does not span multiple availability zones</strong> is incorrect because autoscaling can work with multiple AZs.</p><p>The option that says:<strong> Multi-AZ autoscaling only works in the North Virginia region</strong> is incorrect because autoscaling can be enabled for multi AZ in any single region, not just N. Virginia.</p><p>The option that says:<strong> You have to manually add instances in each AZ for them to receive traffic</strong> is incorrect because instances need not be added manually to AZ.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html/\">https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-disable-az.html/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p></div>"
	},
	{
		"question": "<p>A clothing company is using a proprietary e-commerce platform as their online shopping website. The e-commerce platform is hosted on a fleet of on-demand EC2 instances that are launched in a public subnet. Aside from acting as web servers, these EC2 instances also fetch updates and critical security patches from the Internet. The Solutions Architect was tasked to ensure that the instances can only initiate outbound requests to specific URLs provided by the proprietary e-commerce platform while accepting all inbound requests from the online shoppers.</p><p>Which of the following is the BEST solution that the Architect should implement in this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create a new NAT Gateway in your VPC. Place the EC2 instances to the private subnet and connect it to a NAT Gateway which will handle the outbound URL restriction."
			},
			{
				"correct": false,
				"answer": "Implement a Network ACL to all specific URLs by the e-commerce platform with an implicit deny rule."
			},
			{
				"correct": true,
				"answer": "<p>In your VPC, launch a new web proxy server that only allows outbound access to the URLs provided by the proprietary e-commerce platform.</p>"
			},
			{
				"correct": false,
				"answer": "Create a new NAT Instance in your VPC. Place the EC2 instances to the private subnet and connect it to a NAT Instance which will handle the outbound URL restriction."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Proxy servers usually act as a relay between internal resources (servers, workstations, etc.) and the Internet, and to filter, accelerate and log network activities leaving the private network. One must not confuse proxy servers (also called forwarding proxy servers) with reverse proxy servers, which are used to control and sometimes load-balance network activities entering the private network.</p><p><img src=\"https://media.tutorialsdojo.com/sap_squid_proxy.jpg\"></p><p>Therefore, the correct answer is: <strong>Launch a new web proxy server that only allows outbound access to the URLs provided by the proprietary e-commerce platform in your VPC.</strong> It launches a proxy server which filters requests from the client and then only allows certain URLs provided by the proprietary e-commerce platform.</p><p>The option that says:<strong><em> </em>Create a new NAT Instance in your VPC. Place the EC2 instances to the private subnet and connect it to a NAT Instance which will handle the outbound URL restriction</strong> is absolutely wrong considering that the EC2 instances are used as public facing web servers and thus, must be deployed in the public subnet. An instance in private subnet and connected to a NAT Instance will not be able to accept inbound connections to the online shopping website.</p><p>The option that says: <strong>Create a new NAT Gateway in your VPC. Place the EC2 instances to the private subnet and connect it to a NAT Gateway which will handle the outbound URL restriction</strong> is incorrect with the same reason as the above option. An instance in private subnet and connected to a NAT Gateway will not be able to accept inbound connections to the online shopping site.</p><p>The option that says: <strong>Implementing a Network ACL to all specific URLs by the e-commerce platform with an implicit deny rule </strong>is incorrect because a network access control list (Network ACL) has limited functionality and cannot filter requests based on URLs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/articles/using-squid-proxy-instances-for-web-service-access-in-amazon-vpc-another-example-with-aws-codedeploy-and-amazon-cloudwatch/\">https://aws.amazon.com/articles/using-squid-proxy-instances-for-web-service-access-in-amazon-vpc-another-example-with-aws-codedeploy-and-amazon-cloudwatch/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/\">https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>An e-commerce company is having their annual sale event where buyers will be able to purchase goods at a large discount on their e-commerce website. The e-commerce site will receive millions of visitors in a short period of time when the sale begins. The visitors will first login to the site using either their Facebook or Google credentials and add items to their cart. After purchasing, a page will display the cart items along with the discounted prices. The company needs to build a checkout system that can handle the sudden surge of incoming traffic. </p><p>Which of the following is the MOST scalable solution that they should use?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Combine an Elastic Load balancer in front of multiple web servers with CloudFront for fast delivery. The web servers will first authenticate the users by logging into their social media accounts which are integrated in Amazon Cognito. The web servers will process the user's purchases and store them in a DynamoDB table. Use an IAM Role to gain permissions to the DynamoDB table.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Combine an Elastic Load balancer in front of an Auto Scaling group of web servers with CloudFront for fast delivery. The web servers will first authenticate the users by logging into their social media accounts which are integrated in Amazon Cognito, then process the user's purchases and store them into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the queue. Finally, the items from the queue are retrieved by a set of application servers and stored into a DynamoDB table.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Combine an Elastic Load balancer in front of an Auto Scaling group of web servers with CloudFront for fast delivery. The web servers will first authenticate the users by logging into their social media accounts which are integrated in Amazon Lex, then process the user's purchases and store the cart into a Multi-AZ RDS database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the static website hosting feature of Amazon S3 with the Javascript SDK to authenticate the user login with Amazon Cognito. Set up AWS Global Accelerator to deliver the static content stored in the S3 bucket. Store user purchases in a DynamoDB table and use an IAM Role for managing permissions.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb_workflow.PNG\"></p><p>In this scenario, the best solution is to use a combination of CloudFront, Elastic Load Balancer and SQS to provide a highly scalable architecture.</p><p>Hence, the correct answer is: <strong>Combine an Elastic Load balancer in front of an Auto Scaling group of web servers with CloudFront for fast delivery. The web servers will first authenticate the users by logging into their social media accounts which are integrated in Amazon Cognito, then process the user's purchases and store them into an SQS queue using IAM Roles for EC2 Instances to gain permissions to the queue. Finally, the items from the queue are retrieved by a set of application servers and stored into a DynamoDB table. </strong>This is a highly scalable solution and creates an appropriate IAM Role to access the DynamoDB database. In addition, it uses SQS which decouples the application architecture. This will allow the application servers to process the requests.</p><p>The option that says: <strong>Combine an Elastic Load balancer in front of an Auto Scaling group of web servers with CloudFront for fast delivery. The web servers will first authenticate the users by logging into their social media accounts which are integrated in Amazon Lex, then process the user's purchases and store the cart into a Multi-AZ RDS database</strong> is incorrect because multi-AZ RDS is a more expensive solution when compared to DynamoDB. In addition, Amazon Lex is just a service for building conversational interfaces into any application using voice and text. This is not utilized for user authentication, unlike Amazon Cognito.</p><p>The option that says:<strong> Use the static website hosting feature of Amazon S3 with the Javascript SDK to authenticate the user login with Amazon Cognito. Set up AWS Global Accelerator to deliver the static content. Store user purchases in a DynamoDB table and use an IAM Role for managing permissions</strong> is incorrect. Although this would work, it is not scalable, and storing all the data directly in DynamoDB would consume read and write capacity and increase the cost. Moreover, you cannot use AWS Global Accelerator to deliver the static content stored in the S3 bucket. You have to use Amazon CloudFront instead.</p><p>The option that says:<strong> Combine an Elastic Load balancer in front of multiple web servers with CloudFront for fast delivery. The web servers will first authenticate the users by logging into their social media accounts which are integrated in Amazon Cognito. The web servers will process the user's purchases and store them in a DynamoDB table. Use an IAM Role to gain permissions to the DynamoDB table</strong> is incorrect because it is not scalable and storing all the data directly in DynamoDB would consume read and write capacity and increase the cost. Moreover, the web servers are not placed in an Auto Scaling group, which means that this solution is not scalable.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/authentication-and-access-control.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/authentication-and-access-control.html</a></p><p><a href=\"https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>An e-commerce company is running a three-tier application on AWS. The application includes a web tier as frontend, an application tier as backend, and the database tier that stores the transactions and users' data. The database is currently hosted on an extra-large instance with 128 GB of memory. For the company’s business continuity and disaster recovery plan, the Solutions Architect must ensure a Recovery Time Objective (RTO) of 5 minutes and a Recovery Point Objective (RPO) of 1 hour on the backup site in the event that the application goes down. There is also a requirement for the backup site to be at least 250 miles away from the primary site.</p><p>Which of the following solutions must the Solutions Architect implement to meet the company’s disaster recovery requirements while keeping the cost at a minimum?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a frequently scheduled backup of the application and database that will be stored on an Amazon S3 bucket. Configure Amazon S3 Cross-Region Replication (CRR) on the bucket to copy the backups to another region. In case of disaster, use an AWS CloudFormation template to quickly replicate the same resources to the backup region and restore the data from the S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a multi-region strategy for the backup region to comply with the tight RTO and RPO requirements. Create a fully functional web, application, and database tier on the backup region with the same capacity as the primary region. Set the database on the backup region on standby mode. In case of disaster, update the Amazon Route 53 record to point to the backup region.</p>"
			},
			{
				"correct": true,
				"answer": "<p>On the backup region, create a scaled-down version of the fully functional environment with one EC2 instance of the web server and application server in their own Auto Scaling groups behind Application Load Balancers. Create a standby database instance that replicates data from the primary database. In case of disaster, scale the instances to meet the demand and update the Amazon Route 53 record to point to the backup region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a pilot light strategy for the backup region. Configure the primary database to replicate data to a large standby instance in the backup region. In case of a disaster, vertically resize the database instance to meet the full demand. Create an AWS CloudFormation template to quickly provision the same web servers, application servers, and load balancers on the backup region. Update the Amazon Route 53 records to point to the backup region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Having backups and redundant workload components in place is the start of your DR strategy. RTO and RPO are your objectives for the restoration of your workload. Set these based on business needs.</p><p><strong>Recovery Time Objective (RTO)</strong> is defined by the organization. RTO is the maximum acceptable delay between the interruption of service and restoration of service. This determines what is considered an acceptable time window when service is unavailable.</p><p><strong>Recovery Point Objective (RPO)</strong> is defined by the organization. RPO is the maximum acceptable amount of time since the last data recovery point. This determines what is considered an acceptable loss of data between the last recovery point and the interruption of service.</p><p>When architecting a multi-region disaster recovery strategy for your workload, you should choose one of the following multi-region strategies. They are listed in increasing order of complexity, and decreasing order of RTO and RPO. DR Region refers to an AWS Region other than the one primarily used for your workload (or any AWS Region if your workload is on-premises).</p><p><img src=\"https://media.tutorialsdojo.com/sap_dr_rto_rpo.png\"></p><p><strong>- Backup and restore (RPO in hours, RTO in 24 hours or less)</strong>: Back up your data and applications using point-in-time backups into the DR Region. Restore this data when necessary to recover from a disaster.</p><p><strong>- Pilot light (RPO in minutes, RTO in hours)</strong>: Replicate your data from one region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup such as databases and object storage are always on. Other elements such as application servers are loaded with application code and configurations, but are switched off and are only used during testing or when Disaster Recovery failover is invoked.</p><p><strong>- Warm standby (RPO in seconds, RTO in minutes)</strong>: Maintain a scaled-down but fully functional version of your workload always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load.</p><p><strong>- Multi-region (multi-site) active-active (RPO near zero, RTO potentially zero)</strong>: Your workload is deployed to, and actively serving traffic from, multiple AWS Regions. This strategy requires you to synchronize data across Regions.</p><p>The difference between <strong>Pilot Light</strong> and <strong>Warm Standby</strong> can sometimes be difficult to understand. Both include an environment in your DR Region with copies of your primary region assets. The distinction is that Pilot Light cannot process requests without additional action taken first, while Warm Standby can handle traffic (at reduced capacity levels) immediately. Pilot Light will require you to turn on servers, possibly deploy additional (non-core) infrastructure and then scale up. In Warm Standby, it only requires you to scale up your resources since all the necessary components are already deployed and running). You can choose between these two disaster recovery strategies based on your RTO and RPO needs.</p><p>Therefore, the correct answer is:<strong> On the backup region, create a scaled-down version of the fully functional environment with one EC2 instance of the web server and application server in their own Auto Scaling groups behind Application Load Balancers. Create a standby database instance that replicates data from the primary database. In case of disaster, scale the instances to meet the demand and update the Amazon Route 53 record to point to the backup region.</strong></p><p>The option that says: <strong>Create a frequently scheduled backup of the application and database that will be stored on an Amazon S3 bucket. Configure Amazon S3 Cross-Region Replication (CRR) on the bucket to copy the backups to another region. In case of disaster, use an AWS CloudFormation template to quickly replicate the same resources to the backup region and restore the data from the S3 bucket</strong> is incorrect. Basically, this is a backup and restore strategy that has RPO in hours and RTO in 24 hours or less. Even with using CloudFormation, provisioning resources and restoring the backups from an Amazon S3 bucket may take a long time. Take note that the required RTO is only 5 minutes.</p><p>The option that says: <strong>Use a pilot light strategy for the backup region. Configure the primary database to replicate data to a large standby instance in the backup region. In case of a disaster, vertically resize the database instance to meet the full demand. Create an AWS CloudFormation template to quickly provision the same web servers, application servers, and load balancers on the backup region. Update the Amazon Route 53 records to point to the backup region</strong> is incorrect. Although this reduces the recovery time because the database instance is already replicated, you may still miss the 5 minute RTO because provisioning the needed servers and load balancers may take several minutes.</p><p>The option that says: <strong>Use a multi-region strategy for the backup region to comply with the tight RTO and RPO requirements. Create a fully functional web, application, and database tier on the backup region with the same capacity as the primary region. Set the database on the backup region on standby mode. In case of disaster, update the Amazon Route 53 record to point to the backup region</strong> is incorrect. Although this meets the RTO and RPO requirements, keeping a multi-region strategy is very expensive. With the given cost-effectiveness requirement, using the warm standby strategy is the most suitable one to use in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/\">https://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/Storage/Backup_and_Recovery_Approaches_Using_AWS.pdf\">https://d1.awsstatic.com/whitepapers/Storage/Backup_and_Recovery_Approaches_Using_AWS.pdf</a></p><p><br></p><p><strong>Backup and Restore vs Pilot Light vs Warm Standby vs Multi-Site:</strong></p><p><a href=\"https://tutorialsdojo.com/backup-and-restore-vs-pilot-light-vs-warm-standby-vs-multi-site/?src=udemy\">https://tutorialsdojo.com/backup-and-restore-vs-pilot-light-vs-warm-standby-vs-multi-site/</a></p></div>"
	},
	{
		"question": "<p>A leading online media company runs a popular sports news website. The solutions architect has been tasked to analyze each web visitor's clickstream data on the website to populate user analytics, which gives you an insight on the sequence of pages and advertisements the visitor has clicked. The data will be processed real-time which will then transform the page layout as the visitors click through the web portal to increase user engagement and consequently, increase the revenue for the company.</p><p>Which of the following options should the solutions architect implement to meet the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce. "
			},
			{
				"correct": true,
				"answer": "Push web clicks by session to Amazon Kinesis and analyze behavior using Amazon Kinesis workers. "
			},
			{
				"correct": false,
				"answer": "Write click events directly to Amazon Redshift and then analyze with SQL. "
			},
			{
				"correct": false,
				"answer": "Publish web clicks by session to an Amazon SQS queue and periodically drain these events to Amazon RDS then analyze with SQL. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Kinesis</strong> makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. Amazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application. With Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications. Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.</p><p>In this example, a simple HTML page simulates the content of a blog page. As the reader scrolls the simulated blog post, the browser script uses the SDK for JavaScript to record the scroll distance down the page and send that data to Kinesis using the <code><strong>putRecords</strong></code><strong> </strong>method of the Kinesis client class. The streaming data captured by Amazon Kinesis Data Streams can then be processed by Amazon EC2 instances and stored in any of several data stores including Amazon DynamoDB and Amazon Redshift.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_arch.JPG\"></p><p>Therefore, the correct answer is: <strong>Push web clicks by session to Amazon Kinesis and analyzing behavior using Amazon Kinesis workers.</strong></p><p>The following options are incorrect as SQS, EC2, EMR services do not have the capacity to analyze real-time streaming data:</p><p><strong>- Log clicks in weblogs by URL and store it in Amazon S3, and then analyze with Elastic MapReduce.</strong></p><p><strong>- Write click events directly to Amazon Redshift and then analyze with SQL.</strong></p><p><strong>- Publish web clicks by session to an Amazon SQS queue and periodically drain these events to Amazon RDS then analyze with SQL.</strong></p><p><br><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/introduction.html\">https://docs.aws.amazon.com/streams/latest/dev/introduction.html</a></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><a href=\"https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html\">https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/kinesis-examples-capturing-page-scrolling.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
	},
	{
		"question": "<p>A company stores several terabytes of data on an Amazon S3 bucket. The data will be made available to respective partner companies, however, the management doesn’t want the partner companies to access the files directly from Amazon S3 URLs. The solutions architect has been asked to ensure that all confidential files shared via Amazon S3 should only be accessible through CloudFront.</p><p>Which of the following options could satisfy this requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Assign an IAM user that is granted access to objects in the S3 bucket to CloudFront."
			},
			{
				"correct": true,
				"answer": "<p>Create an Origin Access Identity (OAI) and associate it with your CloudFront distribution. Change the permissions on your Amazon S3 bucket so that only the origin access identity has read permission.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write individual policies for each S3 bucket containing the confidential documents that would grant CloudFront access.</p>"
			},
			{
				"correct": false,
				"answer": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN). "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>To restrict access to content that you serve from <strong>Amazon S3</strong> buckets, you create <strong>CloudFront signed URLs</strong> or <strong>signed cookies</strong> to limit access to files in your Amazon S3 bucket, and then you create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Then you configure permissions so that CloudFront can use the OAI to access and serve files to your users, but users can't use a direct URL to the S3 bucket to access a file there. Taking these steps helps you maintain secure access to the files that you serve through CloudFront.</p><p>In general, if you're using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you limit access by using, for example, CloudFront signed URLs or signed cookies, you also won't want people to be able to view files by simply using the direct URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your protections work.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\"></p><p>Typically, if you're using an Amazon S3 bucket as the origin for a CloudFront distribution, you grant everyone permission to read the objects in your bucket. This allows anyone to access your objects either through CloudFront or using the Amazon S3 URL. CloudFront doesn't expose Amazon S3 URLs, but your users might have those URLs if your application serves any objects directly from Amazon S3 or if anyone gives out direct links to specific objects in Amazon S3.</p><p>Therefore, the correct answer is: <strong>Create an Origin Access Identity (OAI) and associate it with your CloudFront distribution. Change the permissions on your Amazon S3 bucket so that only the origin access identity has read permission.</strong> It gives CloudFront exclusive access to the S3 bucket and prevents other users from accessing the public content of S3 directly via S3 URL.</p><p>The option that says: <strong>Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN)</strong> is incorrect. Creating a bucket policy is unnecessary and it does not prevent other users from accessing the public content of S3 directly via S3 URL.</p><p>The option that says: <strong>Assign an IAM user that is granted access to objects in the S3 bucket to CloudFront</strong> is incorrect. This does not give CloudFront exclusive access to the S3 bucket.</p><p>The option that says: <strong>Write individual policies for each S3 bucket containing the confidential documents that would grant CloudFront access</strong> is incorrect. You do not need to create any individual policies for each bucket.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-access-to-amazon-s3/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)</strong></p><p><a href=\"https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/?src=udemy\">https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A mobile game startup is building an immersive augmented reality (AR), massively multiplayer, first-person online shooter game. All of their servers, databases, and resources are hosted in their cloud infrastructure in AWS. Upon testing the new game, it was noted that the loading time of the game assets and data are quite sluggish including their static content. You recommended adding caching to the application to improve load times.</p><p>In this scenario, which of the following cache services can you recommend for their gaming applications?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use ElastiCache to distribute their static content and CloudFront as an in-memory data store. "
			},
			{
				"correct": false,
				"answer": "Use CloudFront to distribute their static content and DynamoDB as an in-memory data store. "
			},
			{
				"correct": true,
				"answer": "Use CloudFront to distribute their static content and ElastiCache as an in-memory data store. "
			},
			{
				"correct": false,
				"answer": "Use CloudFront to distribute their static content and an Apache Ignite ElastiCache as an in-memory data store. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon ElastiCache</strong> allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Amazon ElastiCache is a popular choice for real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing.</p><p><strong>Amazon CloudFront</strong> is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment.</p><p>In this scenario, the best option is to use a combination of Amazon CloudFront for caching static content and Amazon ElastiCache as the in-memory data store.</p><p>Therefore the correct answer is: <strong>Use CloudFront to distribute their static content and ElastiCache as an in-memory data store</strong>.</p><p>The option that says: <strong>Use ElastiCache to distribute their static content and CloudFront as an in-memory data store</strong> is incorrect. ElastiCache is used for caching database queries and is not suitable for distributing static content.</p><p>The option that says: <strong>Use CloudFront to distribute their static content and an Apache Ignite ElastiCache as an in-memory data store</strong> is incorrect. Although Apache Ignite is an in-memory data store, only Redis and Memcached are supported in ElastiCache.</p><p>The option that says: <strong>Use CloudFront to distribute their static content and DynamoDB as an in-memory data store</strong> is incorrect. DynamoDB is a NoSQL database and hence, not suitable for caching and in using it as an in-memory data store.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><br></p><p><strong>Check out these Amazon Elasticache and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/aamazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p><br>A company located on the west coast of North America plans to release a new online service for its customers. The company already created a new VPC in the us-west-1 region where they will launch the Amazon EC2 instances that will host the web application. The application must be highly-available and must dynamically scale based on user traffic. In addition, the company wants to have a disaster recovery site in the us-east-1 region that will act as a passive backup of the running application.</p><p>Which of the following options should the Solutions Architect implement in order to achieve the requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create an Application Load Balancer (ALB) in the us-west-1 region that spans multiple Availability Zones (AZs) of the VPC. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs and place it behind the ALB. Set up the same configuration to the us-east-1 region VPC. Create record entries in Amazon Route 53 pointing to the ALBs with health check enabled and a failover routing policy.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure an Inter-Region VPC peering between the us-west-1 VPC and a new VPC in the us-east-1 region. Create an Application Load Balancer (ALB) in the us-west-1 region that spans multiple Availability Zones (AZs) of the VPC. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs of both regions and place it behind the ALB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Application Load Balancer (ALB) in the us-west-1 region that spans multiple Availability Zones (AZs) of the VPC. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs and place it behind the ALB. Set up the same configuration to the us-east-1 region VPC. Create separate record entries for each region’s ALB on Amazon Route 53 and enable health checks to ensure high-availability for both regions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure an Inter-Region VPC peering between the us-west-1 VPC and a new VPC in the us-east-1 region. Create an Application Load Balancer (ALB) that spans multiple Availability Zones (AZs) on both VPCs. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs of both regions and place it behind the ALB. Create an Alias record entry in Amazon Route 53 that points to the DNS name of the ALB.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>On <strong>Amazon Route 53</strong>, after you create a hosted zone for your domain, such as tutorialsdojo.com, you create records to tell the Domain Name System (DNS) how you want traffic to be routed for that domain. You can create a record that points to the DNS name of your Application Load Balancer on AWS.</p><p>When you create a record, you choose a routing policy, which determines how Amazon Route 53 responds to queries:</p><p><strong>Simple routing policy</strong> – Use for a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.</p><p><strong>Failover routing policy</strong> – Use when you want to configure active-passive failover.</p><p><strong>Geolocation routing policy</strong> – Use when you want to route traffic based on the location of your users.</p><p><strong>Geoproximity routing policy</strong> – Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</p><p><strong>Latency routing policy</strong> – Use when you have resources in multiple AWS Regions and you want to route traffic to the region that provides the best latency.</p><p><strong>Multivalue answer routing policy</strong> – Use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.</p><p><strong>Weighted routing policy</strong> – Use to route traffic to multiple resources in proportions that you specify.</p><p>You can use <strong>Route 53 health checks</strong> to configure active-active and active-passive failover configurations. You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy.</p><p><img src=\"https://media.tutorialsdojo.com/route53-failover.png\"></p><p>Use an <strong>active-passive failover configuration</strong> when you want a primary resource or group of resources to be available the majority of the time and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. When responding to queries, Route 53 includes only the “healthy primary” resources. If all the primary resources are unhealthy, Route 53 begins to include only the healthy secondary resources in response to DNS queries.</p><p>This way, you can create a failover routing policy that will direct traffic to the backup region when your primary region fails.</p><p>Amazon EC2 Auto Scaling integrates with Elastic Load Balancing to enable you to insert one or more Classic Load Balancers, or a single Application Load Balancer, Network Load Balancer, or Gateway Load Balancer with multiple target groups in front of your Auto Scaling group.</p><p>Creating an Application Load Balancer on each region with an Auto Scaling group that spans multiple Availability Zones ensures that your application will be highly available and will scale based on the user traffic.</p><p>Therefore, the correct answer is: <strong>Create an Application Load Balancer (ALB) in the us-west-1 region that spans multiple Availability Zones (AZs) of the VPC. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs and place it behind the ALB. Set up the same configuration to the us-east-1 region VPC. Create record entries in Amazon Route 53 pointing to the ALBs with health check enabled and a failover routing policy.</strong></p><p>The option that says: <strong>Configure an Inter-Region VPC peering between the us-west-1 VPC and a new VPC in the us-east-1 region. Create an Application Load Balancer (ALB) in the us-west-1 region that spans multiple Availability Zones (AZs) of the VPC. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs of both regions and place it behind the ALB</strong> is incorrect because an Auto Scaling group cannot span AZs on multiple regions and the Application Load Balancer cannot serve traffic to EC2 instances on a different region even with Inter-Region VPC peering.</p><p>The option that says: <strong>Configure an Inter-Region VPC peering between the us-west-1 VPC and a new VPC in the us-east-1 region. Create an Application Load Balancer (ALB) that spans multiple Availability Zones (AZs) on both VPCs. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs of both regions and place it behind the ALB. Create an Alias record entry in Amazon Route 53 that points to the DNS name of the ALB</strong> is incorrect because an Application Load Balancer cannot span to multiple regions, only multiple AZs on the same region. An Auto Scaling group also cannot span multiple regions as it can only deploy EC2 instances in one region.</p><p>The option that says: <strong>Create an Application Load Balancer (ALB) in the us-west-1 region that spans multiple Availability Zones (AZs) of the VPC. Create an Auto Scaling group that will deploy EC2 instances across the multiple AZs and place it behind the ALB. Set up the same configuration to the us-east-1 region VPC. Create separate record entries for each region’s ALB on Amazon Route 53 and enable health checks to ensure high-availability for both regions</strong> is incorrect. Although this setup is possible, it does not mention the routing policy to be used on Amazon Route 53. The question requires that the second region acts as a passive backup, which means only the main region receives all the traffic so you need to specifically use failover routing policy in Amazon Route 53.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html</a></p><p><br></p><p><strong>Check out the Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>A company is using Microsoft Active Directory to manage all employee accounts and devices. The IT department instructed the solutions architect to implement a single sign-on feature to allow the employees to use their existing Windows account password to connect and use the various AWS resources.</p><p>Which of the following options is the recommended way to extend the current Active Directory domain to AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create users and groups with AWS Single Sign-On along with AWS Organizations to help you manage SSO access and user permissions across all the AWS accounts.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use IAM Roles to set up cross-account access and delegate access to resources that are in your AWS account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Cognito to authorize users to your applications using direct sign-in or through third-party apps, and access your apps' backend resources in AWS.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Directory Service to integrate your AWS resources with the existing Active Directory using trust relationship. Enable single sign-on using Managed Microsoft AD.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Because the company is using Microsoft Active Directory already, you can use <strong>AWS Directory Service for Microsoft AD</strong> to create secure Windows trusts between your on-premises Microsoft Active Directory domains and your AWS Microsoft AD domain in the AWS Cloud. By setting up a trust relationship, you can integrate SSO to the AWS Management Console and the AWS Command Line Interface (CLI), as well as your Windows-based workloads.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_directory_service.png\"></p><p><strong>AWS Directory Service</strong> helps you to set up and run a standalone AWS Managed Microsoft AD directory hosted in the AWS Cloud. You can also use AWS Directory Service to connect your AWS resources with an existing on-premises Microsoft Active Directory. To configure AWS Directory Service to work with your on-premises Active Directory, you must first set up trust relationships to extend authentication from on-premises to the cloud.</p><p>Therefore, the correct answer is: <strong>Use AWS Directory Service to integrate your AWS resources with the existing Active Directory using trust relationship. Enable single sign-on using Managed Microsoft AD.</strong></p><p>The option that says: <strong>Use Amazon Cognito to authorize users to your applications using direct sign-in or through</strong> <strong>third-party apps, and accessing your apps' backend resources in AWS</strong> is incorrect because Cognito is primarily used for federation to your web and mobile apps running on AWS. It allows you to authenticate users through social identity providers. But since the company is already using Microsoft AD, AWS Directory Service is the better choice here.</p><p>The option that says: <strong>Creating users and groups with AWS Single Sign-On along with AWS Organizations to help you manage SSO access and user permissions across all the AWS accounts</strong> is incorrect because using the AWS Single Sign-On service alone is not enough to meet the requirement. Although it can help you manage SSO access and user permissions across all your AWS accounts in AWS Organizations, you still have to use the AWS Directory Service to integrate your on-premises Microsoft AD. AWS SSO integrates with Microsoft AD using AWS Directory Service so there is no need to create users and groups.</p><p>The option that says: <strong>Use IAM Roles to set up cross-account access and delegate access to resources that are in your AWS account</strong> is incorrect because setting up cross-account access allows you to share resources in one AWS account with users in a different AWS account. Since the company is already using Microsoft AD then the better choice to use here is the AWS Directory Service.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/directoryservice/\">https://aws.amazon.com/directoryservice/</a></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-directory-connected.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/manage-your-directory-connected.html</a></p><p><br></p><p><strong>AWS Identity Services Overview:</strong></p><p><a href=\"https://youtu.be/AIdUw0i8rr0\">https://youtu.be/AIdUw0i8rr0</a></p><p><br></p><p><strong>Check out this AWS Directory Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-directory-service/?src=udemy\">https://tutorialsdojo.com/aws-directory-service/</a></p></div>"
	},
	{
		"question": "<p>A company has built an application that allows painters to upload photos of their creations. The app allows users from North America and European regions to browse the galleries and order their chosen artworks. The application is hosted on a fixed set of Amazon EC2 instances in the us-east-1 region. Using mobile phones, the artists can scan and upload large, high-resolution images of their artworks which are stored in a centralized Amazon S3 bucket also in the same region. After the initial week of operation, the European artists are reporting slow performance on their image uploads.</p><p>Which of the following is the best solution to improve the image upload process?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Enable Amazon S3 Transfer Acceleration on the central S3 bucket. Use the s3-accelerate endpoint to upload the images.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Enable multipart upload on Amazon S3 and redeploy the application to support it. This allows the transmitting of separate parts of the image in parallel.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set the centralized Amazon S3 bucket as the custom origin on an Amazon CloudFront distribution. This will use CloudFront’s global edge network to improve the upload speed.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Increase the upload capacity by converting the Amazon EC2 instances to an Auto Scaling Group that can scale automatically based on the users' traffic.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon S3 Transfer Acceleration</strong> enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.</p><p><strong><em><img src=\"https://media.tutorialsdojo.com/s3-transfer-acceleration.png\"></em></strong></p><p>You might want to use Transfer Acceleration on a bucket for various reasons, including the following:</p><p>- You have customers that upload to a centralized bucket from all over the world.</p><p>- You transfer gigabytes to terabytes of data on a regular basis across continents.</p><p>- You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.</p><p>You can enable Transfer Acceleration on a bucket in any of the following ways:</p><p>- Use the Amazon S3 console.</p><p>- Use the REST API PUT Bucket accelerate operation.</p><p>- Use the AWS CLI and AWS SDKs.</p><p>You can transfer data to and from the acceleration-enabled bucket by using one of the following s3-accelerate endpoint domain names:</p><p><code>- s3-accelerate.amazonaws.com</code> – to access an acceleration-enabled bucket.</p><p><code>- s3-accelerate.dualstack.amazonaws.com</code> – to access an acceleration-enabled bucket over IPv6. Amazon S3 dual-stack endpoints support requests to S3 buckets over IPv6 and IPv4.</p><p>You can point your Amazon S3 PUT object and GET object requests to the s3-accelerate endpoint domain name after you enable Transfer Acceleration. After Transfer Acceleration is enabled, it can take up to 20 minutes for you to realize the performance benefit. However, the accelerate endpoint will be available as soon as you enable Transfer Acceleration.</p><p>Therefore, the correct answer is: <strong>Enable Amazon S3 Transfer Acceleration on the central S3 bucket. Use the s3-accelerate endpoint to upload the images.</strong></p><p>The option that says: <strong>Enable multipart upload on Amazon S3 and redeploy the application to support it. This allows the transmitting of separate parts of the image simultaneously</strong> is incorrect. Although multipart upload can improve the upload throughput to an S3 bucket, the European users are still limited on their Internet connection to the S3 bucket in the US region. S3 Transfer acceleration uses AWS backbone network which can optimize the transfer from far away regions.</p><p>The option that says: <strong>Set the centralized Amazon S3 bucket as the custom origin on an Amazon CloudFront distribution. This will use CloudFront’s global edge network to improve the upload speed</strong> is incorrect. CloudFront distribution is designed for optimizing content delivery and content caching. Although CloudFront supports content uploads via POST, PUT, other HTTP Methods, there is a limited connection timeout to the origin (60 seconds). If uploads will take several minutes, the connection might get terminated. If you want to optimize performance when uploading large files to Amazon S3, it is recommended to use Amazon S3 Transfer Acceleration which can provide fast and secure transfers over long distances. Transfer Acceleration uses Amazon CloudFront's globally distributed edge locations.</p><p>The option that says: <strong>Increase the upload capacity by converting the Amazon EC2 instances to an Auto Scaling Group that can scale automatically based on the users' traffic</strong> is incorrect because the images are directly being uploaded to the S3 bucket so increasing the number of EC2 instances does not necessarily improve the S3 upload speeds. Even if the application is configured to use the EC2 instance as a temporary storage for the images, the upload experience of the users will not improve because they are uploading from a different continent.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration-examples.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-upload-large-files/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-upload-large-files/</a></p><p><br></p><p><strong>Check out this AWS Transfer Acceleration Comparison Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/?src=udemy\">https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</a></p></div>"
	},
	{
		"question": "<p>A company has recently released a new mobile game. With the boost in marketing, the mobile game suddenly became viral. The registration webpage is bombarded with user registrations from around the world. The registration website is hosted on a fleet of Amazon EC2 instances created as an Auto Scaling group. This cluster is behind an Application Load Balancer to balance the user traffic. The website contains static content that is loaded differently depending on the user’s device type. With the sudden increase in user traffic, the fleet of Amazon EC2 instances experienced high CPU usage and users are reporting sluggishness on the website.</p><p>Which of the following options should the Solutions Architect implement to improve the website response time?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon S3 bucket to host the static contents. Set this bucket as the origin for an Amazon CloudFront distribution. Configure CloudFront to deliver different contents depending on the user’s <code>User-Agent HTTP</code> header.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a Network Load Balancer (NLB) instead of an ALB to distribute the user traffic. Create a dedicated Auto Scaling group for the different device types. Configure the NLB to parse the <code>User-Agent HTTP</code> header to route the users to the appropriate EC2 Auto Scaling groups.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a dedicated Auto Scaling group for the different device types and create separate Application Load Balancers (ALB) for each group. Create an Amazon Route 53 entry to route the users to the appropriate ALB depending on their <code>User-Agent HTTP</code> header.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon S3 bucket to host the static contents. Set this bucket as the origin for an Amazon CloudFront distribution. Write a Lambda@Edge function to parse the <code>User-Agent HTTP</code> header and serve the appropriate contents based on the user’s device type.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Lambda@Edge</strong> is an extension of AWS Lambda, a compute service that lets you execute functions that customize the content that CloudFront delivers. You can author Node.js or Python functions in one Region, US-East-1 (N. Virginia), and then execute them in AWS locations globally that are closer to the viewer, without provisioning or managing servers. Lambda@Edge scales automatically, from a few requests per day to thousands per second. Processing requests at AWS locations closer to the viewer instead of on origin servers significantly reduces latency and improves the user experience.</p><p><img src=\"https://media.tutorialsdojo.com/sap_lambda_workflow.png\"></p><p>When you associate a <strong>CloudFront distribution</strong> with a <strong>Lambda@Edge function</strong>, CloudFront intercepts requests and responses at CloudFront edge locations. You can execute Lambda functions when the following CloudFront events occur:</p><p>- When CloudFront receives a request from a viewer (viewer request)</p><p>- Before CloudFront forwards a request to the origin (origin request)</p><p>- When CloudFront receives a response from the origin (origin response)</p><p>- Before CloudFront returns the response to the viewer (viewer response)</p><p>There are many uses for Lambda@Edge processing. For example:</p><p>- A Lambda function can inspect cookies and rewrite URLs so that users see different versions of a site for A/B testing.</p><p>- CloudFront can return different objects to viewers based on the device they're using by checking the User-Agent header, which includes information about the devices. For example, CloudFront can return different images based on the screen size of their device. Similarly, the function could consider the value of the Referer header and cause CloudFront to return the images to bots that have the lowest available resolution.</p><p>- Or you could check cookies for other criteria. For example, on a retail website that sells clothing, if you use cookies to indicate which color a user chose for a jacket, a Lambda function can change the request so that CloudFront returns the image of a jacket in the selected color.</p><p>- A Lambda function can generate HTTP responses when CloudFront viewer request or origin request events occur.</p><p>- A function can inspect headers or authorization tokens, and insert a header to control access to your content before CloudFront forwards the request to your origin.</p><p>- A Lambda function can also make network calls to external resources to confirm user credentials, or fetch additional content to customize a response.</p><p>You can configure CloudFront to cache objects based on values in the User-Agent header, but AWS doesn't recommend it. The User-Agent header has many possible values, and caching based on those values would cause CloudFront to forward significantly more requests to your origin. If you do not configure CloudFront to cache objects based on values in the <code>User-Agent</code> header, CloudFront adds a <code>User-Agent</code> header with the following value before it forwards a request to your origin: <code>User-Agent = Amazon CloudFront</code>.</p><p>Therefore, the correct answer is: <strong>Create an Amazon S3 bucket to host the static contents. Set this bucket as the origin for an Amazon CloudFront distribution. Write a Lambda@Edge function to parse the </strong><code><strong>User-Agent HTTP</strong></code><strong> header and serve the appropriate contents based on the user’s device type.</strong> CloudFront will run the Lamda@Edge function for every request to serve the appropriate content to the user. This will lessen the load on the EC2 instances of the Auto Scaling group.</p><p>The option that says: <strong>Use a Network Load Balancer (NLB) instead of an ALB to distribute the user traffic. Create a dedicated Auto Scaling group for the different device types. Configure the NLB to parse the </strong><code><strong>User-Agent HTTP</strong></code><strong> header to route the users to the appropriate EC2 Auto Scaling groups</strong> is incorrect. An NLB operates at Layer 4 of the OSI network model so it can't read the <code>User-Agent HTTP</code> header which is present at Layer 7.</p><p>The option that says: <strong>Create a dedicated Auto Scaling group for the different device types and create separate Application Load Balancers (ALB) for each group. Create an Amazon Route 53 entry to route the users to the appropriate ALB depending on their </strong><code><strong>User-Agent HTTP</strong></code><strong> header</strong> is incorrect. This is not possible because Amazon Route 53 does not have a way to direct users based on HTTP headers.</p><p>The option that says: <strong>Create an Amazon S3 bucket to host the static contents. Set this bucket as the origin for an Amazon CloudFront distribution. Configure CloudFront to deliver different contents depending on the user’s </strong><code><strong>User-Agent HTTP</strong></code><strong> header</strong> is incorrect. This may be possible as you can configure CloudFront to cache objects based on values in the Date and User-Agent headers, but AWS doesn't recommend it. These headers have many possible values, and caching based on their values would cause CloudFront to forward significantly more requests to your origin.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorCustomOrigin.html#request-custom-user-agent-header\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorCustomOrigin.html#request-custom-user-agent-header</a></p><p><br></p><p><strong>Check out these AWS Lambda and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "A health insurance company has recently adopted a hybrid cloud architecture which connects their on-premises network and their cloud infrastructure in AWS. They have an ELB which has a set of EC2 instances behind them. As the cloud engineer of the company, your manager instructed you to ensure that the SSL key used to encrypt data is always kept secure at all times. In addition, the application logs should only be decrypted by a handful of key users. \n\nIn this scenario, which of the following meets all of the requirements?",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Use the ELB to distribute traffic to a set of EC2 instances.</p><p>\n2. Upload the private key to the EC2 instances and configure it to offload the SSL traffic.</p><p>\n3. Persist your application server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Use the ELB to distribute traffic to a set of EC2 instances.</p><p>\n2. Configure the ELB to perform TCP load balancing.</p><p>\n3. Use an AWS CloudHSM instance to perform the SSL transactions.</p><p>\n4. Persist your application server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Use the ELB to distribute traffic to a set of EC2 instances.</p><p>\n2. Use TCP load balancing on the ELB and configure your EC2 instances to retrieve the private key from a non-public S3 bucket on boot.</p><p>\n3. Persist your application server logs to a private S3 bucket using SSE.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Use the ELB to distribute traffic to a set of EC2 instances.</p><p>\n2. Configure the ELB to perform TCP load balancing.</p><p>\n3. Use an AWS CloudHSM instance to perform the SSL transactions.</p><p>\n4. Persist your application server logs to a private S3 bucket using SSE.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudHSM</strong> is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud. With CloudHSM, you can manage your own encryption keys using FIPS 140-2 Level 3 validated HSMs. CloudHSM offers you the flexibility to integrate with your applications using industry-standard APIs, such as PKCS#11, Java Cryptography Extensions (JCE), and Microsoft CryptoNG (CNG) libraries.</p><p>You can use <strong>AWS CloudHSM to offload SSL/TLS</strong> processing for your web servers. Using CloudHSM for this processing reduces the burden on your web server and provides extra security by storing your web server's private key in CloudHSM. Secure Sockets Layer (SSL) and Transport Layer Security (TLS) are used to confirm the identity of web servers and establish secure HTTPS connections over the Internet.</p><p><img src=\"https://media.tutorialsdojo.com/sap_hsm_ssl_offload.png\"></p><p>AWS CloudHSM automates time-consuming HSM administrative tasks for you, such as hardware provisioning, software patching, high availability, and backups. You can scale your HSM capacity quickly by adding and removing HSMs from your cluster on-demand. AWS CloudHSM automatically load balances requests and securely duplicates keys stored in any HSM to all of the other HSMs in the cluster.</p><p>CloudHSM provides a better and more secure way of offloading the SSL processing for the web servers and ensures the application logs are durably and securely stored.</p><p>In this scenario, the following option is the best choice because it uses CloudHSM and the application server logs are persisted in an S3 bucket with a Server Side Encryption (SSE):</p><p><strong>1. Use the ELB to distribute traffic to a set of EC2 instances. </strong><br> <strong>2. Configure the ELB to perform TCP load balancing. </strong><br> <strong>3. Use an AWS CloudHSM instance to perform the SSL transactions. </strong><br> <strong>4. Persist your application server logs to a private S3 bucket using SSE.</strong></p><p>The following sets of options are incorrect because the ephemeral volume is just temporary storage and hence, not a suitable option for durable storage:</p><p><strong>1. Use the ELB to distribute traffic to a set of EC2 instances. </strong><br> <strong>2. Upload the private key to the EC2 instances and configure it to offload the SSL traffic. </strong><br> <strong>3. Persist your application server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.</strong></p><p>as well as this option:</p><p><strong>1. Use the ELB to distribute traffic to a set of EC2 instances. </strong><br> <strong>2. Configure the ELB to perform TCP load balancing. </strong><br> <strong>3. Use an AWS CloudHSM instance to perform the SSL transactions. </strong><br> <strong>4. Persist your application server logs to an ephemeral volume that has been encrypted using a randomly generated AES key.</strong></p><p>The following option is incorrect because you should never store sensitive private keys in S3:</p><p><strong>1. Use the ELB to distribute traffic to a set of EC2 instances. </strong><br> <strong>2. Use TCP load balancing on the ELB and configure your EC2 instances to retrieve the private key from a non-public S3 bucket on boot. </strong><br> <strong>3. Persist your application server logs to a private S3 bucket using SSE.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudhsm/\">https://aws.amazon.com/cloudhsm/</a></p><p><a href=\"https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload-overview.html\">https://docs.aws.amazon.com/cloudhsm/latest/userguide/ssl-offload-overview.html</a></p></div>"
	},
	{
		"question": "<p>A leading call center company has its headquarters in Seattle. Its corporate web portal is deployed to AWS. The AWS cloud resources are linked to its corporate data center via a link aggregation group (LAG), which terminates at the same AWS Direct Connect endpoint and is connected on a private virtual interface (VIF) in your VPC. The portal must authenticate against their on-premises LDAP server. Each Amazon S3 bucket can only be accessed by a logged-in user if it belongs to that user.</p><p>Which of the following options should the solutions architect implement in AWS to meet the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "The application first authenticates against LDAP, and then uses the LDAP credentials to log in to IAM service. Finally, it can now use the IAM temporary credentials to access the appropriate S3 bucket. "
			},
			{
				"correct": true,
				"answer": "<p>The application first authenticates against LDAP to retrieve the name of an IAM role associated with the user. It then assumes that role via a call to IAM Security Token Service (STS). Afterward, the application can now use the temporary credentials from the role to access the appropriate S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "Create an identity broker that assumes an IAM role, and retrieve temporary AWS security credentials via IAM Security Token Service (STS). The application gets the AWS temporary security credentials from the identity broker to gain access to the appropriate S3 bucket. "
			},
			{
				"correct": true,
				"answer": "<p>Authenticate against LDAP using an identity broker you created, and have it call IAM Security Token Service (STS) to retrieve IAM federated user credentials. The application then gets the IAM federated user credentials from the identity broker to access the appropriate S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a Direct Connect Gateway instead of a single Direct Connect connection. Set up a Transit VPC which will authenticate against their on-premises LDAP server.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Lightweight Directory Access Protocol (LDAP)</strong> is a standard communications protocol used to read and write data to and from Active Directory. You can manage your user identities in an external system outside of AWS and grant users who sign in from those systems access to perform AWS tasks and access your AWS resources. The distinction is where the external system resides—in your data center or an external third party on the web.</p><p>For enterprise <strong>identity federation</strong>, you can authenticate users in your organization's network, and then provide those users access to AWS without creating new AWS identities for them and requiring them to sign in with a separate user name and password. This is known as the single sign-on (SSO) approach to temporary access. AWS STS supports open standards like Security Assertion Markup Language (SAML) 2.0, with which you can use Microsoft AD FS to leverage your Microsoft Active Directory.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap_broker.png\"></p><p>This scenario has the following attributes:</p><p>- The identity broker application has permissions to access IAM's token service (STS) API to create temporary security credentials.</p><p>- The identity broker application is able to verify that employees are authenticated within the existing authentication system.</p><p>- Users are able to get a temporary URL that gives them access to the AWS Management Console (which is referred to as single sign-on).</p><p>The option that says: <strong>Authenticate against LDAP using an identity broker you created, and have it call IAM Security Token Service (STS) to retrieve IAM federated user credentials. The application then gets the IAM federated user credentials from the identity broker to access the appropriate S3 bucket</strong> is correct because it follows the correct sequence. It develops an identity broker that authenticates users against LDAP, gets the security token from STS, and then accesses the S3 bucket using the IAM federated user credentials.</p><p>Likewise, the option that says:<strong> The application first authenticates against LDAP to retrieve the name of an IAM role associated with the user. It then assumes that role via call to IAM Security Token Service (STS). Afterwards, the application can now use the temporary credentials from the role to access the appropriate S3 bucket</strong> is correct because it follows the correct sequence. It authenticates users using LDAP, gets the security token from STS, and then accesses the S3 bucket using the temporary credentials.</p><p>The option that says: <strong>Create an identity broker that assumes an IAM role, and retrieve temporary AWS security credentials via IAM Security Token Service (STS). The application gets the AWS temporary security credentials from the identity broker to gain access to the appropriate S3 bucket</strong> is incorrect because the users need to be authenticated using LDAP first, not STS. Also, the temporary credentials to log into AWS are provided by STS, not identity broker.</p><p>The option that says: <strong>The application first authenticates against LDAP, and then uses the LDAP credentials to log in to IAM service. Finally, it can now use the IAM temporary credentials to access the appropriate S3 bucket</strong> is incorrect because you cannot use the LDAP credentials to log into IAM.</p><p>The option that says: <strong>Use a Direct Connect Gateway instead of a single Direct Connect connection. Set up a Transit VPC which will authenticate against their on-premises LDAP server</strong> is incorrect because using a Direct Connect Gateway will only improve the availability of your on-premises network connection and using a transit VPC is just a common strategy for connecting multiple, geographically disperse VPCs and remote networks in order to create a global network transit center. These two things will not meet the requirement.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company is planning to launch a mobile app for the Department of Transportation that allows government staff to upload the latest photos of ongoing construction works such as bridges, roads culverts, and dams all over the country. The mobile app should send the photos to a web server hosted on an EC2 instance which then adds a watermark to each photo that contains the project details and the date it was taken. The solutions architect must design a solution in which the photos generated by the server will be uploaded to an S3 bucket for durable storage.</p><p>Which of the following solutions is a secure architecture and allows the EC2 instance to upload photos to S3?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Set up an IAM service role with permissions to list and write objects to the S3 bucket. Attach the IAM role to the EC2 instance which will enable it to retrieve temporary security credentials from the instance userdata and use that access to upload the photos to the S3 bucket."
			},
			{
				"correct": false,
				"answer": "Set up an IAM user with permissions to list and write objects to the S3 bucket. Launch the instance as the IAM user which will enable the EC2 instance to retrieve temporary security credentials from the instance userdata and use that access to upload the photos to the S3 bucket."
			},
			{
				"correct": false,
				"answer": "<p>Set up a service control policy (SCP) with permissions to list and write objects to the S3 bucket. Attach the SCP to the EC2 instance which will enable it to retrieve temporary security credentials from the instance metadata and use that access to upload the photos to the S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up an IAM role with permissions to list and write objects to the S3 bucket. Attach the IAM role to the EC2 instance which will enable it to retrieve temporary security credentials from the instance metadata and use that access to upload the photos to the S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>This question tests your understanding of IAM, specifically on when to use an IAM Role over an SCP. Since the server is running on an EC2 instance and the application makes requests to S3 to store the photos, the more suitable option to use here is an IAM Role.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_organization.jpg\"></p><p>In addition, don't create an IAM user and pass the user's credentials to the application or embed the credentials in the application. That will create a security risk because if an attacker had unauthorized access to that EC2 instance then the user credentials can easily be acquired and exploited. The better way is to create an IAM role that you can attach to the EC2 instance to give applications running on the instance temporary security credentials which can be used to access other AWS resources such as an S3 bucket. The credentials have the permissions specified in the policies attached to the role.</p><p>The option that says: <strong>Set up an IAM role with permissions to list and write objects to the S3 bucket. Attach the IAM role to the EC2 instance which will enable it to retrieve temporary security credentials from the instance metadata and use that access to upload the photos to the S3 bucket</strong> is correct as it uses an IAM Role and fetches the temporary security credentials from the instance metadata.</p><p>The option that says: <strong>Set up a service control policy (SCP) with permissions to list and write objects to the S3 bucket. Attach the SCP to the EC2 instance which will enable it to retrieve temporary security credentials from the instance metadata and use that access to upload the photos to the S3 bucket</strong> is incorrect as SCPs simply enable you to restrict, at the account level of granularity, what services and actions the users, groups, and roles in those accounts can do. SCPs don't grant permissions to any user or role because this is handled through IAM policies.</p><p>The option that says: <strong>Set up an IAM user with permissions to list and write objects to the S3 bucket. Launch the instance as the IAM user which will enable the EC2 instance to retrieve temporary security credentials from the instance userdata and use that access to upload the photos to the S3 bucket</strong> is incorrect as an IAM Role is a better option to use instead of an IAM User. Plus, you should always retrieve the temporary security credentials from the instance metadata and not from the user data.</p><p>The option that says: <strong>Set up an IAM service role with permissions to list and write objects to the S3 bucket. Attach the IAM role to the EC2 instance which will enable it to retrieve temporary security credentials from the instance userdata and use that access to upload the photos to the S3 bucket</strong> is incorrect because although it uses an IAM Role, the temporary security credentials should be retrieved from the instance metadata and not from the user data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-service-control-policy/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html#roles-usingrole-ec2instance-roles\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html#roles-usingrole-ec2instance-roles</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p><p><br></p><p><strong>Here is a deep dive on IAM Policies:</strong></p><p><a href=\"https://youtu.be/YQsK4MtsELU\">https://youtu.be/YQsK4MtsELU</a></p></div>"
	},
	{
		"question": "<p>A company implements best practices and mandates that all of the cloud-related deployments should not be done manually but through the use of CloudFormation. All of the CloudFormation templates should be treated as code and hence, all of them are committed in a private GIT repository. A senior solutions architect has recently left the team. One of the tasks of the junior solutions architect is to handle a distributed system in AWS, in which the architecture is declared in a CloudFormation template. The distributed system needs to be migrated to another VPC and the junior solutions architect tried to read the template to understand the AWS resources that the template will generate. While analyzing the CloudFormation template, he stumbled upon the code below.</p><p>What does this code snippet do in CloudFormation?</p><p><code><br>\"SNSTopic\" : {<br>\"Type\" : \"AWS::SNS::Topic\",<br>\"Properties\" : {<br>\"Subscription\" : [{<br>\"Protocol\" : \"sqs\",<br>\"Endpoint\" : { \"Fn::GetAtt\" : [ \"TutorialsDojoQueue\", \"Arn\" ] }<br>}]<br>}<br></code></p>",
		"answers": [
			{
				"correct": false,
				"answer": "Creates an SNS topic which allows SQS subscription endpoints."
			},
			{
				"correct": false,
				"answer": "Creates an SNS topic and then invokes the call to create an SQS queue with a logical resource name of TutorialsDojoQueue."
			},
			{
				"correct": false,
				"answer": "Creates an SNS topic which allows SQS subscription endpoints to be added as a parameter on the template."
			},
			{
				"correct": true,
				"answer": "Creates an SNS topic and then adds a subscription using the ARN attribute name for the SQS resource, which is created under the logical name TutorialsDojoQueue."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudFormation</strong> provides several built-in functions that help you manage your stacks which are called \"intrinsic functions\". Use intrinsic functions in your templates to assign values to properties that are not available until runtime.</p><p>You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes. You can also use intrinsic functions to conditionally create stack resources.</p><p>The <code><strong>Fn::GetAtt</strong></code> intrinsic function returns the value of an attribute from a resource in the template. It has 2 parameters: the <code><strong>logicalNameOfResource</strong></code><strong> </strong>and the <code><strong>attributeName</strong></code>. The logical name (also called logical ID) of the resource contains the attribute that you want to use. The <code><strong>attributeName</strong></code><strong> </strong>is the name of the resource-specific attribute whose value you want to utilize.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_steps.png\"></p><p>Therefore, the correct answer is: <strong>Create an SNS topic and then add a subscription using the ARN attribute name for the SQS resource, which is created under the logical name TutorialsDojoQueue.</strong> The code snippet creates an SNS topic and then adds a subscription using the ARN attribute name for the SQS resource, which is created under the logical name \"TutorialsDojoQueue\" using the GetAtt intrinsic function.</p><p>The following options are all incorrect because these options incorrectly described what the code snippet does:</p><p><strong>- Creates an SNS topic which allows SQS subscription endpoints to be added as a parameter on the template.<br></strong></p><p><strong>- Creates an SNS topic which allows SQS subscription endpoints.</strong></p><p><strong>- Creates an SNS topic and then invokes the call to create an SQS queue with a logical resource name of TutorialsDojoQueue.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p></div>"
	},
	{
		"question": "<p>A multinational software provider in the US hosts both of its development and test environments in the AWS cloud. The CTO decided to use separate AWS accounts in hosting each environment. The solutions architect has enabled Consolidated Billing to link each of the accounts' bill to a Master AWS account. To make sure that each account is kept within the budget, the administrators in the master account must have the power to stop, delete, and/or terminate resources in both development and test environment AWS accounts.</p><p>Which of the following options is the recommended action to meet the requirements for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "By linking all accounts under Consolidated Billing, you will be able to provide IAM users in the master account access to Dev and Test account resources."
			},
			{
				"correct": false,
				"answer": "IAM users with full admin permissions will be created in the master account. In both Dev and Test accounts, generate cross-account roles that would grant the master account access to Dev and Test account resources through permissions inherited from the master account."
			},
			{
				"correct": false,
				"answer": "In the master account, you are to create IAM users and a cross-account role that has full admin permissions to the Dev and Test accounts."
			},
			{
				"correct": true,
				"answer": "First, create IAM users in the master account. Then in the Dev and Test accounts, generate cross-account roles that have full admin permissions while granting access for the master account."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You share resources in one account with users in a different account. By setting up cross-account access in this way, you don't need to create individual IAM users in each account. In addition, users don't have to sign out of one account and sign into another in order to access resources that are in different AWS accounts.</p><p><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\"></p><p>Therefore, the correct answer is:<strong> First, create IAM users in the master account. Then in the Dev and Test accounts, generate cross-account roles that have full admin permissions while granting access for the master account. </strong>The cross-account role is created in Dev and Test accounts, and the users are created in the Master account that are given that role.</p><p>The option that says:<strong> In the master account, you are to create IAM users and a cross-account role that has full admin permissions to the Dev and Test accounts</strong> is incorrect. A cross-account role should be created in Dev and Test accounts, not Master account.</p><p>The option that says:<strong> IAM users with full admin permissions will be created in the master account. In both Dev and Test accounts, generate cross-account roles that would grant the master account access to Dev and Test account resources through permissions inherited from the master account</strong> is incorrect. The permissions cannot be inherited from one AWS account to another.</p><p>The option that says: <strong>By linking all accounts under Consolidated Billing, you will be able to provide IAM users in the master account access to Dev and Test account resources</strong> is incorrect. Consolidated billing does not give access to resources in this fashion.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A startup is running its customer support application in the AWS cloud. The application is hosted on a set of Auto Scaling Amazon EC2 on-demand instances placed behind an Elastic Load Balancer. The web application runs on large EC2 instance sizes to properly process the high volume of data that are stored in DynamoDB. New application version deployment is done once a week and requires an automated way of creating and testing a new Amazon Machine Image for the application servers. To meet the growing number of support tickets being sent, it was decided that a new video chat feature should be implemented as part of the customer support app, but should be hosted on a different set of servers to allow users to chat with a representative. The startup decided to streamline the deployment process and use AWS OpsWorks as an application lifecycle tool to simplify the management of the app and reduce time-consuming deployment cycles.</p><p>What is the most cost-efficient and flexible way to integrate the new video chat module in AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create two AWS OpsWorks stacks, each with two layers, and two custom recipes. "
			},
			{
				"correct": false,
				"answer": "Create two AWS OpsWorks stacks, each with two layers, and one custom recipe. "
			},
			{
				"correct": true,
				"answer": "Create an AWS OpsWorks stack, with two layers, and one custom recipe."
			},
			{
				"correct": false,
				"answer": "Create an AWS OpsWorks stack, with one layer, and one custom recipe."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS OpsWorks Stacks</strong> lets you manage applications and servers on AWS and on-premises. With OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server. You can deploy and configure Amazon EC2 instances in each layer or connect other resources such as Amazon RDS databases. OpsWorks Stacks lets you set automatic scaling for your servers based on preset schedules or in response to changing traffic levels, and it uses lifecycle hooks to orchestrate changes as your environment scales.</p><p>In <strong>OpsWorks</strong>, you will be provisioning a stack and layers. The stack is the top-level AWS OpsWorks Stacks entity. It represents a set of instances that you want to manage collectively, typically because they have a common purpose such as serving PHP applications. In addition to serving as a container, a stack handles tasks that apply to the group of instances as a whole, such as managing applications and cookbooks.</p><p>Every stack contains one or more layers, each of which represents a stack component, such as a load balancer or a set of application servers. As you work with AWS OpsWorks Stacks layers, keep the following in mind:</p><p>- Each layer in a stack must have at least one instance and can optionally have multiple instances.</p><p>- Each instance in a stack must be a member of at least one layer, except for <a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/registered-instances.html\">registered instances</a>. You cannot configure an instance directly, except for some basic settings such as the SSH key and hostname. You must create and configure an appropriate layer, and add the instance to the layer.</p><p>In the scenario, it tells us that the video chat feature should be implemented as part of the customer support application, but should be hosted on a different set of servers. This means that the chat feature is part of the stack, but should be in a different layer since it will be using a different set of servers. Hence, we have to use one stack and two layers to meet the requirement.</p><p><img src=\"https://media.tutorialsdojo.com/sap_opsworks_layers.png\"></p><p>Therefore, the correct answer is: <strong>Create an AWS OpsWorks stack with two layers and one custom recipe.</strong> Only one stack would be sufficient and two layers would be required for handling separate requirements. One custom recipe for DynamoDB would be required.</p><p>These options are incorrect because two OpsWorks stacks are unnecessary since the new video chat feature is still a part of the customer support website but just deployed on a different set of servers. Hence, this should be deployed on a different layer and not on an entirely different stack.</p><p><strong>- Create two AWS OpsWorks stacks, each with two layers, and two custom recipes</strong></p><p><strong>- Create two AWS OpsWorks stacks, each with two layers, and one custom recipe</strong></p><p>The option that says: <strong>Create an AWS OpsWorks stack with one layer and one custom recipe</strong> is incorrect. It would be a better solution to create two separate layers: one customer support web servers and one for the video chat feature.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/opsworks/stacks/faqs\">https://aws.amazon.com/opsworks/stacks/faqs</a></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workinglayers.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workinglayers.html</a></p><p><br></p><p><strong>Check out this AWS OpsWorks Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-opsworks/?src=udemy\">https://tutorialsdojo.com/aws-opsworks/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company has three AWS accounts each with its own VPCs. There is a requirement for communication between the AWS resources across the accounts, so VPC peering needs to be configured. Please refer to the figure below for details of each VPC:</p><img src=\"https://img-c.udemycdn.com/redactor/raw/quiz_question/2021-04-03_10-22-05-5435a4d04690c149c2d275367420960e.png\" style=\"display: none;\"><span class=\"ud-component--base-components--open-full-size-image\"><div class=\"open-full-size-image--wrapper--2kvh_\" data-purpose=\"open-full-size-image\"><img src=\"https://img-c.udemycdn.com/redactor/raw/quiz_question/2021-04-03_10-22-05-5435a4d04690c149c2d275367420960e.png\" alt=\"\"><button type=\"button\" class=\"udlite-btn udlite-btn-large udlite-btn-link udlite-heading-md open-full-size-image--backdrop--20cbM\"><svg aria-label=\"Larger image\" role=\"img\" focusable=\"false\" class=\"udlite-icon udlite-icon-large\"><use xlink:href=\"#icon-search\"></use></svg></button></div></span><p>VPC-B and VPC-C have matching CIDR blocks. For a short-term requirement, VPC-A needs to communicate only with the database instance in VPC-B with an IP address of <code>10.0.0.77/32</code> while being able to communicate with all the resources in VPC-C. The Solutions Architect already created the necessary VPC peering links but VPC-A cannot effectively communicate to the VPC-B instance. The Solutions Architect suspects that the routes on each VPC still need proper configuration.</p><p>Which of the following solutions will allow VPC-A to communicate with the database instance in VPC-B while being able to communicate with all resources on VPC-C?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>On VPC-A, add a static route for VPC-B CIDR (<code>10.0.0.77/32</code>) with the target <code>pcx-aaaabbbb</code> and another static route for VPC-C CIDR (<code>10.0.0.0/16</code>) with the target <code>pcx-aaaacccc</code>. On VPC-B, add a static route for VPC-A CIDR (<code>172.16.0.0/24</code>) with the target <code>pcx-aaaabbbb</code>. On VPC-C, add a static route for VPC-A CIDR (<code>172.16.0.0/24</code>) with the target <code>pcx-aaaacccc</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>On VPC-A, add a static route for VPC-B CIDR (<code>10.0.0.0/24</code>) with the target <code>pcx-aaaabbbb</code> and another static route for VPC-C CIDR (<code>10.0.0.0/16</code>) with the target <code>pcx-aaaacccc</code>. On VPC-B, add a static route for VPC-A CIDR (<code>172.16.0.0/24</code>) with the target <code>pcx-aaaabbbb</code>. On VPC-C, add a static route for VPC-A CIDR (<code>172.16.0.0/24</code>) with the target <code>pcx-aaaacccc</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Enable dynamic route propagation in VPC-A with the peering targets <code>pcx-aaaabbbb</code> and <code>pcx-aaaacccc</code> respectively. On VPC-B, enable dynamic route propagation with peering target <code>pcx-aaaabbbb</code> and add a network access control list (NACL) that allows only connections to IP address <code>10.0.0.77/32</code> from <code>pcx-aaaabbbb</code>. On VPC-C, enable dynamic route propagation with the peering target <code>pcx-aaaacccc</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>On VPC-A, add a static route for VPC-B CIDR (<code>10.0.0.0/24</code>) with the target <code>pcx-aaaabbbb</code> and another static route for VPC-C CIDR (<code>10.0.0.0/24</code>) with the target <code>pcx-aaaacccc</code>. Add a network access control list (NACL) on VPC-A to deny all connections to VPC-B except for the IP address <code>10.0.0.77/32</code>. On VPC-B, add a static route for VPC-A CIDR (172.16.0.0/24) with the target <code>pcx-aaaabbbb</code>. On VPC-C, add a static route for VPC-A CIDR (<code>172.16.0.0/24</code>) with the target <code>pcx-aaaacccc</code>.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can configure <strong>VPC peering connections</strong> to provide access to part of the CIDR block, a specific CIDR block (if the VPC has multiple CIDR blocks), or a specific instance within the peer VPC. In this scenario, a central VPC is peered to two or more VPCs that have overlapping CIDR blocks.</p><p>You have a central VPC (VPC A) with one subnet, and you have a VPC peering connection between VPC A and VPC B (<code>pcx-aaaabbbb</code>), and between VPC A and VPC C (<code>pcx-aaaacccc</code>). VPC B and VPC C have matching CIDR blocks. You want to use VPC peering connection <code>pcx-aaaabbbb</code> to route traffic between VPC A and specific instance in VPC B. All other traffic destined for the <code>10.0.0.0/16</code> IP address range is routed through <code>pcx-aaaacccc</code> between VPC A and VPC C.</p><p>VPC route tables use longest prefix match to select the most specific route across the intended VPC peering connection. All other traffic is routed through the next matching route, in this case, across the VPC peering connection <code>pcx-aaaacccc</code>.</p><p><img src=\"https://media.tutorialsdojo.com/sap_vpc_peering_cidr_example.png\"></p><p>If you have a VPC peered with multiple VPCs that have overlapping or matching CIDR blocks, ensure that your route tables are configured to avoid sending response traffic from your VPC to the incorrect VPC. AWS currently does not support unicast reverse path forwarding in VPC peering connections that checks the source IP of packets and routes reply packets back to the source. You still need to configure static routes on VPC-B and VPC-C going to VPC-A, respectively.</p><p>Therefore, the correct answer is: <strong>On VPC-A, add a static route for VPC-B CIDR (</strong><code><strong>10.0.0.77/32</strong></code><strong>) with the target </strong><code><strong>pcx-aaaabbbb</strong></code><strong> and another static route for VPC-C CIDR (</strong><code><strong>10.0.0.0/16</strong></code><strong>) with the target</strong><code><strong> pcx-aaaacccc</strong></code><strong>. On VPC-B, add a static route for VPC-A CIDR (</strong><code><strong>172.16.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaabbbb</strong></code><strong>. On VPC-C, add a static route for VPC-A CIDR (</strong><code><strong>172.16.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaacccc</strong></code><strong>.</strong> The standard VPC peering configuration will be done for VPC-A and VPC-C. As for VPC-B, only the static route to the specific should be configured on VPC-A. AWS will handle the longest prefix match to route the traffic.</p><p>The option that says: <strong>On VPC-A, add a static route for VPC-B CIDR (</strong><code><strong>10.0.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaabbbb</strong></code><strong> and another static route for VPC-C CIDR (</strong><code><strong>10.0.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaacccc</strong></code><strong>. Add a network access control list (NACL) on VPC-A to deny all connections to VPC-B except for the IP address </strong><code><strong>10.0.0.77/32</strong></code><strong>. On VPC-B, add a static route for VPC-A CIDR (</strong><code><strong>172.16.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaabbbb</strong></code><strong>. On VPC-C, add a static route for VPC-A CIDR (</strong><code><strong>172.16.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaacccc</strong></code> is incorrect. This will result in a conflict on the routing configuration on VPC-A. Network ACLs can block connections going out of your VPC, however, they can't redirect connections out to specific targets.</p><p>The option that says: <strong>Enable dynamic route propagation in VPC-A with the peering targets </strong><code><strong>pcx-aaaabbbb</strong></code><strong> and </strong><code><strong>pcx-aaaacccc</strong></code><strong> respectively. On VPC-B, enable dynamic route propagation with peering target </strong><code><strong>pcx-aaaabbbb</strong></code><strong> and add a network acces control list (NACL) that allows only connections to IP address </strong><code><strong>10.0.0.77/32</strong></code><strong> from </strong><code><strong>pcx-aaaabbbb</strong></code><strong>. On VPC-C, enable dynamic route propagation with the peering target </strong><code><strong>pcx-aaaacccc</strong></code> is incorrect. Dynamic route propagation is usually used for Direct Connection connections or Site-to-Site VPNs. In this scenario, you want to force a specific route to a specific instance on a specific peering target, thus, you need to configure static routes.</p><p>The option that says: <strong>On VPC-A, add a static route for VPC-B CIDR (</strong><code><strong>10.0.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaabbbb</strong></code><strong> and another static route for VPC-C CIDR (</strong><code><strong>10.0.0.0/16</strong></code><strong>) with the target </strong><code><strong>pcx-aaaacccc</strong></code><strong>. On VPC-B, add a static route for VPC-A CIDR (</strong><code><strong>172.16.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaabbbb</strong></code><strong>. On VPC-C, add a static route for VPC-A CIDR (</strong><code><strong>172.16.0.0/24</strong></code><strong>) with the target </strong><code><strong>pcx-aaaacccc</strong></code> is incorrect. Route configuration for VPC-A and VPC-C is correct here, however, the route configuration of VPC-A and VPC-B is not. This will make traffic to other instances in the CIDR (<code>10.0.0.0/24</code>), which is under VPC-C, to be routed to VPC-B.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/peering-configurations-partial-access.html#one-to-two-vpcs-lpm\">https://docs.aws.amazon.com/vpc/latest/peering/peering-configurations-partial-access.html#one-to-two-vpcs-lpm</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html\">https://docs.aws.amazon.com/vpc/latest/userguide/route-table-options.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-routing.html\">https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-routing.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A global enterprise web application is using a private S3 bucket, named MANILATECH-CONFIG, which has Server-Side Encryption with Amazon S3-Managed Encryption Keys (SSE-S3) to store its configuration files for different regions in North America, Latin America, Europe, and Asia. There has been a lot of database changes and feature toggle switching for the past few weeks. Your CTO assigned you the task of enabling versioning on this bucket to track any changes made to the configuration files and have the ability to use the old settings if needed. In the coming days ahead, a new region in Oceania will be supported by the web application and thus, a new configuration file will be added soon. Currently, there are already four files in the bucket, namely: MNL-NA.config, MNL-LA.config, MNL-EUR.config, and MNL-ASIA.config which are updated regularly. As instructed, you enabled the versioning in the bucket and after a few days, the new MNL-O.config configuration file for the Oceania region has been uploaded. A week after, a configuration has been done on MNL-NA.config, MNL-LA.config, and MNL-O.config files.</p><p>In this scenario, which of the following is correct about files inside the MANILATECH-CONFIG S3 bucket? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>There would be two available versions for each of the MNL-NA.config, MNL-LA.config, and MNL-O.config files. The first Version ID of MNL-NA.config and MNL-LA.config has a value of null.</p>"
			},
			{
				"correct": false,
				"answer": "The MNL-EUR.config and MNL-ASIA.config files will have a Version ID of 1. "
			},
			{
				"correct": false,
				"answer": "The latest Version ID of MNL-NA.config and MNL-LA.config has a value of null. "
			},
			{
				"correct": false,
				"answer": "<p>The first Version ID of MNL-NA.config and MNL-LA.config has a value of 1.</p>"
			},
			{
				"correct": true,
				"answer": "The MNL-EUR.config and MNL-ASIA.config files will have a Version ID of null."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Versioning</strong> is a means of keeping multiple variants of an object in the same bucket. You can use versioning to preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket. With versioning, you can easily recover from both unintended user actions and application failures.</p><p>In this scenario, we have an initial 4 files in the MANILATECH-CONFIG bucket: MNL-NA.config, MNL-LA.config, MNL-EUR.config, and MNL-ASIA.config. Then, the Versioning feature was enabled which caused all of the 4 existing files to have a Version ID of null. This new configuration will enable the new files that will be added to have an alphanumeric VERSION ID, as well as any new updates for the first 4 files. Hence, when a new MNL-O.config configuration file was added, its Version ID was an alphanumeric key since this file was uploaded after the Versioning feature was enabled.</p><p>A week after, a new update has been done on the 3 configuration files only (MNL-NA.config, MNL-LA.config, and MNL-O.config files). Take note that at this point, there are NO changes made on the MNL-EUR.config and MNL-ASIA.config files, which is why their first (and latest) version ID will still remain as <em>null</em> since there were no new updates made yet.</p><p>However, for MNL-NA.config and MNL-LA.config, it has the first Version ID of null and then the second Version ID would be an alphanumeric key. For the MNL-O.config file, the first Version ID is already an alphanumeric key since this file was created after the Versioning was enabled.</p><p>Therefore, the correct answers are:</p><p><strong>- There would be two available versions for each of the MNL-NA.config, MNL-LA.config, and MNL-O.config files. The first Version ID of MNL-NA.config and MNL-LA.config has a value of null.</strong></p><p><strong>- The MNL-EUR.config and MNL-ASIA.config files will have a Version ID of null</strong></p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_versioning.png\"></p><p>The option that says: <strong>The first Version ID of MNL-NA.config and MNL-LA.config has a value of 1</strong> is incorrect. The first VERSION ID of these files would be null since they were already existing when the S3 Versioning was enabled.</p><p>The option that says: <strong>The MNL-EUR.config and MNL-ASIA.config files will have a Version ID of 1</strong> is incorrect because the Version ID for these files is null.</p><p>The option that says: <strong>The latest Version ID of MNL-NA.config and MNL-LA.config has a value of null</strong> is incorrect because the latest VERSION ID value for these 2 files would be an alphanumeric value and not null.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/versioning-workflows.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>An enterprise plans to create a new cloud deployment that will be used by several project teams. The network must be designed so that it allows autonomy for the administrators of the individual AWS accounts to modify their route tables freely. However, the company wants to monitor outbound traffic so it is required to have a centralized and controlled egress Internet connection for all accounts. As more teams are expected to join this deployment, the organization is expected to grow into thousands of AWS accounts.</p><p>Which of the following options should the Solutions Architect implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create a shared transit gateway. Have each spoke VPC connect to the transit gateway. Use a fleet of firewalls, each with a VPN attachment to the transit gateway, to route the outbound Internet traffic.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a centralized shared VPC. On this VPC, create a subnet that will be associated with each AWS account. Use a fleet of proxy servers to control the outbound Internet traffic.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a shared services VPC. On this VPC, host the central assets which include a fleet of firewalls that have a route to the public Internet. Have each spoke VPC connect to the central VPC using VPC peering.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a centralized transit VPC. Have the VPCs on each AWS account connect to the transit VPC using a VPN connection. Control the outbound Internet traffic using firewall appliances.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Transit Gateway</strong> is a highly available and scalable service used to consolidate the AWS VPC routing configuration for a region with a hub-and-spoke architecture. Each spoke VPC only needs to connect to the Transit Gateway to gain access to other connected VPCs. Transit Gateway across different regions can peer with each other to enable VPC communications across regions. With a large number of VPCs, Transit Gateway provides simpler VPC-to-VPC communication management over VPC Peering.</p><p><strong>Transit Gateway</strong> enables customers to connect thousands of VPCs. You can attach all your hybrid connectivity (VPN and Direct Connect connections) to a single Transit Gateway— consolidating and controlling your organization's entire AWS routing configuration in one place. Transit Gateway controls how traffic is routed among all the connected spoke networks using route tables. This hub and spoke model simplifies management and reduces operational costs because VPCs only connect to the Transit Gateway to gain access to the connected networks.</p><p>Transit Gateway is a Regional resource and can connect thousands of VPCs within the same AWS Region. You can create multiple Transit Gateways per Region, but Transit Gateways within an AWS Region cannot be peered, and you can connect to a maximum of three Transit Gateways over a single Direct Connect Connection for hybrid connectivity.</p><p>If the vendor you choose for egress traffic inspection doesn’t support automation for failure detection, or if you need horizontal scaling, you can use an alternative design. In this design, we don’t create a VPC attachment on the transit gateway for egress VPC, instead, we create an IPsec VPN attachment and create an IPsec VPN from Transit Gateway to the EC2 instances leveraging BGP to exchanges routes.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ipsec_vpn_bgp.png\"></p><p>Deploying a NAT Gateway in every spoke VPC can become expensive because you pay an hourly charge for every NAT Gateway you deploy, so centralizing it could be a viable option. To centralize, you create an egress VPC in the network services account and route all egress traffic from the spoke VPCs via a NAT Gateway sitting in this VPC leveraging Transit Gateway, as shown below.</p><p><img src=\"https://media.tutorialsdojo.com/sap_transit_gateway.png\"></p><p>When you centralize NAT Gateway using Transit Gateway, you pay an extra Transit Gateway data processing charge — compared to the decentralized approach of running a NAT Gateway in every VPC. A transit gateway allows you to route all egress traffic from the spoke VPCs to a central NAT Gateway. It can handle up to thousands of attached spoke VPCs.</p><p>Therefore, the correct answer is: <strong>Create a shared transit gateway. Have each spoke VPC connect to the transit gateway. Use a fleet of firewalls, each with a VPN attachment to the transit gateway, to route the outbound Internet traffic.</strong></p><p>The option that says: <strong>Create a centralized transit VPC. Have the VPCs on each AWS account connect to the transit VPC using a VPN connection. Control the outbound Internet traffic using firewall appliances</strong> is incorrect. Using a VPN connection for connecting within Amazon VPCs or with the transit is not needed. You can use the AWS network backbone which is much faster than setting up a VPN connection.</p><p>The option that says: <strong>Create a centralized shared VPC. On this VPC, create a subnet that will be associated with each AWS account. Use a fleet of proxy servers to control the outbound Internet traffic</strong> is incorrect. The default limit for shared VPC subnets is 100. Additionally, on this setup, the participants on the shared subnets will not be able to modify their own route tables.</p><p>The option that says: <strong>Create a shared services VPC. On this VPC, host the central assets which include a fleet of firewalls that have a route to the public Internet. Have each spoke VPC connect to the central VPC using VPC peering</strong> is incorrect. There is a default limit of 50 VPC peering for each VPC. This is not enough to handle peering for thousands of AWS accounts.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/centralized-egress-to-internet.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/centralized-egress-to-internet.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html\">https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-transit-gateway.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-transit-gateway.html</a></p><p><br></p><p><strong>Check out these AWS Transit Gateway and Amazon VPC Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company is building a new cryptocurrency trading platform that will be hosted on the AWS cloud. The solutions architect needs to set up the designed architecture in a single VPC. The solution should mitigate distributed denial-of-service (DDoS) attacks to secure the company’s applications and systems. The solution should also include a notification for incoming Layer 3 or Layer 4 attacks such as SYN floods and UDP reflection attacks. The system should also be protected against SQL injection, cross-site scripting, and other Layer 7 attacks.</p><p>Which of the following solutions should the solutions architect implement together to meet the above requirement? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up rule-based filtering using the AWS Network Firewall service.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Shield Standard that mitigates DDoS attacks including SYN floods and UDP reflection attacks.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS WAF to define customizable web security rules that control which traffic can access your web applications.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Place your servers behind a CloudFront web distribution and improve your cache hit ratio.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Shield Advanced that provides enhanced DDoS attack detection and monitoring for application-layer traffic to your AWS resources.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>Distributed Denial of Service (DDoS)</strong> attack is a malicious attempt to make a targeted system, such as a website or application, unavailable to end-users. To achieve this, attackers use a variety of techniques that consume network or other resources, interrupting access for legitimate end-users.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_shield.png\"></p><p>AWS provides flexible infrastructure and services that help customers implement strong DDoS mitigations and create highly available application architectures that follow AWS Best Practices for DDoS Resiliency. These include services such as Amazon Route 53, Amazon CloudFront, Elastic Load Balancing, and AWS WAF to control and absorb traffic, and deflect unwanted requests. These services integrate with <strong>AWS Shield</strong>, a managed DDoS protection service, that provides always-on detection and automatic inline mitigations to safeguard web applications running on AWS.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF gives you control over which traffic to allow or block to your web applications by defining customizable web security rules. You can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond quickly to changing traffic patterns. Also, AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of web security rules.</p><p>In this scenario, AWS Shield Advanced and AWS WAF are the two services that can provide optimal DDoS attack mitigation and protection against Layer 7 security risks to your cloud infrastructure.</p><p>Therefore the correct answers are:</p><p><strong>- Use AWS Shield Advanced that provides enhanced DDoS attack detection and monitoring for application-layer traffic to your AWS resources.</strong></p><p><strong>- Use AWS WAF to define customizable web security rules that control which traffic can access your web applications</strong>.</p><p>The option that says: <strong>Use AWS Shield Standard that mitigates DDoS attacks including SYN floods and UDP reflection attacks</strong> is incorrect. Although AWS Shield Standard can mitigate Layer 3 or Layer 4 attacks, it does not include a detailed notification of the recent layer attacks to your AWS resources such as SYN floods and UDP reflection attacks. You should upgrade this to AWS Shield Advanced in order to meet the requirement, which also includes an AWS DDoS Response Team (DRT) that supports experts who apply manual mitigations for more complex and sophisticated DDoS attacks.</p><p>The option that says: <strong>Place your servers behind a CloudFront web distribution and improve your cache hit ratio</strong> is incorrect. Although CloudFront can help mitigate DDoS attacks, improving the cache hit ratio of your CloudFront distribution is still not enough to totally protect your infrastructure. This option also fails to mention the geoblocking and HTTPS protocol support features of CloudFront. Using AWS Shield Advanced and AWS WAF will provide more effective protection against DDoS.</p><p>The option that says:<strong> Set up rule-based filtering using the AWS Network Firewall service</strong> is incorrect. AWS Network Firewall is primarily used to manage multiple firewall rules across hundreds of Amazon VPCs and AWS Accounts that are usually under a single AWS Organization. It is explicitly mentioned in the scenario that the company is using its sole AWS account and the solution is only running on a single Amazon VPC. Hence, using the AWS Network Firewall service is not suitable for this case.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/\">https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\">https://d0.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p><p><a href=\"https://aws.amazon.com/network-firewall\">https://aws.amazon.com/network-firewall</a></p><p><br></p><p><strong>Check out these AWS WAF and AWS Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/aws-shield/</a></p></div>"
	},
	{
		"question": "<p>The AWS resources in your production account are shared among various business units of the company. A single business unit may have one or more AWS accounts that have resources in the production account. There were a lot of incidents in which the developers from a specific business unit accidentally terminated the EC2 instances owned by another business unit. You are tasked to come up with a solution to only allow a specific business unit that owns the EC2 instances, and other AWS resources, to terminate their own resources.</p><p>Which of the following is the most suitable multi-account strategy that you should implement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Create an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Provide the cross-account access and the IAM policy to every member accounts of the OU.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to an individual Organization Unit (OU). Create a Service Control Policy in the production account for each business unit which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances that it owns. Provide the cross-account access and the SCP to the individual member accounts to tightly control who can terminate the EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to an individual Organization Unit (OU). Create an IAM Role in the production account for each business unit which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances that it owns. Create an <code>AWSServiceRoleForOrganizations</code> service-linked role for the individual member accounts of the OU to enable trusted access.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to an individual Organization Unit (OU). Create a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Provide the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an <em>organization</em> that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p><img src=\"https://d2908q01vomqb2.cloudfront.net/c5b76da3e608d34edb07244cd9b875ee86906328/2020/06/16/telecom-mulit-account-1-1024x585.jpg\"></p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs. Each OU can contain multiple accounts, and you can move accounts from one OU to another. However, OU names must be unique within a parent OU or root.</p><p>Resource-level permissions refer to the ability to specify which resources users are allowed to perform actions on. Amazon EC2 has partial support for resource-level permissions. This means that for certain Amazon EC2 actions, you can control when users are allowed to use those actions based on conditions that have to be fulfilled, or specific resources that users are allowed to use. For example, you can grant users permissions to launch instances, but only of a specific type, and only using a specific AMI.</p><p>The scenario on this question has a lot of AWS Accounts that need to be managed. AWS Organization solves this problem and provides you with control by assigning the different business units as individual Organization Units (OU). Service control policies (SCPs) are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. However, SCPs alone are not sufficient for allowing access to the accounts in your organization. Attaching an SCP to an AWS Organizations entity just defines a guardrail for what actions the principals can perform. You still need to attach identity-based or resource-based policies to principals or resources in your organization's accounts to actually grant permission to them.</p><p>Since SCPs only allow or deny the use of an AWS service, you don't want to block OUs from completely using the EC2 service. Thus, you will need to provide cross-account access and the IAM policy to every member accounts of the OU.</p><p>Hence, the correct answer is: <strong>Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to individual Organization Units (OU). Create an IAM Role in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Provide the cross-account access and the IAM policy to every member accounts of the OU.</strong></p><p>The option that says: <strong>Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to an individual Organization Unit (OU). Create an IAM Role in the production account for each business unit which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances that it owns. Create an </strong><code><strong>AWSServiceRoleForOrganizations</strong></code><strong> service-linked role for the individual member accounts of the OU to enable trusted access</strong> is incorrect because <strong>AWSServiceRoleForOrganizations</strong> service-linked role is primarily used to only allow AWS Organizations to create service-linked roles for other AWS services. This service-linked role is present in all organizations and not just in a specific OU.</p><p>The following options are incorrect because an SCP policy simply specifies the services and actions that users and roles can use in the accounts:</p><p><strong>1. Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to an individual Organization Unit (OU). Create a Service Control Policy in the production account for each business unit which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances that it owns. Provide the cross-account access and the SCP to the individual member accounts to tightly control who can terminate the EC2 instances.</strong></p><p><strong>2. Use AWS Organizations to centrally manage all of your accounts. Group your accounts, which belong to a specific business unit, to an individual Organization Unit (OU). Create a Service Control Policy in the production account which has a policy that allows access to the EC2 instances including resource-level permission to terminate the instances owned by a particular business unit. Provide the cross-account access and the SCP to the OUs, which will then be automatically inherited by its member accounts.</strong></p><p>SCPs are similar to IAM permission policies except that they don't grant any permissions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-supported-iam-actions-resources.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A leading commercial bank has a hybrid cloud architecture and is using a Volume Gateway under the AWS Storage Gateway service to store their data via the Internet Small Computer Systems Interface (ISCSI). The security team has detected a series of replay attacks to your network, which is basically a form of network attack in which a valid data transmission is maliciously or fraudulently repeated or delayed. After their investigation, they detected that the originator of the attack is trying to intercept the data with an intention to re-transmit it, which is possibly part of a masquerade attack by IP packet substitution.</p><p>As a Solutions Architect of the bank, how can you secure your AWS Storage Gateway from these types of attacks?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Replace the current ISCSI Block Interface with an ISCSI Virtual Tape Library Interface. "
			},
			{
				"correct": false,
				"answer": "<p>Configure a Challenge-Handshake Authentication Protocol (CHAP) to authenticate NFS connections and safeguard your network from replay attacks.</p>"
			},
			{
				"correct": false,
				"answer": "Replace ISCSI with more secure protocols like Common Internet File System (CIFS) Protocol or Server Message Block (SMB)."
			},
			{
				"correct": true,
				"answer": "Configure a Challenge-Handshake Authentication Protocol (CHAP) to authenticate iSCSI and initiator connections."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In <strong>AWS Storage Gatewa</strong>y, your iSCSI initiators connect to your volumes as iSCSI targets. Storage Gateway uses Challenge-Handshake Authentication Protocol (CHAP) to authenticate iSCSI and initiator connections. CHAP provides protection against playback attacks by requiring authentication to access storage volume targets. For each volume target, you can define one or more CHAP credentials. You can view and edit these credentials for the different initiators in the Configure CHAP credentials dialog box.</p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_gateway_stored.png\"></p><p>Therefore the correct answer is: <strong>Configure a Challenge-Handshake Authentication Protocol (CHAP) to authenticate iSCSl and initiator connections</strong>.</p><p>The option that says: <strong>Replace lSCSl with more secure protocols like Common Internet File System (CIFS) Protocol or Server Message Block (SMB)</strong> is incorrect. Replacing ISCSI with CIFS and SMB would be irrelevant since these two do not provide the required security mechanism in the scenario. It is best to use Challenge-Handshake Authentication Protocol (CHAP) instead.</p><p>The option that says: <strong>Replace the current lSCSl Block Interface with an lSCSl Virtual Tape Library Interface </strong>is incorrect. ISCSI Virtual Tape Library Interface is primarily used for Tape Gateways and not for Volume Gateways. It is better to use Challenge-Handshake Authentication Protocol (CHAP) instead.</p><p>The option that says: <strong>Configure a Challenge-Handshake Authentication Protocol (CHAP) to authenticate </strong>is incorrect. CHAP is primarily used to authenticate iSCSI and not NFS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedConfigureChap.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/GettingStartedConfigureChap.html</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/initiator-connection-common.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/initiator-connection-common.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p></div>"
	},
	{
		"question": "<p>An international insurance company has clients all across the globe. The company has financial files that are stored in an Amazon S3 bucket which is behind CloudFront. At present, their clients can access their data by directly using an S3 URL or using their CloudFront distribution. The company wants to deliver their content to a specific client in California and they need to make sure that only that client can access the data.</p><p>Which of the following options is a valid solution that meets the above requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use CloudFront signed URLs to ensure that only their client can access the files. Enable field-level encryption in your CloudFront distribution.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use CloudFront Signed Cookies to ensure that only their client can access the files. Enable HTTPS in your CloudFront distribution.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use CloudFront signed URLs to ensure that only their client can access the files. Create an origin access identity (OAI) and give it permission to read the files in the bucket. Remove permission to use Amazon S3 URLs to read the files for anyone else.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new S3 bucket in US West (N. California) region and upload the files. Set up an origin access identity (OAI) and give it permission to read the files in the bucket. Enable HTTPS in your CloudFront distribution.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a new S3 bucket in US West (N. California) region and upload the files. Use S3 pre-signed URLs to ensure that only their client can access the files. Remove permission to use Amazon S3 URLs to read the files for anyone else.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Many companies that distribute content over the internet want to restrict access to documents, business data, media streams, or content that is intended for selected users, for example, users who have paid a fee. To securely serve this private content by using CloudFront, you can do the following:</p><p>- Require that your users access your private content by using special CloudFront signed URLs or signed cookies.</p><p>- Require that your users access your Amazon S3 content by using CloudFront URLs, not Amazon S3 URLs. Requiring CloudFront URLs isn't necessary, but it is recommended to prevent users from bypassing the restrictions that you specify in signed URLs or signed cookies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\"></p><p>All objects and buckets by default are private. The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions. You can generate a presigned URL programmatically using the AWS SDK for Java or the AWS SDK for .NET. If you are using Microsoft Visual Studio, you can also use AWS Explorer to generate a presigned object URL without writing any code. Anyone who receives a valid presigned URL can then programmatically upload an object.</p><p>Therefore, the correct answer is: <strong>Create a new S3 bucket in US West (N. California) region and upload the files. Use S3 pre-signed URLs to ensure that only their client can access the files. Remove permission to use Amazon S3 URLs to read the files for anyone else</strong> and <strong>Use CloudFront signed URLs to ensure that only their client can access the files. Create an origin access identity (OAI) and give it permission to read the files in the bucket. Remove permission to use Amazon S3 URLs to read the files for anyone else.</strong> Using a presigned URL to your S3 bucket will prevent other users from getting your private data which is intended to a certain client. A combination of Signed URL and OAI is also a valid solution that meets the requirement.</p><p>The option that says: <strong>Use CloudFront Signed Cookies to ensure that only their client can access the files. Enable HTTPS in your CloudFront distribution</strong> is incorrect. The signed cookies feature is primarily used if you want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of website. In addition, this solution is not complete since the users can bypass the restrictions by simply using the direct S3 URLs.</p><p>The option that says: <strong>Use CloudFront signed URLs to ensure that only their client can access the files. Enable field-level encryption in your CloudFront distribution</strong> is incorrect. Although this solution is valid, the users can still bypass the restrictions in CloudFront by simply connecting to the direct S3 URLs.</p><p>The option that says: <strong>Create a new S3 bucket in US West (N. California) region and upload the files. Set up an origin access identity (OAI) and give it permission to read the files in the bucket. Enable HTTPS in your CloudFront distribution</strong> is incorrect. An Origin Access Identity (OAI) will only require your client to only access the files by using the CloudFront URL and not through a direct S3 URL. This can be a possible solution if it mentions the use of Signed URL or Signed Cookies.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)</strong></p><p><a href=\"https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/?src=udemy\">https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A company has a team of data analysts that uploads generated data points to an Amazon S3 bucket. The data points are used by other departments so the objects on this primary S3 bucket need to be replicated to other S3 buckets on several AWS Accounts owned by the company. The Solutions Architect created an AWS Lambda function that is triggered by S3 PUT events on the primary bucket. This Lambda function will replicate the newly uploaded object to other destination buckets. Since there will be thousands of object uploads on the primary bucket every day, the company is concerned that this Lambda function may affect other critical Lambda functions because of the regional concurrency limit in AWS Lambda. The replication of the objects does not need to happen in real-time.</p><p>Which of the following options will ensure that this Lambda function will not affect the execution of other critical Lambda functions?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Implement an exponential backoff algorithm in the new Lambda function to ensure that it will not run if the concurrency limit is being reached. Use Amazon CloudWatch alarms to monitor the Throttles metric for Lambda functions to check if the concurrency limit is reached.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Decouple the Amazon S3 event notifications and send the events to an Amazon SQS queue in a separate AWS account. Create the new Lambda function on this account too. Invoke the Lambda function whenever an event message is received in the SQS queue.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set the execution timeout of the new Lambda function to 5 minutes. This will allow it to wait for other Lambda function executions to finish in case the concurrency limit is reached. Use Amazon CloudWatch alarms to monitor the Throttles metric for Lambda functions to check if the concurrency limit is reached.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure a reserved concurrency limit for the new function to ensure that its executions will not exceed this limit. Use Amazon CloudWatch alarms to monitor the Throttles metric for Lambda functions to ensure that the concurrency limit is not being reached.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Concurrency</strong> is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. Concurrency is subject to a Regional quota that is shared by all functions in a Region.</p><p>There are two types of concurrency available:</p><p><strong>Reserved concurrency</strong> – Reserved concurrency creates a pool of requests that can only be used by its function, and also prevents its function from using unreserved concurrency.</p><p><strong>Provisioned concurrency</strong> – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond to your function's invocations.</p><p>In a single account with the default concurrency limit of 1000 concurrent executions, when other services invoke Lambda function concurrently, there is the possibility for two issues to pop up:</p><p>- One or more of these services could invoke enough functions to consume a majority of the available concurrency capacity. This could cause others to be starved for it, causing failed invocations.</p><p>- A service could consume too much concurrent capacity and cause a downstream service or database to be overwhelmed, which could cause failed executions.</p><p>For Lambda functions that are launched in a VPC, you have the potential to consume the available IP addresses in a subnet or the maximum number of elastic network interfaces to which your account has access. One way to solve both of these problems is by applying a concurrency limit to the Lambda functions in an account.</p><p>You can set a concurrency limit on individual Lambda functions in an account. The concurrency limit that you set reserves a portion of your account level concurrency for a given function. All of your functions’ concurrent executions count against this account-level limit by default.</p><p>If you set a concurrency limit for a specific function then that function’s concurrency limit allocation is deducted from the shared pool and assigned to that specific function. AWS also reserves 100 units of concurrency for all functions that don’t have a specified concurrency limit set. This helps to make sure that future functions have capacity to be consumed.</p><p><img src=\"https://media.tutorialsdojo.com/sap_lambda_concurrency.png\"></p><p>Therefore, the correct answer is: <strong>Configure a reserved concurrency limit for the new function to ensure that its executions will not exceed this limit. Use Amazon CloudWatch alarms to monitor the Throttles metric for Lambda functions to ensure that the concurrency limit is not being reached.</strong></p><p>The option that says: <strong>Set the execution timeout of the new Lambda function to 5 minutes. This will allow it to wait for other Lambda function executions to finish in case the concurrency limit is reached. Use Amazon CloudWatch alarms to monitor the Throttles metric for Lambda functions to check if the concurrency limit is reached</strong> is incorrect. This will still invoke the new Lambda function and it will cause more problems because during the wait time, the slice for the Lambda function is still consumed. Other Lambda functions won't be able to execute if all slices are consumed.</p><p>The option that says: <strong>Decouple the Amazon S3 event notifications and send the events to an Amazon SQS queue in a separate AWS account. Create the new Lambda function on this account too. Invoke the Lambda function whenever an event message is received in the SQS queue</strong> is incorrect. This is possible and you will have the concurrency limit on the separate AWS account all for the new Lambda function. However, this requires more work and the creation of another AWS account. Setting a concurrency limit is recommended as it can be used to limit the number of executions of a particular function.</p><p>The option that says: <strong>Implement an exponential backoff algorithm in the new Lambda function to ensure that it will not run if the concurrency limit is being reached. Use Amazon CloudWatch alarms to monitor the Throttles metric for Lambda functions to check if the concurrency limit is reached </strong>is incorrect. This will require you to write a backoff algorithm to check the concurrency limit. The function needs to execute in order to run the backoff algorithm which defeats the purpose of limiting concurrency.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html\">https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\">https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html\">https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/\">https://aws.amazon.com/blogs/compute/managing-aws-lambda-function-concurrency/</a></p><p><br></p><p><strong>Check out the AWS Lambda Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p></div>"
	},
	{
		"question": "<p>A company has several NFS shares in its on-premises data center that contains millions of small log files totaling around 50TB in size. The files in these NFS shares need to be migrated to an Amazon S3 bucket. To start the migration process, the solutions architect requested an AWS Snowball Edge device that will be used to transfer the files to Amazon S3. A file interface was configured on the Snowball Edge device and is connected to the corporate network. The Solutions Architect initiated the <code>snowball cp</code> command to start the copying process, however, the copying of data is significantly slower than expected.</p><p>Which of the following options are the likely cause of the slow transfer speed and the recommended solution?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>The file interface of the Snowball Edge is limited by the network interface speed. Connect the device directly using a high-speed USB 3.0 interface instead to maximize the copying throughput.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The file interface of the Snowball Edge has reached its throughput limit. Change the interface to an S3 Adapter instead for a significantly faster transfer speed.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ingesting millions of files has saturated the processing power of the Snowball Edge. Request for another Snowball Edge device and cluster them together to increase the ingest throughput.</p>"
			},
			{
				"correct": true,
				"answer": "<p>This is due to encryption overhead when copying files to the Snowball Edge device. Open multiple sessions to the Snowball Edge device and initiate parallel copy jobs to improve the overall copying throughput.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>One of the major ways that you can improve the performance of an <strong>AWS Snowball Edge</strong> device is to speed up the transfer of data going to and from a device. In general, you can improve the transfer speed from your data source to the device in the following ways. The following list is ordered<strong> from largest to smallest positive impact on performance</strong>:</p><p><strong>Perform multiple write operations at one time</strong> – To do this, run each command from multiple terminal windows on a computer with a network connection to a single AWS Snowball Edge device.</p><p><strong>Transfer small files in batches </strong>– Each copy operation has some overhead because of encryption. To speed up the process, batch files together in a single archive. When you batch files together, they can be auto-extracted when they are imported into Amazon S3.</p><p><strong>Write from multiple computers</strong> – A single AWS Snowball Edge device can be connected to many computers on a network. Each computer can connect to any of the three network interfaces at once.</p><p><strong>Don't perform other operations on files during transfer</strong> – Renaming files during transfer, changing their metadata, or writing data to the files during a copy operation has a negative impact on transfer performance. AWS recommends that your files remain in a static state while you transfer them.</p><p><strong>Reduce local network use</strong> – Your AWS Snowball Edge device communicates across your local network. So you can improve data transfer speeds by reducing other local network traffic between the AWS Snowball Edge device, the switch it's connected to, and the computer that hosts your data source.</p><p><strong>Eliminate unnecessary hops</strong> – AWS recommends that you set up your AWS Snowball Edge device, your data source, and the computer running the terminal connection between them so that they're the only machines communicating across a single switch. Doing so can improve data transfer speeds.</p><p>For transferring small files, AWS also recommends transferring in batches. Each copy operation has some overhead because of encryption. To speed up the process of transferring small files to your AWS Snowball Edge device, you can batch them together in a single archive. When you batch files together, they can be auto-extracted when they are imported into Amazon S3, if they were batched in one of the supported archive formats.</p><p>Typically, files that are 1 MB or smaller should be included in batches. There's no hard limit on the number of files you can have in a batch, though AWS recommends that you limit your batches to about 10,000 files. Having more than 100,000 files in a batch can affect how quickly those files import into Amazon S3 after you return the device. AWS recommends that the total size of each batch be no larger than 100 GB. Batching files is a manual process, which you have to manage.</p><p>Therefore, the correct answer is: <strong>This is due to encryption overhead when copying files to the Snowball Edge device. Open multiple sessions to the Snowball Edge device and initiate parallel copy jobs to improve the overall copying throughput. </strong>Performing multiple copy operations to the Snowball Edge device has the biggest impact to improve your transfer speed.</p><p>The option that says: <strong>Ingesting millions of files has saturated the processing power of the Snowball Edge. Request for another Snowball Edge device and cluster them together to increase the ingest throughput </strong>is incorrect. A Snowball Edge cluster has the benefits of increased durability and storage capacity. It does not improve the copy transfer speed.</p><p>The option that says: <strong>The file interface of the Snowball Edge has reached its throughput limit. Change the interface to an S3 Adapter instead for a significantly faster transfer speed </strong>is incorrect. An S3 Adapter is used to transfer data programmatically to and from the AWS Snowball Edge device using Amazon S3 REST API actions. It may be faster to use an S3 interface if you reached the file interface limit. However, the question already states that the copying of data is very slow compared to what is expected.<br></p><p>The option that says: <strong>The file interface of the Snowball Edge is limited by the network interface speed. Connect the device directly using a high-speed USB 3.0 interface instead to maximize the copying throughput</strong> is incorrect. Although some revisions of USB 3.0 or USB 3.1 can support up to 5 Gbps to 10 Gbps speeds, the network interface on the Snowball Edge supports up to 100 Gbps. You can maximize throughput by issuing multiple copy commands to the Snowball device.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/batching-small-files.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/batching-small-files.html</a></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/BestPractices.html</a></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/ug/performance.html\">https://docs.aws.amazon.com/snowball/latest/ug/performance.html</a></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/using-adapter.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/using-adapter.html</a></p><p><br></p><p><strong>Check out this AWS Snowball Edge Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-snowball-edge/?src=udemy\">https://tutorialsdojo.com/aws-snowball-edge/</a></p><p><br></p><p><strong>AWS Snow Family Overview:</strong></p><p><a href=\"https://www.youtube.com/watch?v=9Ar-51Ip53Q\">https://www.youtube.com/watch?v=9Ar-51Ip53Q</a></p></div>"
	},
	{
		"question": "<p>A company is planning to migrate its workload to the AWS cloud. The solutions architect is looking to reduce the amount of time spent managing database instances from the on-premises data center by migrating to a managed relational database service in AWS such as Amazon Relational Database Service (RDS). In addition, the solutions architect plans to move the application hosted in the on-premises data center to a fully managed platform such as AWS Elastic Beanstalk.</p><p>Which of the following is the most cost-effective migration strategy that should be implemented to meet the above requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Rehost</p>"
			},
			{
				"correct": true,
				"answer": "<p>Replatform</p>"
			},
			{
				"correct": false,
				"answer": "<p>Repurchase</p>"
			},
			{
				"correct": false,
				"answer": "<p>Refactor / Re-architect</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Organizations</strong> usually begin to think about how they will migrate an application during Phase 2 (<em>Portfolio Discovery and Planning</em>) of the migration process. This is when you determine what is in your environment and the migration strategy for each application. The six approaches detailed below are common migration strategies employed and build upon “The 5 R’s” that Gartner Inc, a global research and advisory firm, outlined in 2011.</p><p>You should gain a thorough understanding of which migration strategy will be best suited for certain portions of your portfolio. It is also important to consider that while one of the six strategies may be best for migrating certain applications in a given portfolio, another strategy might work better for moving different applications in the same portfolio.</p><p><img src=\"https://media.tutorialsdojo.com/sap_migration_paths.png\"></p><p><strong>1. Rehost (“lift and shift”)</strong> - In a large legacy migration scenario where the organization is looking to quickly implement its migration and scale to meet a business case, we find that the majority of applications are rehosted.</p><p><strong>2. Replatform (“lift, tinker and shift”) </strong>- This entails making a few cloud optimizations in order to achieve some tangible benefit without changing the core architecture of the application.</p><p><strong>3. Repurchase (“drop and shop”) </strong>- This is a decision to move to a different product and likely means your organization is willing to change the existing licensing model you have been using. For workloads that can easily be upgraded to newer versions, this strategy might allow a feature set upgrade and smoother implementation.</p><p><strong>4. Refactor / Re-architect </strong>- Typically, this is driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment.</p><p><strong>5. Retire </strong>- Identifying IT assets that are no longer useful and can be turned off will help boost your business case and direct your attention towards maintaining the resources that are widely used.</p><p><strong>6. Retain</strong> - You may want to retain portions of your IT portfolio because there are some applications that you are not ready to migrate and feel more comfortable keeping them on-premises, or you are not ready to prioritize an application that was recently upgraded and then make changes to it again.</p><p>Therefore, the correct answer is: <strong>Replatform. </strong>This strategy is done by making a few cloud optimizations on your existing systems before migrating them to AWS, which is what will happen if you move your existing database and web applications to AWS. This strategy is more suitable when you want to reduce the amount of time you spend managing database instances by migrating to a managed relational database service such as Amazon Relational Database Service (RDS), or migrating your application to a fully managed platform like AWS Elastic Beanstalk.</p><p><strong>Rehost</strong> is incorrect. Rehost (“lift and shift”) strategy is more suitable for quickly migrating the systems to AWS to meet a certain business case without no additional configuration involved. Take note that if you migrate your systems to either Elastic Beanstalk or RDS, you will still need to set up, configure, and test your systems, which takes additional time and effort.</p><p><strong>Repurchase</strong> is incorrect. This strategy entails a decision to move to a different product and likely means your organization is willing to change the existing licensing model you have been using. Hence, this is not a suitable migration strategy for this scenario.</p><p><strong>Refactor / Re-architect</strong> is incorrect. This strategy is suitable if there is a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment. This type of migration strategy also entails additional cost, compared with the Replatform strategy, since you are allocating time, effort, and budget to optimize your systems.</p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><a href=\"https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/\">https://aws.amazon.com/blogs/enterprise-strategy/6-strategies-for-migrating-applications-to-the-cloud/</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-migration/planning-phase.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-migration/planning-phase.html</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A telecommunications company has several Amazon EC2 instances inside an AWS VPC. To improve data leak protection, the company wants to restrict the internet connectivity of its EC2 instances. The EC2 instances that are launched on a public subnet should be able to access product updates and patches from the Internet. The packages are accessible through the third-party provider via their URLs. The company wants to explicitly deny any other outbound connections from the VPC instances to hosts on the Internet.</p><p>Which of the following options would the solutions architect consider implementing to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use network ACL rules that allow network access to your specific package destinations. Add an implicit deny for all other cases."
			},
			{
				"correct": true,
				"answer": "You can use a forward web proxy server in your VPC and manage outbound access using URL-based rules. Default routes are also removed."
			},
			{
				"correct": false,
				"answer": "Create security groups with the appropriate outbound access rules that will let you retrieve software packages from the Internet."
			},
			{
				"correct": false,
				"answer": "Move all instances from the public subnets to the private subnets. Additionally, remove the default routes from your routing tables and replace them instead with routes that specify your package locations."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A forward proxy server acts as an intermediary for requests from internal users and servers, often caching content to speed up subsequent requests. Companies usually implement proxy solutions to provide URL and web content filtering, IDS/IPS, data loss prevention, monitoring, and advanced threat protection. AWS customers often use a VPN or AWS Direct Connect connection to leverage existing corporate proxy server infrastructure, or build a forward proxy farm on AWS using software such as Squid proxy servers with internal Elastic Load Balancing (ELB).</p><p>You can limit outbound web connections from your VPC to the internet, using a web proxy (such as a squid server) with custom domain whitelists or DNS content filtering services. The solution is scalable, highly available, and deploys in a fully automated way.</p><p><img src=\"https://media.tutorialsdojo.com/sap_squid_proxy.jpg\"></p><p>Therefore, the correct answer is: <strong>You can use a forward web proxy server in your VPC and manage outbound access using URL-based rules. Default routes are also removed.</strong> A proxy server filters requests from the client, and allows only those that are related to the product updates, and in this case helps filter all other requests except the ones for the product updates.</p><p>The option that says: <strong>Move all instances from the public subnets to the private subnets. Additionally, remove the default routes from your routing tables and replace them instead with routes that specify your package locations</strong> is incorrect. Even though moving the instances in a private subnet is a good idea, the routing table does not have the filtering logic. It only connects the subnets with Internet gateway.</p><p>The option that says: <strong>Using network ACL rules that allow network access to your specific package destinations then adding an implicit deny for all other cases</strong> is incorrect. NACLs cannot filter requests based on URLs.</p><p>The option that says: <strong>Creating security groups with the appropriate outbound access rules that will let you retrieve software packages from the Internet</strong> is incorrect. A security group cannot filter requests based on URLs.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-http-proxy.html\">https://docs.aws.amazon.com/cli/latest/userguide/cli-http-proxy.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/\">https://aws.amazon.com/blogs/security/how-to-set-up-an-outbound-vpc-proxy-with-domain-whitelisting-and-content-filtering/</a></p><p><br></p><p><strong>Amazon VPC Overview:</strong></p><p><a href=\"https://youtu.be/oIDHKeNxvQQ\">https://youtu.be/oIDHKeNxvQQ</a><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company is running thousands of virtualized Linux and Microsoft Windows servers on its on-premises data center. The virtual servers host a range of Java and PHP applications that are using MySQL and Oracle databases. There are also several department services hosted on an external data center. The company uses SAN storage to provide iSCSI disks to its physical servers. The company wants to migrate its data center into the AWS Cloud but the technical documentation of the systems is incomplete and outdated. The Solutions Architect was tasked to analyze the current environment and estimate the cost of migrating the resources to the cloud.</p><p>Which of the following should the Solutions Architect do to effectively plan the cloud migration? (Select THREE.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use AWS Application Discovery Service to gather information about the running virtual machines and running applications inside the servers.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Inspector to scan and assess the applications deployed on the on-premises virtual machines and save the generated report to an Amazon S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Server Migration Service (SMS) to automate the migration of the on-premises virtual machines to the AWS Cloud.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS X-Ray to analyze the applications running in the servers and identify possible errors that may be encountered during the migration.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the AWS Cloud Adoption Readiness Tool (CART) to generate a migration assessment report to identify gaps in organizational skills and processes.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Migration Hub to discover and track the status of the application migration across AWS and partner solutions.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The scenario requires tools of services that will help to effectively plan the cloud migration so the answers should be focused on planning.</p><p><strong>AWS Application Discovery Service</strong> helps you plan your migration to the AWS cloud by collecting usage and configuration data about your on-premises servers. Application Discovery Service is integrated with AWS Migration Hub, which simplifies your migration tracking as it aggregates your migration status information into a single console. You can view the discovered servers, group them into applications, and then track the migration status of each application from the Migration Hub console in your home region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_migration_hub.png\"></p><p>Application Discovery Service offers two ways of performing discovery and collecting data about your on-premises servers:</p><p><strong>- Agentless discovery</strong> can be performed by deploying the AWS Agentless Discovery Connector (OVA file) through your VMware Center.</p><p><strong>- Agent-based discovery</strong> can be performed by deploying the AWS Application Discovery Agent on each of your VMs and physical servers.</p><p>The <strong>AWS Cloud Adoption Readiness Tool (CART)</strong> helps organizations of all sizes develop efficient and effective plans for cloud adoption and enterprise cloud migrations. This 16-question online survey and assessment report details your cloud migration readiness across six perspectives including business, people, process, platform, operations, and security. Once you complete a CART survey, you can provide your contact details to download a customized cloud migration assessment that charts your readiness and what you can do to improve it. This tool is designed to help organizations assess their progress with cloud adoption and identify gaps in organizational skills and processes.</p><p><strong>AWS Migration Hub (Migration Hub)</strong> provides a single place to discover your existing servers, plan migrations, and track the status of each application migration. The Migration Hub provides visibility into your application portfolio and streamlines planning and tracking. You can visualize the connections and the status of the servers and databases that make up each of the applications you are migrating, regardless of which migration tool you are using. Migration Hub gives you the choice to start migrating right away and group servers while migration is underway, or to first discover servers and then group them into applications.</p><p>Therefore, the correct answers are:</p><p><strong>- Use AWS Application Discovery Service to gather information about the running virtual machines and running applications inside the servers.</strong></p><p><strong>- Use the AWS Cloud Adoption Readiness Tool (CART) to generate a migration assessment report to identify gaps in organizational skills and processes.</strong></p><p><strong>- Use AWS Migration Hub to discover and track the status of the application migration across AWS and partner solutions.</strong></p><p>The option that says: <strong>Use AWS Server Migration Service (SMS) to automate the migration of the on-premises virtual machines to the AWS Cloud</strong> is incorrect because SMS is primarily used for the actual migration of your on-premises virtual machines to the AWS cloud and not for planning. Take note that in the scenario, the Solutions Architect was tasked to analyze the existing on-premises architecture first before doing the actual migration to AWS.</p><p>The option that says: <strong>Use AWS X-Ray to analyze the applications running in the servers and identify possible errors that may be encountered during the migration</strong> is incorrect because AWS X-Ray is used to debug production and distributed applications such as those built using a microservices architecture. This is not helpful for planning the migration.</p><p>The option that says: <strong>Use Amazon Inspector to scan and assess the applications deployed on the on-premises virtual machines and save the generated report to an Amazon S3 bucket</strong> is incorrect because Amazon Inspector is simply an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. This is not helpful for assessing the applications on the on-premises data center.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html\">https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html</a></p><p><a href=\"https://cloudreadiness.amazonaws.com/#/cart\">https://cloudreadiness.amazonaws.com/#/cart</a></p><p><a href=\"https://docs.aws.amazon.com/migrationhub/latest/ug/getting-started.html\">https://docs.aws.amazon.com/migrationhub/latest/ug/getting-started.html</a></p><p><br></p><p><strong>AWS Migration Services Overview:</strong></p><p><a href=\"https://youtu.be/yqNBkFMnsL8\">https://youtu.be/yqNBkFMnsL8</a><br></p><p><strong>Check out the AWS Migration Services Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cheat-sheets-migration-services/?src=udemy\">https://tutorialsdojo.com/aws-cheat-sheets-migration-services/</a></p></div>"
	},
	{
		"question": "<p>A company wants to create a new service that will complement the launch of its new product. The site must be highly-available and scalable to handle the unpredictable workload, and should also be stateless and REST compliant. The solution needs to have multiple persistent storage layers for service object metadata and durable storage for static content. All requests to the service should be authenticated and securely processed. The company also wants to keep the costs at a minimum.</p><p>Which of the following is the recommended solution that will meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Package the REST service on a Docker-based container and run it using the AWS Fargate service. Create a cross-zone Application Load Balancer in front of the Fargate service. Control user access to the API by using Amazon Cognito user pools. Store service object metadata in an Amazon DynamoDB table with Auto Scaling enabled. Create an encrypted Amazon S3 bucket to store the static content. Generate presigned URLs when referencing objects stored on the S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure Amazon API Gateway with the required resources and methods. Create unique Lambda functions to process each resource and configure the API Gateway methods with proxy integration to the respective Lambda functions. Control user access to the API by using the API Gateway custom authorizer. Store service object metadata in an Amazon ElastiCache Multi-AZ cluster. Create a secured Amazon S3 bucket to store the static content. Generate presigned URLs when referencing objects stored on the S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Package the REST service on a Docker-based container and run it using the AWS Fargate service. Create an Application Load Balancer in front of the Fargate service. Create a custom authenticator that will control access to the API. Store service object metadata in an Amazon DynamoDB table with Auto Scaling enabled. Create an Amazon S3 bucket to store the static content and enable secure-signed requests for the objects. Proxy the data through the REST service.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure Amazon API Gateway with the required resources and methods. Create unique Lambda functions to process each resource and configure the API Gateway methods with proxy integration to the respective Lambda functions. Control user access to the API by using Amazon Cognito user pools. Store service object metadata in an Amazon DynamoDB table with Auto Scaling enabled. Create a secured Amazon S3 bucket to store the static content. Generate presigned URLs when referencing objects stored on the S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon API Gateway Lambda proxy</strong> <strong>integration</strong> is a simple, powerful, and nimble mechanism to build an API with a setup of a single API method. The Lambda proxy integration allows the client to call a single Lambda function in the backend. The function accesses many resources or features of other AWS services, including calling other Lambda functions.</p><p>In Lambda proxy integration, when a client submits an API request, API Gateway passes to the integrated Lambda function the raw request as-is, except that the order of the request parameters is not preserved. This request data includes the request headers, query string parameters, URL path variables, payload, and API configuration data. The configuration data can include current deployment stage name, stage variables, user identity, or authorization context (if any).</p><p>A user pool is a user directory in <strong>Amazon Cognito</strong>. With a user pool, your users can sign in to your web or mobile app through Amazon Cognito. Your users can also sign in through social identity providers like Google, Facebook, Amazon, or Apple, and through SAML identity providers. Whether your users sign in directly or through a third party, all members of the user pool have a directory profile that you can access through a Software Development Kit (SDK).</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_user_pool.png\"></p><p>User pools provide:</p><p>- Sign-up and sign-in services.</p><p>- A built-in, customizable web UI to sign in users.</p><p>- Social sign-in with Facebook, Google, Login with Amazon, and Sign in with Apple, as well as sign-in with SAML identity providers from your user pool.</p><p>- User directory management and user profiles.</p><p>- Security features such as multi-factor authentication (MFA), checks for compromised credentials, account takeover protection, and phone and email verification.</p><p>- Customized workflows and user migration through AWS Lambda triggers.</p><p><strong>Amazon S3</strong> is a simple key-based object store whose scalability and low cost make it ideal for storing large datasets or objects. When finding objects based on attributes or other metadata, a common solution is to build an external index such as a DynamoDB Table that maps queryable attributes to the S3 object key. DynamoDB is a NoSQL data store that can be used for storing the index itself.</p><p>One way of securing objects that are shared on S3 buckets is by using presigned URLs. When you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object), and expiration date and time. The presigned URLs are valid only for the specified duration.</p><p>Anyone who receives the presigned URL can then access the object. For example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a presigned URL.</p><p>With the above solutions, the correct answer is:<strong> Configure Amazon API Gateway with the required resources and methods. Create unique Lambda functions to process each resource and configure the API Gateway methods with proxy integration to the respective Lambda functions. Control user access to the API by using Amazon Cognito user pools. Store service object metadata in an Amazon DynamoDB table with Auto Scaling enabled. Create a secured Amazon S3 bucket to store the static content. Generate presigned URLs when referencing objects stored on the S3 bucket.</strong></p><p>The option that says: <strong>Package the REST service on a Docker-based container and run it using the AWS Fargate service. Create an Application Load Balancer in front of the Fargate service. Create a custom authenticator that will control access to the API. Store service object metadata in an Amazon DynamoDB table with Auto Scaling enabled. Create an Amazon S3 bucket to store the static content and enable secure-signed requests for the objects. Proxy the data through the REST service </strong>is incorrect. Although running Docker-based containers on Amazon Fargate is possible, this solution does not offer the lowest possible cost to satisfy the given scenario. AWS Lambda is suited for creating serverless/stateless APIs and costs cheaper than AWS Fargate.</p><p>The option that says: <strong>Package the REST service on a Docker-based container and run it using the AWS Fargate service. Create a cross-zone Application Load Balancer in front of the Fargate service. Control user access to the API by using Amazon Cognito user pools. Store service object metadata in an Amazon DynamoDB table with Auto Scaling enabled. Create an encrypted Amazon S3 bucket to store the static content. Generate presigned URLs when referencing objects stored on the S3 bucket </strong>is incorrect. Running a Fargate cluster continuously is more expensive than running Lambda functions which only runs on-demand.</p><p>The option that says: <strong>Configure Amazon API Gateway with the required resources and methods. Create unique Lambda functions to process each resource and configure the API Gateway methods with proxy integration to the respective Lambda functions. Control user access to the API by using the API Gateway custom authorizer. Store service object metadata in an Amazon ElastiCache Multi-AZ cluster. Create a secured Amazon S3 bucket to store the static content. Generate presigned URLs when referencing objects stored on the S3 bucket</strong> is incorrect because it is recommended to use Amazon Cognito user pools for user access controls compared to an API Gateway custom authorizer. It is more robust and offers more features because it is designed to handle user access.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html</a></p><p><a href=\"https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/\">https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers/</a></p><p><br></p><p><strong>Check out these AWS Comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/?src=udemy\">https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</a></p><p><a href=\"https://tutorialsdojo.com/ec2-container-service-ecs-vs-lambda/?src=udemy\">https://tutorialsdojo.com/ec2-container-service-ecs-vs-lambda/</a></p></div>"
	},
	{
		"question": "<p>A hospital chain in London uses an online central hub for its doctors and nurses. The application interacts with millions of requests per day to fetch various medical data of their patients. The system is composed of a web tier, an application tier, and a database tier that receives large and unpredictable traffic demands. The Solutions Architect must ensure that this infrastructure is highly-available and scalable enough to handle web traffic fluctuations automatically.</p><p>Which of the following options should the solutions architect implement to meet the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Run the web and application tiers in stateless instances in an autoscaling group, using Elasticache Memcached for tier synchronization and CloudWatch for monitoring. Run the database tier using RDS with read replicas.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Run the web and application tiers in stateless instances in an autoscaling group, using Elasticache Memcached for tier synchronization and CloudWatch for monitoring. Run the database tier using RDS with Multi-AZ enabled.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Run the web and application tiers in stateful instances in an autoscaling group, using CloudWatch for monitoring. Run the database tier using RDS with Multi-AZ enabled.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Run the web and application tiers in stateful instances in an autoscaling group, using CloudWatch for monitoring. Run the database tier using RDS with read replicas.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When users or services interact with an application, they will often perform a series of interactions that form a session. A session is unique data for users that persists between requests while they use the application. A stateless application is an application that does not need knowledge of previous interactions and does not store session information.</p><p><img src=\"https://media.tutorialsdojo.com/sap_spot_stateless_session.png\"></p><p>For example, an application that, given the same input, provides the same response to any end user, is a stateless application. Stateless applications can scale horizontally because any of the available compute resources (such as EC2 instances and AWS Lambda functions) can service any request. Without stored session data, you can simply add more compute resources as needed. When that capacity is no longer required, you can safely terminate those individual resources, after running tasks have been drained. Those resources do not need to be aware of the presence of their peers—all that is required is a way to distribute the workload to them.</p><p>In this scenario, the best option is to use a combination of <strong>Elasticache</strong>, <strong>Cloudwatch</strong>, and <strong>RDS Read Replica</strong>.</p><p>Therefore, the correct answer is: <strong>Run the web and application tiers in stateless instances in an autoscaling group, using Elasticache Memcached for tier synchronization and CloudWatch for monitoring. Run the database tier using RDS with read replicas.</strong> It uses stateless instances. The web server uses ElastiCache for read operations and CloudWatch which monitors fluctuations in the traffic and notifies the autoscaling group to scale in/scale out accordingly. In addition, it uses read replicas for RDS to handle the read-heavy workload.</p><p>The option that says: <strong>Run the web and application tiers in stateful instances in an autoscaling group, using CloudWatch for monitoring. Run the database tier using RDS with Multi-AZ enabled</strong> is incorrect because it uses stateful instances. It also does not use any caching mechanism for web and application tiers, and multi-AZ RDS does not improve read performance.</p><p>The option that says: <strong>Run the web and application tiers in stateful instances in an autoscaling group, using CloudWatch for monitoring. Run the database tier using RDS with read replicas</strong> is incorrect because it uses stateful instances and it does not use any caching mechanism for web and application tiers.</p><p>The option that says: <strong>Run the web and application tiers in stateless instances in an autoscaling group, using Elasticache Memcached for tier synchronization and CloudWatch for monitoring. Run the database tier using RDS with Multi-AZ enabled</strong> is incorrect because multi-AZ RDS only improves Availability, not read performance.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company runs its internal tool on AWS. It is used for logistics and shipment tracking for the company’s warehouse. With the current system process, the application receives an order and it sends an email to the employees with the information needed for the package shipment. After the employees prepare the order and ship the package, they reply to the email so that the application can mark the order as shipped. The company wants to migrate to a serverless application model to stop relying on emails and minimize the operational overhead for the application.</p><p>Which of the following options should the Solutions Architect implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use an Amazon EFS volume to store the new order information. Configure an instance to pull the order information from the EFS share and print the shipping label for the package. Once the package is scanned and leaves the warehouse, remove the order information on the EFS share by using an Amazon API Gateway call to the instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create AWS Batch jobs corresponding to different tasks needed to ship a package. Write an AWS Lambda function with AWS Batch as the trigger to create and print the shipping label for the package. Once the package is scanned and leaves the warehouse, trigger another Lambda function to move the AWS Batch job to the next stage of the shipping process.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Store the order information on an Amazon DynamoDB table. Create an AWS Step Functions workflow that will be triggered for every new order. Have the workflow mark the order as “in progress” and print the shipping label for the package. Once the package is scanned and leaves the warehouse, trigger an AWS Lambda function to mark the order as “shipped” and complete the Step Functions workflow.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store order information on an Amazon SQS queue when a new order is created. Schedule an AWS Lambda function to poll the queue every 5 minutes and start processing if any orders are found. Use the Lambda function to print the shipping label for the package. Once the package is scanned and leaves the warehouse, have an Amazon EC2 instance update the SQS queue to remove the order.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Step Functions</strong> is a serverless orchestration service that lets you combine AWS Lambda functions and other AWS services to build business-critical applications. Orchestration centrally manages a workflow by breaking it into multiple steps, adding flow logic, and tracking the inputs and outputs between the steps. As your applications execute, Step Functions maintains the application state, tracking exactly which workflow step your application is in, and stores an event log of data that is passed between application components.</p><p>Step Functions is based on state machines and tasks. A state machine is a workflow. A task is a state in a workflow that represents a single unit of work that another AWS service performs. Each step in a workflow is a state.</p><p>With Step Functions' built-in controls, you examine the state of each step in your workflow to make sure that your application runs in order and as expected. Depending on your use case, you can have Step Functions call AWS services, such as <strong>AWS Lambda</strong>, to perform tasks.</p><p>Step Functions is ideal for coordinating session-based applications. You can use Step Functions to coordinate all of the steps of a checkout process on an e-commerce site, for example. Step Functions can read and write from <strong>Amazon DynamoDB</strong> as needed to manage inventory records.</p><p><img src=\"https://media.tutorialsdojo.com/sap_step_function_api_gateway.JPG\"></p><p>You can use Step Functions to make decisions about how best to process data, for example, to do post-processing of groups of satellite images to determine the amount of trees per acre of land. Depending on the size and resolution of the image, this Step Functions workflow will determine whether to use AWS Lambda or AWS Fargate to complete post-processing of each file, in order to optimize runtime and costs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_step_function_amazon_s3.JPG\"></p><p>Using AWS Step Functions, you define your workflows as state machines, which transform complex code into easy-to-understand statements and diagrams. Building apps and confirming that they are implementing your desired functionality is quicker and easier.</p><p>Therefore, the correct answer is: <strong>Store the order information on an Amazon DynamoDB table. Create an AWS Step Functions workflow that will be triggered for every new order. Have the workflow mark the order as “in progress” and print the shipping label for the package. Once the package is scanned and leaves the warehouse, trigger an AWS Lambda function to mark the order as “shipped” and complete the Step Functions workflow.</strong> Step Functions is suitable to orchestrate this workflow to update a DynamoDB table for the order progress as well trigger AWS Lambda functions for various actions. <br></p><p>The option that says: <strong>Create AWS Batch jobs corresponding to different tasks needed to ship a package. Write an AWS Lambda function with AWS Batch as the trigger to create and print the shipping label for the package. Once the package is scanned and leaves the warehouse, trigger another Lambda function to move the AWS Batch job to the next stage of the shipping process</strong> is incorrect. AWS Batch is not designed to orchestrate a workflow. AWS Batch is used to run batch jobs such as transaction reporting or analysis reporting, which usually run as stand-alone jobs.</p><p>The option that says: <strong>Store order information on an Amazon SQS queue when a new order is created. Schedule an AWS Lambda function to poll the queue every 5 minutes and start processing if any orders are found. Use the Lambda function to print the shipping label for the package. Once the package is scanned and leaves the warehouse, have an Amazon EC2 instance update the SQS queue to remove the order</strong> is incorrect. Using an Amazon EC2 instance does not fulfill the company's serverless requirement. Polling every 5 minutes is not ideal as orders should be processed as soon as they are received.</p><p>The option that says: <strong>Use an Amazon EFS volume to store the new order information. Configure an instance to pull the order information from the EFS share and print the shipping label for the package. Once the package is scanned and leaves the warehouse, remove the order information on the EFS share by using an Amazon API Gateway call to the instances</strong> is incorrect. Using an EFS share will require EC2 instances and will increase the operational overhead needed to manage the infrastructure. This is a serverless solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/step-functions/use-cases/\">https://aws.amazon.com/step-functions/use-cases/</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\">https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html</a></p><p><a href=\"https://aws.amazon.com/step-functions/features/\">https://aws.amazon.com/step-functions/features/</a></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/\">https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/</a></p><p><br></p><p><strong>Check out the AWS Step Functions Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-step-functions/?src=udemy\">https://tutorialsdojo.com/aws-step-functions/</a></p></div>"
	},
	{
		"question": "<p>A company wants to improve data protection for the sensitive information stored on its AWS account - both in transit and at rest. Data protection in transit simply means that the data should be secured while it travels to and from Amazon S3. Data protection at rest means that the stored data on disk must be secured in Amazon S3 data centers. You can protect data in transit by using SSL or by using client-side encryption. To secure data at rest, you can choose from a variety of available Server-Side Encryption in S3.</p><p>Which of the following best describes how Amazon S3-Managed Keys (SSE-S3) encryption method works?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "SSE-S3 provides separate permissions to use an API key that provides added protection against unauthorized access of your objects in S3."
			},
			{
				"correct": false,
				"answer": "In SSE-S3, you will be able to manage the customer master keys (CMKs) and Amazon S3 manages the encryption for reading and writing objects in your S3 bucket."
			},
			{
				"correct": true,
				"answer": "SSE-S3 provides strong multi-factor encryption in which each object is encrypted with a unique key. It also encrypts the key itself with a master key that it rotates regularly."
			},
			{
				"correct": false,
				"answer": "In SSE-S3, a randomly generated data encryption key is returned which is used by the client to encrypt the object data."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With <strong>Amazon S3</strong> default encryption, you can set the default encryption behavior for an S3 bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or AWS KMS keys stored in AWS Key Management Service (SSE-KMS).</p><p>When you configure your bucket to use default encryption with SSE-KMS, you can also enable S3 Bucket Keys to decrease request traffic from Amazon S3 to AWS Key Management Service (AWS KMS) and reduce the cost of encryption. When you use server-side encryption, Amazon S3 encrypts an object before saving it to disk and decrypts it when you download the objects.</p><p><strong>Server-side encryption</strong> with Amazon S3-managed encryption keys (SSE-S3) uses strong multi-factor encryption. Amazon S3 encrypts each object with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it rotates regularly. Amazon S3 server-side encryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption Standard (AES-256), to encrypt your data.</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_sse_encryption.png\"></p><p>Therefore the correct answer is: <strong>SSE-S3 provides strong multi-factor encryption in which each object is encrypted with a unique key. It also encrypts the key itself with a master key that it rotates regularly.</strong></p><p>The option that says: <strong>SSE-S3 provides separate permissions to use an API key that provides added protection against unauthorized access of your objects in S3</strong> is incorrect. SSE-S3 does not use API keys but rather encryption keys.</p><p>The option that says: <strong>In SSE-S3, you will be able to manage the customer master keys (CMKs) and Amazon S3 manages the encryption for reading and writing objects in your S3 bucket </strong>is incorrect. Customer master keys (CMKs) are being used in SSE-KMS and not in SSE-S3.</p><p>The option that says:<strong> In SSE-S3, a randomly generated data encryption key is returned which is used by the client to encrypt the object data</strong> is incorrect. SSE-S3 does not use a randomly generated data encryption key.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A company is running 150 virtual machines (VMs) using 40 TB of storage on its on-premises data center. The company wants to migrate its whole environment to AWS within the next three months. The VMs are mainly used during business hours only so they can be taken offline but some are mission-critical which means that the downtime needs to be minimized. Since upgrading the Internet connection is quite costly for the company, the on-premises network administrator provisioned only a 12 Mbps Internet bandwidth for the migration. The Solutions Architect must design a cost-effective plan to complete the migration within the target time frame.</p><p>Which of the following options should the Solutions Architect implement to fulfill the requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Deploy the AWS Agentless Discovery connector on the company VMware vCenter to assess each application. With the information gathered, refactor each application to run on AWS services or using AWS Marketplace solutions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Request for a 1 Gbps AWS Direct Connect connection from the on-premises data center to AWS. Create a private virtual interface on Direct Connect. Migrate the virtual machines to AWS using the AWS Server Migration Service (SMS).</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an export of your virtual machines during out of office hours. Use the AWS Transfer service to securely upload the VMs to Amazon S3 using the SFTP protocol. Import the VMs into Amazon EC2 instances using the VM Import/Export service.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Server Migration Service to migrate the mission-critical virtual machines to AWS. Request for an AWS Snowball device and transfer your exported VMs to it. Once the VMs are on Amazon S3, import the VMs into Amazon EC2 instances using the VM Import/Export service.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Server Migration Service</strong> automates the migration of your on-premises VMware vSphere, Microsoft Hyper-V/SCVMM, and Azure virtual machines to the AWS Cloud. AWS SMS incrementally replicates your server VMs as cloud-hosted Amazon Machine Images (AMIs) ready for deployment on Amazon EC2. Working with AMIs, you can easily test and update your cloud-based images before deploying them in production.</p><p>By using AWS SMS to manage your server migrations, you can:</p><p>- Simplify the cloud migration process.</p><p>- Orchestrate multi-server migrations.</p><p>- Test server migrations incrementally.</p><p>- Support the most widely used operating systems.</p><p>- Minimize downtime.</p><p><strong>AWS Snowball</strong> is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data into and out of the AWS cloud. Using Snowball addresses common challenges with large-scale data transfers including high network costs, long transfer times, and security concerns.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_sms.png\"></p><p>AWS Snowball moves terabytes of data in about a week. You can use it to move things like databases, backups, archives, healthcare records, analytics datasets, IoT sensor data and media content, especially when network conditions prevent realistic timelines for transferring large amounts of data both into and out of AWS. Data from the Snowball device will be imported to your selected Amazon S3 bucket.</p><p>AWS<strong> VM Import/Export</strong> enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances and export them back to your on-premises environment. VM Import/Export is available at no additional charge beyond standard usage charges for Amazon EC2 and Amazon S3. As part of the import process, VM Import will convert your VM into an Amazon EC2 AMI, which you can use to run Amazon EC2 instances.</p><p>Therefore, the correct answer is: <strong>Use AWS Server Migration Service to migrate the mission-critical virtual machines to AWS. Request for an AWS Snowball device and transfer your exported VMs to it. Once the VMs are on Amazon S3, import the VMs into Amazon EC2 instances using the VM Import/Export service.</strong></p><p>The option that says: <strong>Request for a 1 Gbps AWS Direct Connect connection from the on-premises data center to AWS. Create a private virtual interface on Direct Connect. Migrate the virtual machines to AWS using the AWS Server Migration Service (SMS)</strong> is incorrect. Although this is a fast solution that can finish the migration within the required time frame, this is not the most cost-effective solution. As stated in the question, upgrading the Internet connection (and probably requesting a new connection) is costly for the company. Moreover, the requirement is primarily to migrate to virtual machines and not to establish a fast, dedicated network connection from the on-premises network to AWS.</p><p>The option that says: <strong>Deploy the AWS Agentless Discovery connector on the company VMware vCenter to assess each application. With the information gathered, refactor each application to run on AWS services or using AWS Marketplace solutions</strong> is incorrect. This requires a lot of effort for planning since you will have to match each application to the AWS service. This will require changes in the application and can cause possible interruptions during the migration.</p><p>The option that says<strong>: Create an export of your virtual machines during out of office hours. Use the AWS Transfer service to securely upload the VMs to Amazon S3 using the SFTP protocol. Import the VMs into Amazon EC2 instances using the VM Import/Export service</strong> is incorrect. Although this is possible, with the 12 Mbps Internet connection, it will take more than three months to upload all the 40 TB worth of VM storage to an Amazon S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/projects/migrate-petabyte-scale-data/services-costs/\">https://aws.amazon.com/getting-started/projects/migrate-petabyte-scale-data/services-costs/</a></p><p><a href=\"https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html\">https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html</a></p><p><a href=\"https://aws.amazon.com/ec2/vm-import/\">https://aws.amazon.com/ec2/vm-import/</a></p><p><br></p><p><strong>Check out this AWS Server Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-server-migration-service-sms/?src=udemy\">https://tutorialsdojo.com/aws-server-migration-service-sms/</a></p></div>"
	},
	{
		"question": "A leading mobile game company is planning to host their GraphQL API in AWS which will be heavily used for their massively multiplayer online role-playing games (MMORPGs) for 3 years or more. You are assigned to prepare the architecture of the entire system and to ensure consistent connection and faster loading times for their players across the globe. \n\nWhich of the following is the most cost-effective solution that you can implement in this scenario?",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use Reserved EC2 Instances to host the GraphQL API and CloudFront for web distribution of the static assets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use an Auto Scaling group of Spot EC2 Instances to host the GraphQL API and an API Gateway for the web distribution of the static assets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use an Auto Scaling group of Spot EC2 Instances to host the GraphQL API and<strong> </strong>CloudFront for web distribution of the static assets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use On-Demand EC2 Instances to host the GraphQL API and CloudFront for web distribution of the static assets.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Reserved Instances</strong> provide you with significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. Reserved Instances are not physical instances, but rather a billing discount applied to the use of On-Demand Instances in your account. These On-Demand Instances must match certain attributes, such as instance type and Region, in order to benefit from the billing discount.</p><p>Savings Plans also offer significant savings on your Amazon EC2 costs compared to On-Demand Instance pricing. With Savings Plans, you make a commitment to a consistent usage amount, measured in USD per hour. This provides you with the flexibility to use the instance configurations that best meet your needs and continue to save money, instead of making a commitment to a specific instance configuration.</p><p>Reserved Instances are best to use for these scenarios:</p><p>- Applications that have been in use for years and that you plan to continue to use.</p><p>- Applications with steady state or predictable usage.</p><p>- Applications that require reserved capacity.</p><p>- Users who want to make upfront payments to further reduce their total computing costs.</p><p>Since the game company is planning to use this application for 3 years or more, the best and the most cost-effective type of EC2 to use is a Reserved Instance.</p><p>Therefore the correct answer is: <strong>Use Reserved EC2 Instances to host the GraphQL API and CloudFront for web distribution of the static assets</strong>.</p><p>The option that says: <strong>Use On-Demand EC2 Instances to host the GraphQL API and CloudFront for web distribution of the static assets</strong> is incorrect. An On-Demand instance may be a valid option but it costs more than Reserved instances.</p><p>The option that says: <strong>Use an Auto Scaling group of Spot EC2 Instances to host the GraphQL API and an API Gateway for the web distribution of the static assets</strong> is incorrect. You cannot use a Spot instance here as you need to provide a consistent service to your users without any interruption.</p><p>The option that says: <strong>Use an Auto Scaling group of Spot EC2 Instances to host the GraphQL API and CloudFront for web distribution of the static assets</strong> is incorrect. You cannot use a Spot instance here as you need to provide a consistent service to your users without any interruption.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/pricing/reserved-instances/\">https://aws.amazon.com/ec2/pricing/reserved-instances/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A finance company plans to launch a new website to allow users to view tutorials that promote the proper usage of their mobile app. The website contains static media files that are stored on a private Amazon S3 bucket while the dynamic contents are hosted on an AWS Fargate cluster. The AWS Fargate tasks are accepting traffic behind an Application Load Balancer (ALB). To improve user experience, the static and dynamic content are placed behind a CloudFront distribution. An Amazon Route 53 Alias record has already been created to point the website URL to the CloudFront distribution. The company wants to ensure that access to both static and dynamic content is done through CloudFront only.</p><p>Which of the following options should the Solutions Architect implement to meet this requirement? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a network ACL that will allow connections from Amazon CloudFront only. Associate the NACL to the Application Load Balancer subnets.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use CloudFront to add a custom header to all origin requests. Using AWS WAF, create a web rule that denies all requests without this custom header. Associate the web ACL to the Application Load Balancer.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the Amazon S3 bucket ACL to block all access except requests coming from the Amazon CloudFront distribution.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Configure the S3 bucket policy to only access from the OAI.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use CloudFront to add a custom header to all origin requests. Using AWS WAF, create a web rule that denies all requests without this custom header. Associate the web ACL to the CloudFront distribution.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS WAF</strong> is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront, and lets you control access to your content. Based on conditions that you specify, such as the values of query strings or the IP addresses that requests originate from, CloudFront responds to requests either with the requested content or with an HTTP status code <code>403(Forbidden)</code>. You can also configure CloudFront to return a custom error page when a request is blocked.</p><p>After you create an <strong>AWS WAF web access control list (web ACL)</strong>, create or update a web distribution to associate the distribution with the web ACL. You can associate as many CloudFront distributions as you want with the same web ACL or with different web ACLs.</p><p>On <strong>Amazon CloudFront</strong>, you can control user access to your private content in two ways:</p><p>Restrict access to files in CloudFront caches.</p><p>Restrict access to files in your origin by doing one of the following:</p><p>- Set up an origin access identity (OAI) for your Amazon S3 bucket.</p><p>- Configure custom headers for a private HTTP server (a custom origin).</p><p>You can secure the content in your Amazon S3 bucket so that users can access it through CloudFront but cannot access it directly by using Amazon S3 URLs. This prevents someone from bypassing CloudFront and using the Amazon S3 URL to get content that you want to restrict access to. To require that users access your content through CloudFront URLs, you do the following tasks:</p><p>Create a special CloudFront user called an origin access identity and associate it with your CloudFront distribution.</p><p>Give the origin access identity permission to read the files in your bucket.</p><p>Remove permission for anyone else to use Amazon S3 URLs to read the files.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\"></p><p>If you use a custom origin, you can optionally set up custom headers to restrict access. For CloudFront to get your files from a custom origin, the files must be accessible by CloudFront using a standard HTTP (or HTTPS) request. But by using custom headers, you can further restrict access to your content so that users can access it only through CloudFront, not directly. This step isn't required to use signed URLs, but it is recommended. To require that users access content through CloudFront, change the following settings in your CloudFront distributions:</p><p><strong>Origin Custom Headers</strong> - Configure CloudFront to forward custom headers to your origin.</p><p><strong>Viewer Protocol Policy</strong> - Configure your distribution to require viewers to use HTTPS to access CloudFront.</p><p><strong>Origin Protocol Policy</strong> - Configure your distribution to require CloudFront to use the same protocol as viewers to forward requests to the origin.</p><p>The option that says: <strong>Use CloudFront to add a custom header to all origin requests. Using AWS WAF, create a web rule that denies all requests without this custom header. Associate the web ACL to the Application Load Balancer </strong>is correct. After you create an AWS WAF web access control list (web ACL), create or update a web distribution to associate the distribution with the ALB. Associating the Web ACL to the ALB ensures that only requests from CloudFront will reach the ALB. Any request going to the ALB without the custom header will be denied by WAF.</p><p>The option that says: <strong>Create a special CloudFront user called an origin access identity (OAI) and associate it with your distribution. Configure the S3 bucket policy to only access from the OAI</strong> is correct. After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.</p><p>The option that says: <strong>Use CloudFront to add a custom header to all origin requests. Using AWS WAF, create a web rule that denies all requests without this custom header. Associate the web ACL to the CloudFront distribution</strong> is incorrect. If any new requests are going to CloudFront, they won't have the custom header initially so AWS WAF may block the request immediately. This could deny any new connections to CloudFront. Therefore, you need to associate the web ACL to the ALB, which is after CloudFront adds the custom header.</p><p>The option that says: <strong>Create a network ACL that will allow connections from Amazon CloudFront only. Associate the NACL to the Application Load Balancer subnets</strong> is incorrect. This will limit all resources inside the ALB subnets to accept only traffic from the CloudFront distribution. However, there are no fixed IP addresses for Amazon CloudFront and if you manually add AWS IP addresses, you will have to update the NACL as AWS updates its IP pool.</p><p>The option that says: <strong>Configure the Amazon S3 bucket ACL to block all access except requests coming from the Amazon CloudFront distribution</strong> is incorrect. You can't directly configure a bucket ACL to allow access from Amazon CloudFront only. You will need an origin access identity (OAI) for this setup.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-awswaf.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#forward-custom-headers-restrict-access\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#forward-custom-headers-restrict-access</a></p><p><br></p><p><strong>Check out these Amazon CloudFront and AWS WAF Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p></div>"
	},
	{
		"question": "An enterprise software company has just recently started using AWS as their cloud infrastructure. They are building an enterprise proprietary issue tracking system which would be accessed by their customers worldwide. Hence, the CTO carefully instructed you to ensure that the architecture of the issue tracking system is both scalable and highly available to avoid any complaints from the clients. It is expected that the application will have a steady-state usage and the database would be used for online transaction processing (OLTP).\n\nWhich of the following would be the best architecture setup to satisfy the above requirement?",
		"answers": [
			{
				"correct": false,
				"answer": "Use a Dedicated EC2 instance as the application server and Redshift as a petabyte-scale data warehouse service. Use ElastiCache for in-memory data caching for your database to improve performance."
			},
			{
				"correct": false,
				"answer": "Use multiple On-Demand EC2 instances to host the application and a highly scalable DynamoDB for the database. Use ElastiCache for in-memory data caching for your database to improve performance."
			},
			{
				"correct": true,
				"answer": "Use a CloudFormation template to launch an Auto Scaling group of EC2 instances across multiple Availability Zones which are all connected via an ELB to handle the load balancing. Leverage on CloudFront in distributing your static content and a RDS instance with Multi-AZ deployments configuration."
			},
			{
				"correct": false,
				"answer": "Launch an Auto Scaling group of Spot EC2 instances with an ELB in front to handle the load balancing. Leverage on CloudFront in distributing your static content and a RDS instance with Read Replicas."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>It is recommended to use a <strong>CloudFormation</strong> template to launch your architecture in AWS. An Auto Scaling group of EC2 instances across multiple Availability Zones with an ELB in front is a highly available and scalable architecture. In addition, leveraging on CloudFront in distributing your static content improves the load times of the system. Finally, an RDS instance with Multi-AZ deployments configuration can ensure the availability of your database in case one instance goes down.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_steps.png\"></p><p>Therefore, the correct answer is: <strong>Use a CloudFormation template to launch an Auto Scaling group of EC2 instances across multiple Availability Zones which are all connected via an ELB to handle the load balancing. Leverage on CloudFront in distributing your static content and a RDS instance with Multi-AZ deployments configuration</strong>. This offers high availability and scalability for the application.</p><p>The option that says: <strong>Use a Dedicated EC2 instance as the application server and Redshift as a petabyte-scale data warehouse service. Use ElastiCache for in-memory data caching for your database to improve performance</strong> is incorrect. Using Dedicated EC2 instances without Auto Scaling is not a scalable nor a highly available architecture. In addition, RedShift is not applicable to OLTP but rather with OLAP.</p><p>The option that says: <strong>Launch an Auto Scaling group of Spot EC2 instances with an ELB in front to handle the load balancing. Leverage on CloudFront in distributing your static content and an RDS instance with Read Replicas</strong> is incorrect. Spot EC2 instances are not suitable for steady state usage and Read Replicas only provide limited availability compared to Multi-AZ Deployments.</p><p>The option that says: <strong>Use multiple On-Demand EC2 instances to host the application and a highly scalable DynamoDB for the database. Use ElastiCache for in-memory data caching for your database to improve performance</strong> is incorrect. This does not use Auto Scaling and is not deployed across multiple Availability Zones.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf\">https://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/launchwizard/latest/userguide/launch-wizard-best-practices.html\">https://docs.aws.amazon.com/launchwizard/latest/userguide/launch-wizard-best-practices.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/oracle-database-aws-best-practices/architecting-for-high-availability.html\">https://docs.aws.amazon.com/whitepapers/latest/oracle-database-aws-best-practices/architecting-for-high-availability.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A leading fast-food chain has recently adopted a hybrid cloud infrastructure that extends its data centers into AWS Cloud. The solutions architect has been tasked to allow on-premises users, who are already signed in using their corporate accounts, to manage AWS resources without creating separate IAM users for each of them. This is to avoid having two separate login accounts and memorizing multiple credentials.</p><p>Which of the following is the best way to handle user authentication in this hybrid architecture?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Retrieve AWS temporary security credentials with Web Identity Federation using STS and AssumeRoleWithWebIdentity to enable users to log in to the AWS console."
			},
			{
				"correct": true,
				"answer": "Authenticate using your on-premises SAML 2.0-compliant identity provider (IDP), retrieve temporary credentials using STS, and grant federated access to the AWS console via the AWS single sign-on (SSO) endpoint using a browser."
			},
			{
				"correct": false,
				"answer": "Authenticate through your on-premises SAML 2.0-compliant identity provider (IDP) using STS and AssumeRoleWithWebIdentity to retrieve temporary security credentials, which enables your users to log in to the AWS console using a browser."
			},
			{
				"correct": false,
				"answer": "Retrieve temporary AWS credentials with OAuth 2.0 to enable your members to log in to the AWS Console."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, you need to provide temporary access to AWS resources to the existing users but you should not create new IAM users for them, to avoid having to maintain two login accounts. This means that you need to setup a single-sign on authentication for your users so they only need to sign-in once in their on-premises network and can also access AWS cloud at the same time.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap.png\"></p><p>You can use a role to configure your SAML 2.0-compliant identity provider (IdP) and AWS to permit your federated users to access the AWS Management Console. The role grants the user permissions to carry out tasks in the console.</p><p>Therefore, the correct answer is: <strong>Authenticate using your on-premises SAML 2.0-compliant identity provider (IDP), retrieving temporary credentials using STS, and granting federated access to the AWS console via the AWS single sign-on (SSO) endpoint using a browser. </strong>It gives federated access to the users to AWS resources by using SAML 2.0 identity provider and it uses on-premises single sign-on (SSO) endpoint to authenticate users which gives them access tokens prior to providing the federated access.</p><p>The option that says:<strong> Retrieve temporary AWS credentials with OAuth 2.0 to enable your members to log in to the AWS Console</strong> is incorrect. OAuth 2.0 is not applicable in this scenario. We are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google, etc.</p><p>The option that says:<strong> Authenticate through your on-premises SAML 2.0-compliant identity provider (IDP) using STS and AssumeRoleWithWebIdentity to retrieve temporary security credentials, which enables your users to log in to the AWS console using a browser</strong> is incorrect. The use of AssumeRoleWithWebIdentity is wrong which is only for Web Identity Federation (Facebook, Google, and other social logins). Even though it uses SAML 2.0 identity provider, the requirement is to provide a single sign-on to users which means that the users should not sign in to AWS console using any security credentials but through their corporate identity provider.</p><p>The option that says:<strong> Retrieve AWS temporary security credentials with Web Identity Federation using STS and AssumeRoleWithWebIdentity to enable users to log in to the AWS console</strong> is incorrect. We are not using Web Identity Federation as it is used with public identity providers such as Facebook, Google, etc.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html</a></p><p><a href=\"https://aws.amazon.com/identity/federation/\">https://aws.amazon.com/identity/federation/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/\">https://aws.amazon.com/blogs/security/how-to-establish-federated-access-to-your-aws-resources-by-using-active-directory-user-attributes/</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company is developing a NodeJS application that has an NGINX server for the front end, Elasticsearch, Logstash for log processing, as well as a MongoDB database instance for document management. In order to improve the process of updating the application stack for a newer version, the solutions architect presented two deployment methods to the management: In-place and Disposable deployments.</p><p>Which of the following is true about the In-place and Disposable method?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "An in-place upgrade involves performing application updates on live Amazon EC2 instances. A disposable upgrade, on the other hand, involves rolling out a new set of EC2 instances by terminating older instances."
			},
			{
				"correct": false,
				"answer": "An in-place upgrade involves performing application updates on live Amazon EC2 instances. A disposable upgrade, on the other hand, combines the simplicity of managing AWS infrastructure provided by Elastic Beanstalk and the automation of custom network segmentation provided by AWS CloudFormation."
			},
			{
				"correct": false,
				"answer": "An in-place upgrade involves rolling out a new set of EC2 instances by terminating older instances. A disposable upgrade, on the other hand, involves performing application updates on live Amazon EC2 instances."
			},
			{
				"correct": false,
				"answer": "An in-place upgrade combines the simplicity of managing AWS infrastructure provided by Elastic Beanstalk and the automation of custom network segmentation provided by AWS CloudFormation.  A disposable upgrade, on the other hand, involves rolling out a new set of EC2 instances by terminating older instances."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The deployment services offer two methods to help you update your application stack, namely in-place and disposable. An in-place upgrade involves performing application updates on live Amazon EC2 instances. A disposable upgrade, on the other hand, involves rolling out a new set of EC2 instances by terminating older instances.</p><p>An in-place upgrade is typically useful in a rapid deployment with a consistent rollout schedule. It is designed for sessionless applications. You can still use the in-place upgrade method for stateful applications by implementing a rolling deployment schedule.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codedeploy_rolling.JPG\"></p><p>In contrast, disposable upgrades offer a simpler way to know if your application has unknown dependencies. The underlying EC2 instance usage is considered temporary or ephemeral in nature for the period of deployment until the current release is active. During the new release, a new set of EC2 instances are rolled out by terminating older instances. This type of upgrade technique is more common in an immutable infrastructure.</p><p>Hence, the option that says: <strong>An in-place upgrade involves performing application updates on live Amazon EC2 instances. A disposable upgrade, on the other hand, involves rolling out a new set of EC2 instances by terminating older instances</strong> is the correct answer.</p><p>The option that says: <strong>An in-place upgrade involves rolling out a new set of EC2 instances by terminating older instances. A disposable upgrade, on the other hand, involves performing application updates on live Amazon EC2 instances</strong> is incorrect because the definition of the in-place and disposable upgrades were switched. Remember that an in-place upgrade involves performing application updates on live Amazon EC2 instances. A disposable upgrade, on the other hand, involves rolling out a new set of EC2 instances by terminating older instances.</p><p>The option that says: <strong>An in-place upgrade combines the simplicity of managing AWS infrastructure provided by Elastic Beanstalk and the automation of custom network segmentation provided by AWS CloudFormation. A disposable upgrade, on the other hand, involves rolling out a new set of EC2 instances by terminating older instances</strong> is incorrect because the explanation that is given for in-place upgrade in this option actually describes a Hybrid Deployment model approach.</p><p>The option that says: <strong>An in-place upgrade involves performing application updates on live Amazon EC2 instances. A disposable upgrade, on the other hand, combines the simplicity of managing AWS infrastructure provided by Elastic Beanstalk and the automation of custom network segmentation provided by AWS CloudFormation</strong> is incorrect because the explanation that is given for disposable upgrade in this option actually describes a Hybrid Deployment model approach.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf#page=13\">https://d1.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf#page=13</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A government organization is currently developing a multi-tiered web application prototype that consists of various components for registration, transaction processing, and reporting. All of the components will be using different IP addresses and they are all hosted on one, extra-large EC2 instance as its main server. They will be using S3 as a durable and scalable storage service. For security purposes, the IT manager wants to implement 2 separate SSL certificates for the separate components.</p><p>How can the organization achieve this with a single EC2 instance?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an EC2 instance that has multiple subnets in two separate Availability Zones attached to it and each will have a separate IP address.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Launch an on-demand EC2 instance that has multiple network interfaces with multiple elastic IP addresses.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an EC2 instance with multiple security groups attached to it which contain separate rules for each IP address, including custom rules in the Network ACL.</p>"
			},
			{
				"correct": false,
				"answer": "Create an EC2 instance with a NAT address."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The attributes of a network interface follow it as it's attached or detached from an instance and reattached to another instance.</p><p>When you move a network interface from one instance to another, network traffic is redirected to the new instance. You can also modify the attributes of your network interface, including changing its security groups and managing its IP addresses. Every instance in a VPC has a default network interface, called the primary network interface (eth0). You cannot detach a primary network interface from an instance. You can create and attach additional network interfaces. The maximum number of network interfaces that you can use varies by instance type.</p><p><img src=\"https://media.tutorialsdojo.com/sap_multiple_elastic_ip.png\"></p><p>In this scenario, you basically need to<strong> provide multiple IP addresses to a single EC2 instance.</strong> This can be easily achieved by using an Elastic Network Interface (ENI). An elastic network interface is a logical networking component in a VPC that represents a virtual network card.</p><p><strong>Creating an EC2 instance with multiple security groups attached to it which contain separate rules for each IP address, including custom rules in the Network ACL </strong>is incorrect because a security group is mainly used to control the incoming or outgoing traffic to the instance and doesn't provide multiple IP addresses to an EC2 instance.</p><p><strong>Creating an EC2 instance that has multiple subnets in two separate Availability Zones attached to it and each will have a separate IP address</strong> is incorrect because you cannot place the same EC2 instance in two separate Availability Zones.</p><p><strong>Creating an EC2 instance with a NAT address</strong> is incorrect because a NAT address doesn't provide multiple IP addresses to an EC2 instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/MultipleIP.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A technology company runs an industrial chain orchestration software on the AWS cloud. It consists of a web application tier that is currently deployed on a fixed fleet of Amazon EC2 instances. The database tier is deployed on Amazon RDS. The web and database tiers are deployed in the public and private subnet of the VPC respectively. The company wants to improve the service to make it more cost-effective, scalable, highly available, and should require minimal human intervention.</p><p>Which of the following actions should the solutions architect implement to improve the availability and load balancing of this cloud architecture? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create a CloudFront distribution whose origin points to the private IP addresses of your web servers. Also set up a CNAME record in Route 53 mapped to your CloudFront distribution. "
			},
			{
				"correct": false,
				"answer": "<p>Set up a NAT instance in your VPC. Update your route table by creating a default route via the NAT instance with all subnets associated with it. Configure a DNS A Record in Route 53 pointing to the NAT instance's public IP address.</p>"
			},
			{
				"correct": true,
				"answer": "Place an Application Load Balancer in front of all the web servers. Create a new Alias Record in Route 53 which maps to the DNS name of the load balancer. "
			},
			{
				"correct": false,
				"answer": "Launch a load balancer in front of all the web servers then create a Non-Alias Record in Route 53 which maps to the DNS name of the load balancer. "
			},
			{
				"correct": true,
				"answer": "Create a Non-Alias Record in Route 53 with a Multivalue Answer Routing configuration and add all the IP addresses for your web servers."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Route 53 <em>alias records</em></strong> provide a Route 53–specific extension to DNS functionality. Alias records let you route traffic to selected AWS resources, such as CloudFront distributions and Amazon S3 buckets. They also let you route traffic from one record in a hosted zone to another record.</p><p>Unlike a CNAME record, you can create an alias record at the top node of a DNS namespace, also known as the <em>zone apex</em>. For example, if you register the DNS name <code>tutorialsdojo.com</code>, the zone apex is <code>tutorialsdojo.com</code>. You can't create a CNAME record for <code>tutorialsdojo.com</code>, but you can create an alias record for <code>tutorialsdojo.com</code> that routes traffic to <code><strong>www</strong>.tutorialsdojo.com </code>(take note of the <strong>www</strong> subdomain).</p><p>You can also type the domain name for the resource. For example:</p><p>- CloudFront distribution domain name: dtut0rial5d0j0.cloudfront.net<br>- Elastic Beanstalk environment CNAME: tutorialsdojo.elasticbeanstalk.com<br>- ELB load balancer DNS name:tutorialsdojo-1.us-east-2.elb.amazonaws.com<br>- S3 website endpoint: s3-website.us-east-2.amazonaws.com<br>- Resource record set in this hosted zone: www.tutorialsdojo.com<br>- VPC endpoint: tutorialsdojo.us-east-2.vpce.amazonaws.com<br>- API Gateway custom regional API: d-tut5d0j0c0m.execute-api.us-west-2.amazonaws.com</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_alias.png\"></p><p>Multivalue answer routing lets you configure Amazon Route 53 to return multiple values, such as IP addresses for your web servers, in response to DNS queries. You can specify multiple values for almost any record, but multivalue answer routing also lets you check the health of each resource, so Route 53 returns only values for healthy resources. It's not a substitute for a load balancer, but the ability to return multiple health-checkable IP addresses is a way to use DNS to improve availability and load balancing.</p><p><strong>Placing an Application Load Balancer in front of all the web servers then creating a new Alias Record in Route 53 which maps to the DNS name of the load balancer</strong> is correct because if the web servers are behind an ELB, the load on the web servers will be uniformly distributed which means that if any of the web servers goes offline, the web traffic would be routed to other web servers. In this way, there would be no unnecessary downtime. You can also use Route 53 to set the ALIAS record that points to the ELB endpoint.</p><p><strong>Creating a Non-Alias Record in Route 53 with a Multivalue Answer Routing configuration and adding all the IP addresses for your web servers</strong> is correct. Although a Multivalue answer routing is not a substitute for a load balancer, its ability to return multiple health-checkable IP addresses can still improve the availability and load balancing of your system.</p><p><strong>Creating a CloudFront distribution whose origin points to the private IP addresses of your web servers and also setting up a CNAME record in Route 53 mapped to your CloudFront distribution</strong> is incorrect as it is using Amazon CloudFront, which is directly pointing to the web server as its origin. In case the server or the EC2 instances go down, the entire website would also become unavailable.</p><p>The option that says: <strong>Set</strong> <strong>up a NAT instance in your VPC. Update your route table by creating a default route via the NAT instance with all subnets associated with it. Configure a DNS A Record in Route 53 pointing to the NAT instance's public IP address</strong> is incorrect as a NAT instance is mainly used to allow EC2 instance launched on a private subnet to access the Internet via a public subnet. In addition, the issue is mainly on the web servers which are hosted on the public subnet and not on the private subnet.</p><p>The option that says: <strong>Launch a load balancer in front of all the web servers then create a Non-Alias Record in Route 53 which maps to the DNS name of the load balancer<em> </em></strong>is incorrect. Although it is recommended to use a load balancer in front of your EC2 instances, you need to use an <em>Alias</em> Record in Route 53, and not a Non-Alias Record.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>A financial startup offers flexible short-term loans of up to $5,000 to its users. Their online portal is hosted in AWS which uses S3 for scalable storage, DynamoDB as a NoSQL database, and a fleet of EC2 instances to host their web servers. To meet the financial regulation, the company is required to undergo a compliance audit.</p><p>In this scenario, how will you provide the auditor access to the logs of your AWS resources?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "1. Contact AWS and inform them of the upcoming audit activities.\n2. AWS will grant required access to the third-party auditor to see the logs."
			},
			{
				"correct": false,
				"answer": "1. Create an SNS Topic.\n2. Configure the SNS to send out an email with the attached CloudTrail log files to the auditor's email every time the CloudTrail delivers the logs to S3."
			},
			{
				"correct": true,
				"answer": "1. Enable CloudTrail logging to required AWS resources. \n2. Create an IAM user with read-only permissions to the required AWS resources.\n3. Provide the access credential to the auditor. "
			},
			{
				"correct": false,
				"answer": "1. Create an IAM role that has the required permissions for the auditor.\n2. Attach the roles to the EC2, S3, and DynamoDB."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudTrail</strong> is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain events related to API calls across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account, including API calls made through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This history simplifies security analysis, resource change tracking, and troubleshooting.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_works.jpg\"></p><p>Therefore the correct answer is:</p><p><strong>1. Enable CloudTrail logging to required AWS resources.</strong></p><p><strong>2. Create an IAM user with read-only permissions to the required AWS resources.</strong></p><p><strong>3. Provide the access credential to the auditor.</strong></p><p>The following option is incorrect because you do not need to contact AWS for any audit activities since you can just use CloudTrail:</p><p><strong>1. Contact AWS and inform them of the upcoming audit activities.</strong></p><p><strong>2. AWS will grant required access to the third-party auditor to see the logs.</strong></p><p>You can contact AWS in case you will perform penetration testing to or originating from any of your AWS resources as part of the shared responsibility model, but for audit activities like this, an authorization is not required.</p><p>The following option is incorrect because it is a security risk to send the CloudTrail logs via email:</p><p><strong>1. Create an SNS Topic.</strong></p><p><strong>2. Configure the SNS to send out an email with the attached CloudTrail log files to the auditor's email every time the CloudTrail delivers the logs to S3.</strong></p><p>It is best to keep them stored inside an S3 bucket and just provide read access to the bucket to the auditor.</p><p>The following option is incorrect because an IAM role should be attached to the IAM user of the auditor but the most preferred way to do this is still to use CloudTrail:</p><p><strong>1. Create an IAM role that has the required permissions for the auditor.</strong></p><p><strong>2. Attach the roles to the EC2, S3, and DynamoDB.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudtrail/\">https://aws.amazon.com/cloudtrail/</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-audit-cross-account-roles-using-aws-cloudtrail-and-amazon-cloudwatch-events/\">https://aws.amazon.com/blogs/security/how-to-audit-cross-account-roles-using-aws-cloudtrail-and-amazon-cloudwatch-events/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p></div>"
	},
	{
		"question": "<p>An online gambling site is hosted in two Elastic Compute Cloud (EC2) instances inside a Virtual Private Cloud (VPC) in the same Availability Zone (AZ) but in different subnets. The first EC2 instance is running a database and the other EC2 instance is a web application that fetches data from the database. You are required to ensure that the two EC2 instances can connect with each other in order for your application to work properly. You also need to track historical changes to the security configurations associated to your instances. </p><p>Which of the following options below can meet this requirement? (Select TWO.) </p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use Route 53 to ensure that there is proper routing between the two subnets.</p>"
			},
			{
				"correct": false,
				"answer": "Ensure that the default route is set to a NAT instance or Internet Gateway (IGW)."
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Config to track historical changes to the security configurations associated to your instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Check and configure the network ACL to allow communication between the two subnets. Ensure that the security groups allow the application host to talk to the database on the right port and protocol.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Systems Manager to track historical changes to the security configurations associated to your instances.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS provides two features that you can use to increase security in your VPC: security groups and network ACLs. <strong>Security groups</strong> control inbound and outbound traffic for your instances, and <strong>network ACLs</strong> control inbound and outbound traffic for your subnets. In most cases, security groups can meet your needs; however, you can also use network ACLs if you want an additional layer of security for your VPC.</p><p><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\"></p><p><strong>AWS Config</strong> is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. With Config, you can review changes in configurations and relationships between AWS resources, dive into detailed resource configuration histories, and determine your overall compliance against the configurations specified in your internal guidelines. This enables you to simplify compliance auditing, security analysis, change management, and operational troubleshooting.</p><p>The option that says: <strong>Check and configure the network ACL to allow communication between the two subnets. Ensure that the security groups allow the application host to talk to the database on the right port and protocol</strong> is correct. NACLs and security groups act like firewalls for communication within your instances and subnets.</p><p>The option that says: <strong>Use AWS Config to track historical changes to the security configurations associated to your instances</strong> is correct. AWS Config can help you maintain compliance with your security setting by monitoring and detecting violations on your security groups depending on the rules you have specified.</p><p>The option that says: <strong>Use Route 53 to ensure that there is proper routing between the two subnets </strong>is incorrect. Route 53 can't be used to connect two subnets. You should use Network ACLs and Security Groups instead.</p><p>The option that says: <strong>Ensure that the default route is set to NAT instance or Internet Gateway (IGW)</strong> is incorrect. Neither a NAT instance nor an Internet gateway is needed for the two EC2 instances to communicate.</p><p>The option that says: <strong>Use AWS Systems Manager to track historical changes to the security configurations associated to your instances</strong> is incorrect. Using AWS Systems Manager is not suitable to track historical changes to the security configurations associated to your instances. You have to use AWS Config instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Security.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Security.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html</a></p><p><br></p><p><strong>Check out these Amazon VPC and AWS Config Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p><p><br></p><p><strong>Learn more about AWS Config in this 19-minute video:</strong></p><p><a href=\"https://youtu.be/QbA0859qNI8\">https://youtu.be/QbA0859qNI8</a></p></div>"
	},
	{
		"question": "<p>An online stock trading application is deployed to multiple Availability Zones in the us-east-1 region (N. Virginia) and uses RDS to host the database. Considering the massive financial transactions that the trading application handles, the company has hired you to be a consultant to make sure that the system is scalable, highly-available, and disaster resilient. In the event of failure, the Recovery Time Objective (RTO) must be less than 2 hours and the Recovery Point Objective (RPO) must be 10 minutes to meet the compliance requirements set by the regulators.</p><p>In this scenario, which Disaster Recovery strategy can be used to achieve the RTO and RPO requirements in the event of system failure?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Configure your database to use synchronous master-slave replication between multiple Availability Zones."
			},
			{
				"correct": false,
				"answer": "Take 15-minute database backups stored in Glacier with transaction logs stored in S3 every 5 minutes."
			},
			{
				"correct": false,
				"answer": "Store hourly database backups to an EC2 instance store volume with transaction logs stored in an S3 bucket every 5 minutes."
			},
			{
				"correct": true,
				"answer": "<p>Take hourly database backups and export to an S3 bucket with transaction logs stored in S3 every 5 minutes. Set up a Cross-Region Replication (CRR) to another AWS Region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Point-in-time recovery (PITR)</strong> is the process of restoring a database to the state it was in at a specified date and time.</p><p>When automated backups are turned on for your DB instance, Amazon RDS automatically performs a full daily snapshot of your data. The snapshot occurs during your preferred backup window. It also captures transaction logs to Amazon S3 every 5 minutes (as updates to your DB instance are made). Archiving the transaction logs is an important part of your DR process and PITR. When you initiate a point-in-time recovery, transactional logs are applied to the most appropriate daily backup in order to restore your DB instance to the specific requested time.</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_replication.jpg\"></p><p>With S3 Cross-Region Replication (CRR), you can replicate objects (and their respective metadata and object tags) into other AWS Regions for reduced latency, compliance, security, disaster recovery, and other use cases. S3 CRR is configured to a source S3 bucket and replicates objects into a destination bucket in another AWS Region.</p><p>Amazon S3 CRR automatically replicates data between buckets across different AWS Regions. With CRR, you can set up replication at a bucket level, a shared prefix level, or an object level using S3 object tags. You can use CRR to provide lower-latency data access in different geographic regions. CRR can also help if you have a compliance requirement to store copies of data hundreds of miles apart. You can use CRR to change account ownership for the replicated objects to protect data from accidental deletion.</p><p>In this scenario, you have to use durable storage and database backups to satisfy the RTO and RPO requirements. Hence, the correct answer is the option that says: <strong>Take hourly database backups and export to an S3 bucket with transaction logs stored in S3 every 5 minutes. Set up a Cross-Region Replication (CRR) to another AWS Region</strong> as this solution meets the 2-hour RTO as well as the 10-minute RPO requirement.</p><p><strong>Storing hourly database backups to an EC2 instance store volume with transaction logs stored in an S3 bucket every 5 minutes</strong> is incorrect because an instance store volume is ephemeral and it is not suitable to store the database backups.</p><p><strong>Taking 15-minute database backups stored in Glacier with transaction logs stored in S3 every 5 minutes</strong> is incorrect because the RTO is at least 3 hours, which means that Amazon Glacier is not the ideal solution to use. Note that the standard retrieval time for Glacier is 3 to 5 hours and with that time, you will surely miss your RTO.</p><p><strong>Configuring your database to use synchronous master-slave replication between multiple Availability Zones</strong> is incorrect because it provides a highly available architecture but it doesn't provide any durable storage nor DB snapshots.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws\">https://www.slideshare.net/AmazonWebServices/disaster-recovery-options-with-aws</a></p><p><a href=\"https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/\">https://aws.amazon.com/blogs/database/implementing-a-disaster-recovery-strategy-with-amazon-rds/</a></p><p><br></p><p><strong>Check out this AWS Well-Architected Framework Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-well-architected-framework-disaster-recovery/?src=udemy\">https://tutorialsdojo.com/aws-well-architected-framework-disaster-recovery/</a></p><p><br></p><p><strong>RPO and RTO Explained:</strong></p><p><a href=\"https://youtu.be/rD3nBaS3OG4\">https://youtu.be/rD3nBaS3OG4</a></p></div>"
	},
	{
		"question": "<p>A BPO company uses a multitiered, java-based content management system (CMS) hosted on an on-premises data center. The CMS has a JBoss Application server present in the application tier. The database tier consists of an Oracle database which is regularly backed up to S3 using the Oracle RMAN backup utility. The application's static files and content are kept on a 512 GB Storage Gateway volume which is attached to the application server via an iSCSI interface. The solutions architect was tasked to create a disaster recovery solution for the application and its data.</p><p>Which AWS-based disaster recovery strategy will give you the best RTO?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Provision EC2 servers for both your JBoss application and Oracle database, and then restore the database backups from an S3 bucket. Also provision an EBS volume containing static content obtained from Storage Gateway, and attach the volume to the JBoss EC2 server."
			},
			{
				"correct": false,
				"answer": "Provision EC2 servers for both your JBoss application and Oracle database, and then restore the database backups from an S3 bucket. Attach an AWS Storage Gateway running on Amazon EC2 as an iSCSI volume to the JBoss EC2 server to access the static content."
			},
			{
				"correct": false,
				"answer": "Use RDS for your Oracle database and EC2 for the JBoss application server. Restore the RMAN Oracle backups from Amazon Glacier, and provision an EBS volume containing static content obtained from Storage Gateway. The volume will be attached to the JBoss EC2 server."
			},
			{
				"correct": false,
				"answer": "Provision EC2 servers for both your JBoss application and Oracle database, and then restore the database backups from an S3 bucket. Use an AWS Storage Gateway-VTL running on Amazon EC2 as your source for restoring static content."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Recovery Manager (RMAN)</strong> is an Oracle Database client that performs backup and recovery tasks on your databases and automates the administration of your backup strategies. It greatly simplifies backing up, restoring, and recovering database files.</p><p>By using stored volumes, you can store your primary data locally, while asynchronously backing up that data to AWS. Stored volumes provide your on-premises applications with low-latency access to their entire datasets. At the same time, they provide durable, offsite backups. You can create storage volumes and mount them as iSCSI devices from your on-premises application servers. Data written to your stored volumes is stored on your on-premises storage hardware. This data is asynchronously backed up to Amazon S3 as Amazon Elastic Block Store (Amazon EBS) snapshots.</p><p>If you are restoring an <strong>AWS Storage Gateway volume snapshot</strong>, you can choose to restore the snapshot as an AWS Storage Gateway volume or as an Amazon EBS volume. AWS Backup integrates with both services, and any AWS Storage Gateway snapshot can be restored to either an AWS Storage Gateway volume or an Amazon EBS volume.</p><p>The option that says: <strong>Provision EC2 servers for both your JBoss application and Oracle database, and then restore the database backups from an S3 bucket. Also provision an EBS volume containing static content obtained from Storage Gateway, and attach the volume to the JBoss EC2 server</strong> is correct because it deploys the Oracle database on an EC2 instance by restoring the backups from S3 which can provide a faster recovery time, and it generates the EBS volume of static content from Storage Gateway.</p><p>The option that says: <strong>Use RDS for your Oracle database and EC2 for the JBoss application server. Restore the RMAN Oracle backups from Amazon Glacier, and provision an EBS volume containing static content obtained from Storage Gateway. The volume will be attached to the JBoss EC2 server</strong> is incorrect because restoring the backups from Amazon Glacier will be slower than S3 and will not meet the RTO.</p><p>The option that says: <strong>Provision EC2 servers for both your JBoss application and Oracle database, and then restore the database backups from an S3 bucket. Attach an AWS Storage Gateway running on Amazon EC2 as an iSCSI volume to the JBoss EC2 server to access the static content</strong> is incorrect because there is no need to attach the Storage Gateway as an iSCSI volume; you can just easily and quickly create an EBS volume from the Storage Gateway. Then, you can generate snapshots from the EBS volumes for better recovery time.</p><p>The option that says: <strong>Provision EC2 servers for both your JBoss application and Oracle database, and then restore the database backups from an S3 bucket. Use an AWS Storage Gateway-VTL running on Amazon EC2 as your source for restoring static content</strong> is incorrect as restoring the content from Virtual Tape Library will not fit into the RTO.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.CommonDBATasks.RMAN.html</a></p><p><a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/restoring-storage-gateway.html\">https://docs.aws.amazon.com/aws-backup/latest/devguide/restoring-storage-gateway.html</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p></div>"
	},
	{
		"question": "<p>An international humanitarian aid organization has a requirement to store 20 TB worth of scanned files for their relief operations which can grow to up to a total of 50 TB of data. There is also a requirement to have a website with a search feature in place that can be used to easily find a certain item through the thousands of scanned files. The new system is expected to run for more than 3 years. </p><p>Which of the following is the most cost-effective option in implementing the search feature in their system?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set up a new S3 bucket with standard storage to store and serve the scanned files. Use CloudSearch for query processing and use Elastic Beanstalk to host the website across multiple availability zones.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use S3 for both storing and searching the scanned files by utilizing the native search capabilities of S3.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use EFS to store and serve the scanned files. Install a 3rd-party search software<strong> </strong>on an Auto Scaling group of On-Demand EC2 Instances and an Elastic Load Balancer.</p>"
			},
			{
				"correct": false,
				"answer": "Design the new system on a CloudFormation template. Use an EC2 instance running NGINX web server and an open source search application. Launch multiple standard EBS volumes with RAID configuration to store the scanned files with a search index."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Storage Service (S3)</strong> is an excellent object-based storage that is highly durable and scalable. However, its native search capability is not effective. Hence, you need to have a separate service to handle the search feature.</p><p><strong>Amazon CloudSearch</strong> is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application. Amazon CloudSearch supports 34 languages and popular search features such as highlighting, autocomplete, and geospatial search.</p><p>The option that says: <strong>Set up a new S3 bucket with standard storage to store and serve the scanned files. Use CloudSearch for query processing and use Elastic Beanstalk to host the website across multiple availability zones</strong> is correct because it uses S3 to store the images, which is a durable and scalable solution. It also uses CloudSearch for query processing, and with a multi-AZ implementation, it achieves high availability.</p><p>The option that says: <strong>Use EFS to store and serve the scanned files. Install a 3rd-party search software on an Auto Scaling group of On-Demand EC2 Instances and an Elastic Load Balancer</strong> is incorrect. It is stated in the scenario that the new system is expected to run for more than 3 years which means that using Reserved EC2 instances would be a more cost-effective choice than using On-Demand instances. In addition, purchasing and installing a 3rd-party search software might be more expensive than just using Amazon CloudSearch.</p><p>The option that says: <strong>Design the new system on a CloudFormation template. Use an EC2 instance running NGINX web server and an open source search application. Launch multiple standard EBS volumes with RAID configuration to store the scanned files with a search index</strong> is incorrect because a system composed of RAID configuration of EBS volumes is not a durable and scalable solution compared to S3.</p><p>The option that says: <strong>Use S3 for both storing and searching the scanned files by utilizing the native search capabilities of S3</strong> is incorrect as the native search capability of S3 is not effective. It is better to use CloudSearch or another service that provides search functionality.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudsearch/\">https://aws.amazon.com/cloudsearch/</a></p><p><a href=\"https://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html\">https://docs.aws.amazon.com/cloudsearch/latest/developerguide/what-is-cloudsearch.html</a></p><p><br></p><p><strong>Check out this Amazon CloudSearch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudsearch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudsearch/</a></p></div>"
	},
	{
		"question": "<p>An analytics company plans to create a self-service solution that will provide a safe and cost-effective way for the data scientists to access Amazon SageMaker on the company AWS accounts. The data scientists have limited knowledge of AWS cloud so the complex setup requirements for their ML models should not be exposed to them. The company wants the data scientists to be able to launch a Jupyter notebook instance as they need it. The data at rest on the storage volume of the notebook instance must be encrypted with a preconfigured AWS KMS key.</p><p>Which of the following solutions will meet the company requirements with the LEAST amount of operational overhead?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Write an AWS CloudFormation template that contains the <code>AWS::SageMaker::NotebookInstance</code> resource type to launch a Jupyter notebook instance with a preconfigured AWS KMS key. Create Mappings on the CloudFormation to map simpler parameter names for instance sizes such as Small, Medium, Large. Reference the URL of the notebook instance on the Outputs section of the template. Create a portfolio in AWS Service Catalog and upload the template to be shared with the IAM role of the data scientists.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write a custom AWS CLI script that will take step-by-step instructions of the input parameters from the data scientist for the requested Jupyter notebook instance with the pre-configured AWS KMS key. Upload this script to a shared Amazon S3 bucket for distribution with the data scientists. Have the data scientists execute the script locally on their computers.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon S3 bucket with website hosting enabled. Create a simple form as a front-end website hosted on the S3 bucket that allows the data scientist to input their request for Jupyter notebook creation. Send the request to an Amazon API Gateway that will invoke an AWS Lambda function with an IAM role permission to create the Jupyter notebook instance with a preconfigured AWS KMS key. Have the Lambda function reply the URL of the notebook instance for display on the front-end website.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write an AWS CloudFormation template that contains the <code>AWS::SageMaker::NotebookInstance</code> resource type to launch a Jupyter notebook instance with a preconfigured AWS KMS key. On the Outputs section of the CloudFormation template, reference the URL of the notebook instance. Rename this template to be more user-friendly and upload it to a shared Amazon S3 bucket for distribution to the data scientists.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Service Catalog</strong> allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application architectures. AWS Service Catalog allows you to centrally manage deployed IT services and your applications, resources, and metadata.</p><p>With AWS Service Catalog, you define your own catalog of AWS services and AWS Marketplace software and make them available for your organization. Then, end users can quickly discover and deploy IT services using a self-service portal.</p><p><strong>Amazon SageMaker</strong> is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows.</p><p>You can easily create a self-service, secured data science using Amazon SageMaker, AWS Service Catalog, and AWS Key Management Service (KMS). Using AWS Service Catalog you can use a pre-configured AWS KMS key to encrypt data at rest on the machine learning (ML) storage volume that is attached to your notebook instance without ever exposing the complex, unnecessary details to data scientists. ML storage volume encryption is enforced by an AWS Service Catalog product that is pre-configured by centralized security and/or infrastructure teams.</p><p><img src=\"https://media.tutorialsdojo.com/sap_service_catalog.png\"></p><p>Therefore, the correct answer is: <strong>Write an AWS CloudFormation template that contains the </strong><code><strong>AWS::SageMaker::NotebookInstance</strong></code><strong> resource type to launch a Jupyter notebook instance with a preconfigured AWS KMS key. Create Mappings on the CloudFormation to map simpler parameter names for instance sizes such as Small, Medium, Large. Reference the URL of the notebook instance on the Outputs section of the template. Create a portfolio in AWS Service Catalog and upload the template to be shared with the IAM role of the data scientists.</strong> This solution has less operational overhead because you just need to maintain a single template. Additionally, AWS Service Catalog allows end-users to quickly discover and deploy IT services using a self-service portal.</p><p>The option that says: <strong>Create an Amazon S3 bucket with website hosting enabled. Create a simple form as a front-end website hosted on the S3 bucket that allows the data scientist to input their request for Jupyter notebook creation. Send the request to an Amazon API Gateway that will invoke an AWS Lambda function with an IAM role permission to create the Jupyter notebook instance with a preconfigured AWS KMS key. Have the Lambda function reply the URL of the notebook instance for display on the front-end website</strong> is incorrect. Although this is possible, this requires a lot of operational overhead as you need to write a custom website and write a proper Lambda function that has the appropriate code to create the needed resources. Additionally, as the company has several accounts, you will need to create the Lambda function for each AWS account.</p><p>The option that says: <strong>Write an AWS CloudFormation template that contains the </strong><code><strong>AWS::SageMaker::NotebookInstance</strong></code><strong> resource type to launch a Jupyter notebook instance with a preconfigured AWS KMS key. On the Outputs section of the CloudFormation template, reference the URL of the notebook instance. Rename this template to be more user-friendly and upload it to a shared Amazon S3 bucket for distribution to the data scientists</strong> is incorrect. This is possible, however, it is not very user-friendly as the users need to download the appropriate CloudFormation template from Amazon S3 and then upload it to CloudFormation. They will then need to input the needed parameters for the creation of their Jupyter notebook instance.</p><p>The option that says: <strong>Write a custom AWS CLI script that will take step-by-step instructions of the input parameters from the data scientist for the requested Jupyter notebook instance with the pre-configured AWS KMS key. Upload this script to a shared Amazon S3 bucket for distribution with the data scientists. Have the data scientists execute the script locally on their computers</strong> is incorrect. This is also possible, however, this is not very user-friendly and does not meet the company requirement of a self-service portal solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/enable-self-service-secured-data-science-using-amazon-sagemaker-notebooks-and-aws-service-catalog/\">https://aws.amazon.com/blogs/mt/enable-self-service-secured-data-science-using-amazon-sagemaker-notebooks-and-aws-service-catalog/</a></p><p><a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html\">https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html</a></p><p><a href=\"https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\">https://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html</a></p><p><br></p><p><strong>Check out these Amazon SageMaker and AWS Service Catalog Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sagemaker/?src=udemy\">https://tutorialsdojo.com/amazon-sagemaker/</a></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p></div>"
	},
	{
		"question": "<p>A company is using AWS Managed Active Directory Service to host the company AD in the AWS Cloud with a custom AD domain name private.tutorialsdojo.com. A pair of domain controllers are launched with the default configuration inside the VPC. A VPC interface endpoint was also created for the Amazon Kinesis using AWS Private Link to allow instances to connect to Kinesis service endpoints from inside the VPC. The solutions architect launched several EC2 instances in the VPC, however, the instances were not able to resolve the company’s custom AD domain name.</p><p>Which of the following steps should the Solutions Architect implement to allow the instances to resolve both AWS VPC endpoints and the AWS Managed Microsoft AD domain’s FQDN? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an inbound endpoint on the Amazon Route 53 console. Set the AmazonProvidedDNS as the DNS resolver for the VPC.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reconfigure the DNS service on every client on the VPC to split DNS queries. Use the Active Directory servers for the custom AD domain and the VPC resolver for all other DNS queries.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an outbound endpoint on the Amazon Route 53 console. Set the AmazonProvidedDNS as the DNS resolver for the VPC.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a conditional forwarder inside the endpoint to forward any queries for private.tutorialsdojo.com to the IP addresses of the two domain controllers.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a forwarding rule inside the endpoint to forward any queries for private.tutorialsdojo.com to the IP addresses of the two domain controllers.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you create a VPC using Amazon VPC, <strong>Route 53 Resolver</strong> automatically answers DNS queries for local VPC domain names for EC2 instances (ec2-192-0-2-44.compute-1.amazonaws.com) and records in private hosted zones (private.tutorialsdojo.com). For all other domain names, Resolver performs recursive lookups against public name servers.</p><p>You also can integrate DNS resolution between Resolver and DNS resolvers on your network by configuring forwarding rules. Your network can include any network that is reachable from your VPC, such as the following:</p><p>The VPC itself</p><p>Another peered VPC</p><p>An on-premises network that is connected to AWS with AWS Direct Connect, a VPN, or a network address translation (NAT) gateway.</p><p>A Route 53 Resolver Endpoint is a customer-managed resolver consisting of one or more Elastic Network Interfaces (ENIs) deployed on your VPC. Resolver Endpoints are classified into two types:</p><p><strong>Inbound Endpoint -</strong> provides DNS resolution of AWS resources, such as EC2 instances, for your corporate network.</p><p><strong>Outbound Endpoint</strong> - provides resolution of specific DNS names that you configure using forwarding rules to your VPC.</p><p>Outbound Resolver Endpoints host Forwarding Rules that forward queries for specified domain names to specific IP addresses. You create forwarding rules when you want to forward DNS queries for specified domain names to DNS resolvers on your network.</p><p>To forward selected queries, you create Resolver rules that specify the domain names for the DNS queries that you want to forward (such as example.com), and the IP addresses of the DNS resolvers on your network that you want to forward the queries to. If a query matches multiple rules (tutorialsdojo.com, portal.tutorialsdojo.com), the Resolver chooses the rule with the most specific match (portal.tutorialsdojo.com) and forwards the query to the IP addresses that you specified in that rule.</p><p><img src=\"https://media.tutorialsdojo.com/sap_outbound_resolver.png\"></p><p>When <strong>Outbound Endpoint</strong> and <strong>Forwarding Rule</strong> are created, any resource in the VPC that queries the AmazonProvidedDNS as its DNS resolver is able to seamlessly resolve for AWS Managed Microsoft AD domain’s FQDN, as well as any AWS resources on the VPC such as (interface) VPC Endpoints.</p><p>The option that says: <strong>Create an outbound endpoint on the Amazon Route 53 console. Set the AmazonProvidedDNS as the DNS resolver for the VPC</strong> is correct. You need an outbound endpoint to forward and resolve custom domain names inside your VPC.</p><p>The option that says: <strong>Create a forwarding rule inside the endpoint to forward any queries for private.tutorialsdojo.com to the IP addresses of the two domain controllers</strong> is correct. The forwarding rules will handle queries for a given DNS domain and forward them to the AD server to resolve them.</p><p>The option that says: <strong>Create an inbound endpoint on the Amazon Route 53 console. Set the AmazonProvidedDNS as the DNS resolver for the VPC</strong> is incorrect. An inbound endpoint is used for DNS resolution of AWS services. You need an outbound endpoint for this scenario.</p><p>The option that says: <strong>Reconfigure the DNS service on every client on the VPC to split DNS queries. Use the Active Directory servers for the custom AD domain and the VPC resolver for all other DNS queries</strong> is incorrect. It is not recommended to manually configure resources to split DNS queries. This entails a lot of management overhead. You just need to set the Active Directory servers as the DNS servers and the requests will be forwarded to the VPC resolver accordingly.</p><p>The option that says: <strong>Create a conditional forwarder inside the endpoint to forward any queries for private.tutorialsdojo.com to the IP addresses of the two domain controllers</strong> is incorrect. A conditional forwarder is configured inside the AD servers, not on the Route 53 resolver endpoint.</p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-forwarding-outbound-queries.html#resolver-forwarding-outbound-queries-configuring\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-forwarding-outbound-queries.html#resolver-forwarding-outbound-queries-configuring</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/integrating-your-directory-services-dns-resolution-with-amazon-route-53-resolvers/\">https://aws.amazon.com/blogs/networking-and-content-delivery/integrating-your-directory-services-dns-resolution-with-amazon-route-53-resolvers/</a></p><p><br></p><p><strong>Check out these Amazon VPC and Amazon Route 53 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>A software development company implements cloud best practices on its AWS infrastructure. The solutions architect has been instructed to manage its AWS cloud infrastructure as code to automate its software build, test, and deploy process. The company would like to have the ability to easily deploy exact copies of different versions of your cloud infrastructure, stage changes into different environments, revert back to previous versions, and identify the specific versions running in the VPC. Plus, all new public-facing applications should also have a global content delivery network (CDN) service.</p><p>Which of the following options is the recommended action to meet the company requirement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Use AWS CloudFormation to manage the cloud architecture and CloudFront as the CDN."
			},
			{
				"correct": false,
				"answer": "Use CloudFront as the CDN and Elastic Beanstalk to deploy and manage the cloud architecture. "
			},
			{
				"correct": false,
				"answer": "Use CloudWatch as the CDN and Elastic Beanstalk to deploy and manage the cloud architecture. "
			},
			{
				"correct": false,
				"answer": "Use CloudWatch as the CDN and CloudFormation to manage the cloud architecture. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudFormation</strong> provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts. This file serves as the single source of truth for your cloud environment.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation.png\"></p><p><strong>Amazon CloudFront</strong> is a web service that speeds up the distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p>Therefore, the correct answer is: <strong>Use AWS CloudFormation to manage the cloud architecture and CloudFront as the CDN.</strong></p><p>The option that says: <strong>Using CloudWatch as the CDN and Elastic Beanstalk to deploy and manage the cloud architecture</strong> is incorrect. CloudWatch is not a CDN service.</p><p>The option that says: <strong>Use CloudWatch as the CDN and CloudFormation to manage the cloud architecture</strong> is incorrect. As with the case above, CloudWatch is not a CDN service.</p><p>The option that says: <strong>Use CloudFront as the CDN and Elastic Beanstalk to deploy and manage the cloud architecture</strong> is incorrect. Even though Elastic Beanstalk enables you to quickly deploy and manage applications in the AWS Cloud, you still can't manage the cloud infrastructure as an application code with it. The best option is to use CloudFormation.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudformation/\">https://aws.amazon.com/cloudformation/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>Elastic Beanstalk vs CloudFormation vs OpsWorks vs CodeDeploy:</strong></p><p><a href=\"https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/?src=udemy\">https://tutorialsdojo.com/elastic-beanstalk-vs-cloudformation-vs-opsworks-vs-codedeploy/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A call center company uses its custom application to process and store call recordings in its on-premises data center. The recordings are stored on an NFS share. An offshore team is contracted to transcribe about 2% of the call recordings to be used for quality assurance purposes. It could take up to 3 days before the recordings are completely transcribed. The application that processes the calls and manages the transcription queue is hosted on Linux servers. A web portal is available for the quality assurance team to review the call recordings. After 90 days, the recordings are sent to an offsite location for long-term storage. The company plans to migrate the system to the AWS cloud to reduce storage costs and automate the transcription of the recordings.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Store all recordings in an Amazon S3 bucket and send the object key to an Amazon SQS queue. Create an S3 lifecycle policy to move objects older than 90 days to Amazon S3 Glacier. Create an Auto Scaling group of Amazon EC2 instances to push the recordings to Amazon Mechanical Turk for transcription. Set the Auto Scaling policy based on the number of objects on the SQS queue. Update the web portal so it can be hosted on an Amazon S3 bucket, Amazon API Gateway, and AWS Lambda.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Store all recordings in an Amazon S3 bucket. Create an S3 lifecycle policy to move objects older than 90 days to Amazon S3 Glacier. Create an AWS Lambda trigger to start a transcription job using Amazon Transcribe. Update the web portal so it can be hosted on an Amazon S3 bucket, Amazon API Gateway, and AWS Lambda.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Auto Scaling group of Amazon EC2 instances to host the web portal. Provision an Application Load Balancer in front of the Auto Scaling group. Store all recordings in an Amazon EFS share that is mounted on all instances. After 90 days, archive all call recordings using AWS Backup and use Amazon Transcribe to transcribe the recordings.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store all recordings in an Amazon S3 bucket. Create an S3 lifecycle policy to move objects older than 90 days to Amazon S3 Glacier. Create an AWS Lambda trigger to start a transcription job using Amazon Mechanical Turk. Create an Auto Scaling group of Amazon EC2 instances to host the web portal. Provision an Application Load Balancer in front of the Auto Scaling group.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Transcribe</strong> is an AWS service that makes it easy for customers to convert speech-to-text. Using Automatic Speech Recognition (ASR) technology, customers can choose to use Amazon Transcribe for a variety of business applications, including transcription of voice-based customer service calls, generation of subtitles on audio/video content, and conduct (text-based) content analysis on audio/video content.</p><p><strong>Amazon Transcribe</strong> analyzes audio files that contain speech and uses advanced machine learning techniques to transcribe the voice data into text. You can then use the transcription as you would any text document.</p><p>Amazon Transcribe uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately. Amazon Transcribe can be used to transcribe customer service calls, automate subtitling, and generate metadata for media assets to create a fully searchable archive. Amazon Transcribe automatically adds speaker diarization, punctuation, and formatting so that the output closely matches the quality of manual transcription at a fraction of the time and expense. Speech to text processing can be applied to live audio streams or batch audio content for transcription.</p><p>To transcribe an audio file, Amazon Transcribe uses three operations:</p><p><code><strong>StartTranscriptionJob</strong></code> – Starts a batch job to transcribe the speech in an audio file to text.</p><p><code><strong>ListTranscriptionJobs</strong></code> – Returns a list of transcription jobs that have been started. You can specify the status of the jobs that you want the operation to return. For example, you can get a list of all pending jobs, or a list of completed jobs.</p><p><code><strong>GetTranscriptionJob</strong></code> – Returns the result of a transcription job. The response contains a link to a JSON file containing the results.</p><p>To manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their Amazon S3 Lifecycle. An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p><p><strong>Transition actions</strong>—Define when objects transition to another Using Amazon S3 storage classes. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.</p><p><strong>Expiration actions</strong>—Define when objects expire. Amazon S3 deletes expired objects on your behalf. The lifecycle expiration costs depend on when you choose to expire objects.</p><p><img src=\"https://media.tutorialsdojo.com/sap_step_function_amazon_transcribe.jpg\"></p><p>Therefore, the correct answer is: <strong>Store all recordings in an Amazon S3 bucket. Create an S3 lifecycle policy to move objects older than 90 days to Amazon S3 Glacier. Create an AWS Lambda trigger to start a transcription job using Amazon Transcribe. Update the web portal so it can be hosted on an Amazon S3 bucket, Amazon API Gateway, and AWS Lambda.</strong> Amazon S3 and Glacier offer very cheap object storage for the recordings. Amazon Transcribe offers speech-to-text services which can quickly transcribe the recordings. Amazon S3, API Gateway, and Lambda is a cheap and scalable way to host the web portal.</p><p>The option that says: <strong>Store all recordings in an Amazon S3 bucket. Create an S3 lifecycle policy to move objects older than 90 days to Amazon S3 Glacier. Create an AWS Lambda trigger to start a transcription job using Amazon Mechanical Turk. Create an Auto Scaling group of Amazon EC2 instances to host the web portal. Provision an Application Load Balancer in front of the Auto Scaling group</strong> is incorrect. Although Amazon Mechanical Turk allows you to outsource your processes and jobs to a distributed workforce, Amazon Transcribe can be faster on the transcription process which is a requirement by the company.</p><p>The option that says: <strong>Create an Auto Scaling group of Amazon EC2 instances to host the web portal. Provision an Application Load Balancer in front of the Auto Scaling group. Store all recordings in an Amazon EFS share that is mounted on all instances. After 90 days, archive all call recordings using AWS Backup and use Amazon Transcribe to transcribe the recordings </strong>is incorrect. To use Amazon Transcribe, your recordings must be stored on Amazon S3, not on Amazon EFS. Storing the call recordings to Amazon S3 is cheaper compared to Amazon EFS.</p><p>The option that says: <strong>Store all recordings in an Amazon S3 bucket and send the object key to an Amazon SQS queue. Create an S3 lifecycle policy to move objects older than 90 days to Amazon S3 Glacier. Create an Auto Scaling group of Amazon EC2 instances to push the recordings to Amazon Mechanical Turk for transcription. Set the Auto Scaling policy based on the number of objects on the SQS queue. Update the web portal so it can be hosted on an Amazon S3 bucket, Amazon API Gateway, and AWS Lambda</strong> is incorrect. You do not have to subscribe to Amazon Mechanical Turk to outsource your transcription services because Amazon Transcribe is a managed service by Amazon designed to add speech-to-text capabilities to your applications.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/transcribe/faqs/\">https://aws.amazon.com/transcribe/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/transcribe/latest/dg/API_StartTranscriptionJob.html\">https://docs.aws.amazon.com/transcribe/latest/dg/API_StartTranscriptionJob.html</a></p><p><a href=\"https://docs.aws.amazon.com/transcribe/latest/dg/how-it-works.html\">https://docs.aws.amazon.com/transcribe/latest/dg/how-it-works.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html</a></p><p><br></p><p><strong>Check out the Amazon Transcribe and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-transcribe/?src=udemy\">https://tutorialsdojo.com/amazon-transcribe/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><br></p><p><strong>Amazon S3 and S3 Glacier Overview:</strong></p><p><a href=\"https://youtu.be/1ymyeN2tki4\">https://youtu.be/1ymyeN2tki4</a></p></div>"
	},
	{
		"question": "<p>A company runs a popular blogging platform that is hosted on AWS. Bloggers from all around the world upload millions of entries per month, and the average blog entry size is 300 KB. The access rate to blog entries drops to a negligible level six months after publishing and after a year, bloggers rarely access a blog. The blog entries have a high update rate during the first 3 months after the blogger has published it and this drops to no updates after 6 months. The company wants to use CloudFront to improve the load times of the blogging platform.</p><p>Which of the following is an ideal cloud implementation for this scenario?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "You can use one S3 source bucket that is partitioned according to the month a blog entry was submitted, and store the entry in that partition. Create a CloudFront distribution with access permissions to S3 and is restricted only to it."
			},
			{
				"correct": false,
				"answer": "Create two different CloudFront distributions: one with US-Europe price class for your US/Europe users and another one with all edge locations included for your remaining users."
			},
			{
				"correct": false,
				"answer": "Store two copies of each entry in two different S3 buckets, and let each bucket have its own CloudFront distribution where S3 access is permitted to that CloudFront identity only."
			},
			{
				"correct": false,
				"answer": "Create a CloudFront distribution and set the Restrict Viewer Access Forward Query string to true with a minimum TTL of 0. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can control how long your objects stay in a <strong>CloudFront</strong> cache before CloudFront forwards another request to your origin. Reducing the duration allows you to serve dynamic content. Increasing the duration means your users get better performance because your objects are more likely to be served directly from the edge cache. A longer duration also reduces the load on your origin.</p><p>Typically, <strong>CloudFront</strong> serves an object from an edge location until the cache duration that you specified passes—that is, until the object expires. After it expires, the next time the edge location gets a user request for the object, CloudFront forwards the request to the origin server to verify that the cache contains the latest version of the object.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_partitions.png\"></p><p>Therefore, the correct answer is: <strong>You can use one S3 source bucket that is partitioned according to the month a blog entry was submitted, and store the entry in that partition. Create a CloudFront distribution with access permissions to S3 and is restricted only to it.</strong> The content is only accessed by CloudFront, and if the content is partitioned at the origin based on the month it was uploaded, you can control the cache behavior accordingly and keep only the latest updated content in the CloudFront cache so that it can be accessed with fast load-time, hence, improving the performance.</p><p>The option that says: <strong>Store two copies of each entry in two different S3 buckets, and letting each bucket have its own CloudFront distribution where S3 access is permitted to that CloudFront identity only</strong> is incorrect. Maintaining two separate buckets is not going to improve the load time for the users.</p><p>The option that says: <strong>Create a CloudFront distribution and setting the Restrict Viewer Access Forward Query string to true with a minimum TTL of 0</strong> is incorrect. Setting minimum TTL of 0 will enforce loading of the content from origin every time, even if it has not been updated over 6 months.</p><p>The option that says: <strong>Create two different CloudFront distributions: one with US-Europe price class for your US/Europe users and another one with all edge locations included for your remaining users</strong> is incorrect. The location-wise distribution is not going to improve the load time for the users.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>An advertising company plans to release a new photo-sharing app that will be hosted on the AWS Cloud. The app will store all pictures directly uploaded by users in a single Amazon S3 bucket and users will also be able to view and download their own pictures directly from the Amazon S3 bucket. The solutions architect must ensure the security of the application and it should be able to handle potentially millions of users in the most secure manner.</p><p>How should the solutions architect set up the user registration flow in AWS for this mobile app?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create an IAM user and generate an access key and a secret key to be stored in the mobile app for the IAM user. After applying the appropriate permissions to the S3 bucket policy, use the generated credentials to access S3."
			},
			{
				"correct": false,
				"answer": "<p>Create an IAM user, assign appropriate permissions to it, and generate an access key and a secret key that will be stored in the mobile app and used to access Amazon S3.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Generate long-term credentials using AWS STS and apply the appropriate permissions. Store the credentials in the mobile app, and use them to access Amazon S3.</p>"
			},
			{
				"correct": true,
				"answer": "Store user information in Amazon RDS and create an IAM Role with appropriate permissions. Generate new temporary credentials using the AWS Security Token Service 'AssumeRole' function every time the user uses their mobile app and creates new temporary credentials. These credentials will be stored in the mobile app's memory and will be used to access Amazon S3."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the best solution is to use a combination of an IAM Role and STS for authentication. The STS AssumeRole returns a set of temporary security credentials that you can use to access AWS resources that you might not normally have access to. These temporary credentials consist of an access key ID, a secret access key, and a security token. Typically, you use AssumeRole for cross-account access or federation.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sts_token.JPG\"></p><p>Therefore the correct answer is: <strong>Store user information in Amazon RDS and create an IAM Role with appropriate permissions. Generate new temporary credentials using the AWS Security Token Service 'AssumeRole' function every time the user uses their mobile app and creates new temporary credentials. These credentials will be stored in the mobile app's memory and will be used to access Amazon S3.</strong> It creates an IAM Role with appropriate permissions and then generates temporary security credentials using STS AssumeRole. Then, it generates new credentials when the user runs the app the next time.</p><p>The option that says: <strong>Create an IAM user and generate an access key and a secret key to be stored in the mobile app for the IAM user. After applying the appropriate permissions to the S3 bucket policy, use the generated credentials to access S3</strong> is incorrect. It suggests creating an IAM User, not the IAM Role - which is not a good solution. You should create an IAM Role so that the app can access the AWS Resource using STS AssumeRole.</p><p>The option that says: <strong>Generate long-term credentials using AWS STS and apply the appropriate permissions. Store the credentials in the mobile app, and use them to access Amazon S3</strong> is incorrect. You should always grant short-term or temporary credentials for the mobile application. This option recommends creating long-term credentials.</p><p>The option that says: <strong>Create an IAM user, assign appropriate permissions to it, and generate an access key and a secret key that will be stored in the mobile app and used to access Amazon S3</strong> is incorrect. It does not create the required IAM Role but instead, an IAM user.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A financial company is building a new online document portal system that allows its employees and developers to upload yearly and bi-annual corporate earnings report files to a private S3 bucket in which other confidential corporate files will also be stored. You are working as a Solutions Architect and you were instructed to create the private S3 bucket as well as the IAM users for the application developers to start their work. You assigned the required policies in IAM to the developers that allows them read and write access to the S3 bucket. After a few weeks, they have completed the new online portal and hosted it on a fleet of Spot EC2 instances. One of the application developers created a pre-signed URL that points to the correct S3 bucket and after a few testing, he has successfully uploaded the files from his laptop using the generated URL. He then made the necessary code change to the online portal to generate the pre-signed URL to upload the files in S3. However, after a few days, the development team complained that they cannot upload the files anymore using the online portal.&nbsp; &nbsp;Which of the following options are valid reasons for this behavior? (Select TWO.) </p>",
		"answers": [
			{
				"correct": false,
				"answer": "There was a recent change in the S3 bucket that allows object versioning which invalidates all presigned URLs."
			},
			{
				"correct": true,
				"answer": "<p>The required AWS credentials in the <code>~/.aws/credentials</code> configuration file located on the EC2 instances of the online portal is missing and hence, it does not generate the pre-signed URL properly.</p>"
			},
			{
				"correct": false,
				"answer": "The ACL of the S3 bucket blocks the online portal and prevents the developers from uploading any files."
			},
			{
				"correct": true,
				"answer": "<p>The expiration date of the pre-signed URL is incorrectly set to expire too quickly and thus, may have already expired when they used it.</p>"
			},
			{
				"correct": false,
				"answer": "The application developers do not have access to either read or upload objects to the S3 bucket."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the main issue is that the online portal cannot upload files to the S3 bucket but the application developers can successfully upload files on their laptops. Take note that in this scenario, the online portal is deployed to a group of EC2 instances and it was not mentioned that you attached an IAM Role to these instances nor added security credentials in the <code>~/.aws/credentials</code> configuration file.</p><p>With all of these data in mind, we can deduce that the online portal is generating pre-signed URLs that are set to have an overly tight expiration date which causes the issue. In addition, there might be no security credentials added in the EC2 instances that host the online portal considering that it is not mentioned in the scenario. Remember that this is required to properly generate the pre-signed URLs.</p><p>Therefore, the correct answers are:</p><p><strong>- The expiration date of the pre-signed URL is incorrectly set to expire too quickly and thus, may have already expired when they used it</strong>.</p><p><strong>- The required AWS credentials in the </strong><code><strong>~/.aws/credentials</strong></code><strong> configuration file located on the EC2 instances of the online portal is missing and hence, it does not generate the pre-signed URL properly.</strong></p><p>The option that says:<strong> There was a recent change in the S3 bucket that allows object versioning which invalidates all presigned URLs </strong>is incorrect. Enabling object versioning in S3 will not hinder uploads that are done via a pre-signed URL.</p><p>The options that says: <strong>The application developers do not have access to either read or upload objects to the S3 bucket</strong> is incorrect. You have already provided the IAM role.</p><p>The option that says: <strong>The ACL of the S3 bucket blocks the online portal and prevents the developers from uploading any files</strong> is incorrect based on the fact that one developer has managed to successfully upload one file in the S3 bucket. The S3 ACL of the bucket is not an issue here.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html</a></p><p><a href=\"https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/s3-example-presigned-urls.html\">https://docs.aws.amazon.com/sdk-for-go/v1/developer-guide/s3-example-presigned-urls.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A business news portal is visited by thousands of readers each day to check on the latest hot topics in the world of business and technology. The news portal runs on a fleet of Spot EC2 instances behind an Application Load Balancer (ALB). Readers can also submit their comments in every article. Currently, the system's database is running on an on-premises data center, and the CTO is concerned that the content delivery time is not meeting company objectives. The portal's page load time is of utmost importance for the company to maintain its daily visitors.</p><p>Which of the following options would allow the solutions architect to quickly and cost-effectively modify the current infrastructure to reduce latency for their customers?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a CloudFront web distribution to speed up the delivery of data to their readers around the globe. Migrate the entire portal to an S3 bucket and then enable static web hosting. Set the S3 bucket as the origin of your CloudFront distribution.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Add an in-memory datastore using Amazon ElastiCache for Redis to reduce the burden on the database. Enable Redis replication to scale database reads and to have highly available clusters.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Replace the on-premises database of the news portal with a fast, scalable full-text search engine using Amazon ES by setting up an ELK stack (Elasticsearch, Logstash, and Kibana). Use a CloudFront web distribution to speed up the delivery of data to your users across the globe.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate your database on-premises to Amazon Aurora using the AWS Database Migration Service (DMS) and AWS Schema Conversion Tool (SCT). Create Aurora Replicas across Availability Zones and reconfigure the web servers to query from the Aurora database instead.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The success of the website and business is significantly affected by the speed at which you deliver content. Even the most optimized database query or remote API call is going to be noticeably slower than retrieving a flat key from an in-memory cache.</p><p>The primary purpose of an in-memory key-value store is to provide ultrafast (submillisecond latency) and inexpensive access to copies of data. Most data stores have areas of data that are frequently accessed but seldom updated. Additionally, querying a database is always slower and more expensive than locating a key in a key-value pair cache. By caching such query results, you pay the price of the query once and then are able to quickly retrieve the data multiple times without having to re-execute the query.</p><p><img src=\"https://media.tutorialsdojo.com/sap_elasticache_how_it_works.png\"></p><p>With the cache inside the VPC along with the web servers and application servers, the application doesn’t have to constantly go from AWS to the local data center. This change makes the physical distance between servers irrelevant. The new architecture should greatly improve the customer experience.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_reduce_latency.jpg\"></p><p>Therefore, the correct answer is: <strong>Use an in-memory cache based on Amazon ElastiCache to reduce network latency and to offload the database pressure.</strong> The main issue in this problem involves latency to the backend database. This solution dramatically reduces data retrieval latency. It also scales request volume considerably, because Amazon ElastiCache can deliver extremely high request rates, measured at over 20 million per second.</p><p>The option that says:<strong> Migrate your database on-premises to Amazon Aurora using the AWS Database Migration Service (DMS) and AWS Schema Conversion Tool (SCT). Create Aurora Replicas across Availability Zones and reconfigure the web servers to query from the Aurora database instead</strong> is incorrect. There is no guarantee that the database engine currently being used by the news portal is supported by Amazon Aurora. It is also important to take note that the question asks for a quick and cost-effective solution. Migrating the database may take a long amount of time, and the number of requests being sent each day can rack up costs.</p><p>The option that says: <strong>Replace the on-premises database of the news portal with a fast, scalable full-text search engine using Amazon ES by setting up an ELK stack (Elasticsearch, Logstash, and Kibana). Use a CloudFront web distribution to speed up the delivery of data to your users across the globe</strong> is incorrect. Elasticsearch is commonly used for log analytics, full-text search, security intelligence, business analytics, and operational intelligence use cases but not as a full-fledged database. A search engine could possibly be beneficial to the news portal as an additional layer but not as its sole data source. This solution might provide some advantages when they are both integrated together.</p><p>The option that says: <strong>Create a CloudFront web distribution to speed up the delivery of data to their readers around the globe. Migrate the entire portal to an S3 bucket and then enable static web hosting. Set the S3 bucket as the origin of your CloudFront distribution</strong> is incorrect. It is stated in the scenario that the readers can also submit their comments in every article, which means that the news portal is a dynamic website and not static. Hence, using S3 is not suitable for this scenario which is why this option is incorrect, even though it mentions the use of CloudFront web distribution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/elasticache-use-cases.html</a></p><p><a href=\"https://aws.amazon.com/blogs/database/latency-reduction-of-hybrid-architectures-with-amazon-elasticache/\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html</a></p><p><br></p><p><strong>Check out this Amazon ElastiCache Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p></div>"
	},
	{
		"question": "<p>A company has scheduled to launch a promotional sale on its e-commerce platform. The company’s web application is hosted on a fleet of Amazon EC2 instances in an Auto Scaling group. The database tier is hosted on an Amazon RDS for PostgreSQL DB instance. This is a large event so the management expects a sudden spike and unpredictable user traffic for the duration of the event. New users are also expected to register and participate in the event so there will be a lot of database writes during the event. The Solutions Architect has been tasked to create a solution that will ensure all submissions are committed to the database without changing the underlying data model.</p><p>Which of the following options is the recommended solution for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>To minimize any changes on the application or the current infrastructure, manually scale the current DB instance to a significantly larger instance size before the event. Choose a larger instance size depending on the anticipated user traffic, and scale down after the event is completed.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Decouple the application and database tier by creating an Amazon SQS queue between them. Create an AWS Lambda function that picks up the messages on the SQS queue and writes them into the database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon ElastiCache for Memcached cluster between the application and database tier. The cache will temporarily store the user submissions until the database is able to commit those entries.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Instead of using Amazon RDS, migrate the database to an Amazon DynamoDB table instead. Utilize the built-in automatic scaling in DynamoDB to scale the database based on user traffic.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware and empowers developers to focus on differentiating work. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.</p><p>Use Amazon SQS to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available. SQS lets you decouple application components so that they run and fail independently, increasing the overall fault tolerance of the system. Multiple copies of every message are stored redundantly across multiple availability zones so that they are available whenever needed.</p><p>Amazon SQS leverages the AWS cloud to dynamically scale based on demand. SQS scales elastically with your application so you don’t have to worry about capacity planning and pre-provisioning. There is no limit to the number of messages per queue, and standard queues provide nearly unlimited throughput.</p><p>You can use Amazon Simple Queue Service (SQS) to trigger AWS Lambda functions. Lambda functions can act as message consumers. Message consumers are processes that make the <strong>ReceiveMessage</strong> API call on SQS. Messages from queues can be processed either in batch or as a single message at a time. Each approach has its advantages and disadvantages.</p><p><strong>- Batch processing</strong>: This is where each message can be processed independently, and an error on a single message is not required to disrupt the entire batch. Batch processing provides the most throughput for processing, and also provides the most optimizations for resources involved in reading messages.</p><p><strong>- Single message processing</strong>: Single message processing is commonly used in scenarios where each message may trigger multiple processes within the consumer. In case of errors, the retry is confined to the single message.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_lambda_consumption.jpg\"></p><p>Therefore, the correct answer is: <strong>Decouple the application and database tier by creating an Amazon SQS queue between them. Create an AWS Lambda function that picks up the messages on the SQS queue and writes them into the database.</strong> This is an excellent scenario for which Amazon SQS is designed for. The SQS queue can scale reliably to hold user submissions to ensure they will be written to the database. The SQS queue is highly durable which ensures that you will not lose any submissions.</p><p>The option that says: <strong>To minimize any changes on the application or the current infrastructure, manually scale the current DB instance to a significantly larger instance size before the event. Choose a larger instance size depending on the anticipated user traffic, and scale down after the event is completed</strong> is incorrect. There is no mention in the question if the database is Multi-AZ configuration. If Multi-AZ is not enabled, this will result in a brief downtime as the database is being scaled up or down. Additionally, since the user traffic is unpredictable, there is no guarantee the larger instance can handle the user traffic.</p><p>The option that says: <strong>Instead of using Amazon RDS, migrate the database to an Amazon DynamoDB table instead. Utilize the built-in automatic scaling in DynamoDB to scale the database based on user traffic</strong> is incorrect. This is not recommended for the scenario as changing the database requires major changes in the application and database layers.</p><p>The option that says: <strong>Create an Amazon ElastiCache for Memcached cluster between the application and database tier. The cache will temporarily store the user submissions until the database is able to commit those entries</strong> is incorrect. Amazon ElastiCache is designed for caching frequent requests to the database and not for holding data that is waiting to be written to the database.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><a href=\"https://aws.amazon.com/blogs/architecture/application-integration-using-queues-and-messages/\">https://aws.amazon.com/blogs/architecture/application-integration-using-queues-and-messages/</a></p><p><a href=\"https://aws.amazon.com/sqs/faqs/\">https://aws.amazon.com/sqs/faqs/</a></p><p><br></p><p><strong>Check out the Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
	},
	{
		"question": "<p>A company is building an innovative AI-powered traffic monitoring portal and uses AWS to host its cloud infrastructure. For the initial deployment, the application would be used by an entire city. The application should be highly available and fault-tolerant to avoid unnecessary downtime.</p><p>Which of the following options is the MOST suitable architecture that you should implement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use Route 53 and create an A record to point to the ELB.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Launch an Auto Scaling group of EC2 instances on three Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use an Amazon Aurora Multi-Master as the database tier. Use Route 53 and create an Alias record to point to the ELB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use ElastiCache for the database caching of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration and Read Replicas. Use Route 53 and create a CNAME to point to the ELB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an Auto Scaling group of EC2 instances on two Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration. Use Route 53 and create an A record to point to the ELB.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Suppose that you start out running your app or website on a single EC2 instance, and over time, traffic increases to the point that you require more than one instance to meet the demand. You can launch multiple EC2 instances from your AMI and then use Elastic Load Balancing to distribute incoming traffic for your application across these EC2 instances. This increases the availability of your application. Placing your instances in multiple Availability Zones also improves the fault tolerance in your application. If one Availability Zone experiences an outage, traffic is routed to the other Availability Zone.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_cross_zone.PNG\"></p><p>You can use<strong> Amazon EC2 Auto Scaling</strong> to maintain a minimum number of running instances for your application at all times. Amazon EC2 Auto Scaling can detect when your instance or application is unhealthy and replace it automatically to maintain the availability of your application. You can also use Amazon EC2 Auto Scaling to scale your Amazon EC2 capacity up or down automatically based on demand, using criteria that you specify.</p><p>In this scenario, all of the options are highly available architectures. The main difference here is how they use Route 53.</p><p>The correct answer is the option that says: <strong>Launch an Auto Scaling group of EC2 instances on three Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use an Amazon Aurora Multi-Master as the database tier. Use Route 53 and create an Alias record to point to the ELB. </strong>It uses the correct type of record in Route 53, which is an alias record, to point to the ELB.</p><p>The option that says: <strong>Launch an Auto Scaling group of EC2 instances on two Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration. Use Route 53 and create an A record to point to the ELB</strong> is incorrect. You need to create an Alias record with the DNS name and not a normal A-record that points to an IP address.</p><p>The option that says: <strong>Use DynamoDB as the database of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use Route 53 and create an A record to point to the ELB </strong>is incorrect. To route domain traffic to an ELB load balancer, use Amazon Route 53 to create an alias resource record set that points to your load balancer.</p><p>The option that says: <strong>Use ElastiCache for the database caching of the portal. Launch an Auto Scaling group of EC2 instances on four Availability Zones. Attach an application load balancer to the Auto Scaling Group. Use a MySQL RDS instance with Multi-AZ deployments configuration and Read Replicas. Use Route 53 and create a CNAME to point to the ELB</strong> is incorrect. You have to use an Alias record to route to the ELB and not a CNAME record.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/attach-load-balancer-asg.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A new startup uses AWS to quickly deploy its application prototype. To host the application, a new VPC has been created with an IPv4 CIDR block 10.0.0.0/24. After a few weeks of testing, the Senior IT Manager wants to expand the VPC CIDR to host more resources inside the VPC.</p><p>Which of the following options can achieve this requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Delete all the subnets in the VPC and allocate a larger CIDR range. "
			},
			{
				"correct": false,
				"answer": "There is no method in changing your CIDR blocksize. You will have to create a new VPC."
			},
			{
				"correct": true,
				"answer": "You can expand your existing VPC by adding four (4) secondary IPv4 IP ranges (CIDRs) to it. "
			},
			{
				"correct": false,
				"answer": "Create a new VPC with a greater IP range and link it with the old VPC. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>After you create a CIDR block, you can't resize it. However, you can make other modifications to accommodate additional hosts. You can add another IPv4 CIDR block to your VPC.</p><p>When you create a VPC, you must specify an IPv4 CIDR block for the VPC. The allowed block size is between a /16 netmask (65,536 IP addresses) and /28 netmask (16 IP addresses). After you've created your VPC, you further expand your network by associating one to utmost 4 secondary CIDR blocks to your VPC.</p><p>If your depleted CIDR block is a secondary CIDR block, complete one of the following:</p><p>- Disassociate the secondary CIDR block and associate a larger CIDR block</p><p>- Associate another CIDR block with a new IP address range</p><p>If your depleted CIDR block is the primary CIDR block, you can’t disassociate it. Instead, complete one of the following:</p><p>- Associate a secondary CIDR block in adherence to the CIDR block association restrictions</p><p>- Create a new VPC and associate a larger CIDR block</p><p>By default, a local route is added to all routing tables in the VPC for every CIDR block associated with the VPC. This enables you to route traffic between the primary and secondary CIDR resources without additional routing.</p><p>Therefore, the correct answer is: <strong>Expand your existing VPC by adding four (4) secondary IPv4 IP ranges (CIDRs) to it. </strong>You can expand your existing VPC by adding four secondary IPv4 IP ranges (CIDRs) to your VPC.</p><p>The option that says: <strong>Delete all the subnets in the VPC and allocating a larger CIDR range</strong> is incorrect. Deleting the subnets is unnecessary as this will delete your existing resources on that subnet as well.</p><p>The option that says: <strong>Create a new VPC with a greater IP range and link it with the old VPC</strong> is incorrect. This configuration would peer the VPC, it will not alter the existing VPC's CIDR.</p><p>The option that says: <strong>There is no method in changing your CIDR blocksize. You will have to create a new VPC</strong> is incorrect. You can change the CIDR of VPC by adding up to 4 secondary IPv4 IP CIDRs to your VPC.</p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/vpc/faqs/\">https://aws.amazon.com/vpc/faqs/</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html\">https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/vpc-modify-cidr-more-hosts/\">https://aws.amazon.com/premiumsupport/knowledge-center/vpc-modify-cidr-more-hosts/</a></p><p><br></p><p><strong>Amazon VPC Overview:</strong></p><p><a href=\"https://youtu.be/oIDHKeNxvQQ\">https://youtu.be/oIDHKeNxvQQ</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company offers a service that allows users to upload media files through a web portal. The web servers accept the media files and are directly uploaded on the on-premises Network Attached Storage (NAS). For each uploaded media file, a corresponding message is sent to the message queue. A processing server picks up each message and processes each media file which can take up to 30 minutes to process. The company noticed that the number of media files waiting in the processing queue is significantly higher during business hours, but the processing server quickly catches up after business hours. To save costs, the company hired a Solutions Architect to improve the media processing by migrating the workload to AWS Cloud.</p><p>Which of the following options is the most cost-effective solution?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon EFS mount point and shut down the EC2 instances after processing is complete.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon EFS volume.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Amazon SQS offers \"standard\" as the default queue type. Standard queues support at-least-once message delivery.</p><p>You can use standard message queues in many scenarios, as long as your application can process messages that arrive more than once and out of order, for example:</p><p>- Decouple live user requests from intensive background work – Let users upload media while resizing or encoding it.</p><p>- Allocate tasks to multiple worker nodes – Process a high number of credit card validation requests.</p><p>- Batch messages for future processing – Schedule multiple entries to be added to a database.</p><p>You can scale-in/scale-out your Amazon EC2 Auto Scaling group in response to changes in system load in an Amazon Simple Queue Service (Amazon SQS) queue. For example, suppose that you have a web app that lets users upload images and use them online. In this scenario, each image requires resizing and encoding before it can be published. The app runs on EC2 instances in an Auto Scaling group, and it's configured to handle your typical upload rates. Unhealthy instances are terminated and replaced to maintain current instance levels at all times.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_cloudwatch_metric.png\"></p><p>The app places the raw bitmap data of the images in an SQS queue for processing. It processes the images and then publishes the processed images where they can be viewed by users. The architecture for this scenario works well if the number of image uploads doesn't vary over time. But if the number of uploads changes over time, you might consider using dynamic scaling to scale the capacity of your Auto Scaling group. If you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively.</p><p><strong>Amazon SQS</strong> is a queue service that is highly scalable, simple to use, and doesn't require you to set up message brokers. AWS recommends this service for new applications that can benefit from nearly unlimited scalability and simple APIs.</p><p><strong>Amazon MQ</strong> is a managed message broker service that provides compatibility with many popular message brokers. AWS recommends Amazon MQ for migrating applications from existing message brokers that rely on compatibility with APIs such as JMS or protocols such as AMQP, MQTT, OpenWire, and STOMP.</p><p>Therefore, the correct answer is: <strong>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon S3 bucket.</strong></p><p>The option that says: <strong>Reconfigure the existing web servers to publish messages to a standard queue on Amazon SQS. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon S3 bucket</strong> is incorrect. Although this answer is the most cost-effective, AWS Lambda only allows functions to run up to 15 minutes. Remember that the media files can take up to 30 minutes to process.</p><p>The option that says: <strong>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an Auto Scaling group of Amazon EC2 instances that will pull requests from the queue and process the media files. Configure the Auto Scaling group to scale based on the length of the SQS queue. Send the processed media files into an Amazon EFS mount point and shut down the EC2 instances after processing is complete</strong> is incorrect. It is recommended to use Amazon SQS for this because it is not stated in the scenario to have compatibility with JMS or other protocols like MQTT AMQP, etc. Also, storing media files on Amazon EFS is more expensive than using Amazon S3.</p><p>The option that says: <strong>Reconfigure the existing web servers to publish messages to a queue in Amazon MQ. Create an AWS Lambda function that will pull requests from the SQS queue and process the media files. Invoke the Lambda function every time a new message is sent to the queue. Send the processed media files into an Amazon EFS volume</strong> is incorrect. Lambda functions cannot run for more than 15 minutes, while in the scenario, the processing time is 30 minutes. Moreover, storing media files on Amazon EFS is more expensive than using Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/standard-queues.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html</a></p><p><br></p><p><strong>Check out these Amazon SQS and Amazon MQ Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-mq/?src=udemy\">https://tutorialsdojo.com/amazon-mq/</a></p></div>"
	},
	{
		"question": "<p>A national library is planning to store around 50 TB of data containing all their books, articles, and other written materials in AWS. One of the requirements is to have a search feature to enable the users to look for their collection on their dynamic website.</p><p>As a Cloud Engineer, what is the most suitable solution that you should implement in AWS to satisfy the needed functionality?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use CodeDeploy as the deployment service to deploy two S3 buckets in which the first one serves as the storage service and the second one for hosting their dynamic website. Use the native search functionality of S3 to satisfy the search feature requirement."
			},
			{
				"correct": false,
				"answer": "Use OpsWorks as the deployment service to deploy Kinesis as the storage service and an EC2 instance to serve their website."
			},
			{
				"correct": false,
				"answer": "Use Elastic Beanstalk as the deployment service. Deploy the needed AWS resources such as the Multi-AZ RDS for storage and an EC2 instance to host their website."
			},
			{
				"correct": true,
				"answer": "Use CloudFormation as the deployment service to deploy the needed AWS resources such as an S3 bucket for storage; CloudSearch to provide the needed search functionality, and an EC2 instance to host their website."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon S3</strong> offers a highly durable, scalable, and secure destination for backing up and archiving your critical data. You can use S3’s versioning capability to provide even further protection for your stored data. Whether you’re storing pharmaceutical or financial data, or multimedia files such as photos and videos, Amazon S3 can be used as your data lake for big data analytics.</p><p><strong>Amazon CloudSearch</strong> is a managed service in the AWS Cloud that makes it simple and cost-effective to set up, manage, and scale a search solution for your website or application. With Amazon CloudSearch, you can quickly add rich search capabilities to your website or application. You don't need to become a search expert or worry about hardware provisioning, setup, and maintenance.</p><p><strong>Amazon Elastic Compute Cloud (Amazon EC2)</strong> provides scalable computing capacity in the Amazon Web Services (AWS) Cloud. Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster. You can use Amazon EC2 to launch as many or as few virtual servers as you need, configure security and networking, and manage storage. Amazon EC2 enables you to scale up or down to handle changes in requirements or spikes in popularity, reducing your need to forecast traffic.</p><p>Amazon S3 is great for storage but it lacks an effective search feature. Hence, it is better to use a service like Amazon CloudSearch to fulfill the requirement in the scenario.</p><p>Therefore, the correct answer is: <strong>Use CloudFormation as the deployment service to deploy the needed AWS resources such as an S3 bucket for storage; CloudSearch to provide the needed search functionality, and an EC2 instance to host their website.</strong></p><p>The option that says: <strong>Use Elastic Beanstalk as the deployment service. Deploy the needed AWS resources such as the Multi-AZ RDS for storage and an EC2 instance to host their website</strong> is incorrect. You can use RDS as your database for the application, however, this will be very expensive. It is recommended to use CloudSearch instead to have a cost-effective search solution for the website application.</p><p>The option that says: <strong>Use OpsWorks as the deployment service to deploy Kinesis as the storage service and an EC2 instance to serve their website</strong> is incorrect. Amazon Kinesis is not designed to be used as a long term storage service.</p><p>The option that says: <strong>Use CodeDeploy as the deployment service to deploy two S3 buckets in which the first one serves as the storage service and the second one for hosting their dynamic website. Use the native search functionality of S3 to satisfy the search feature requirement</strong> is incorrect. You cannot host the dynamic application on Amazon S3, and there is no built-in search feature on Amazon S3 that will satisfy the application requirements.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudsearch\">https://aws.amazon.com/cloudsearch</a></p><p><a href=\"https://aws.amazon.com/what-is-cloud-object-storage/\">https://aws.amazon.com/what-is-cloud-object-storage/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html</a></p><p><br></p><p><strong>Check out this Amazon CloudSearch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudsearch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudsearch/</a></p></div>"
	},
	{
		"question": "<p>The European Organization for Nuclear Research, also known as CERN, is a research organization that operates the largest particle accelerator in the world and generates terabytes of experimental data every day. A group of data scientists is planning to use an Elastic MapReduce cluster for their data analysis, which will only be run once. The cluster is designed to ingest 300 TB of data with a total of 200 EC2 instances and is expected to run for about 8 hours. The resulting data set must be stored temporarily until it is permanently stored in their AWS Redshift database.</p><p>Which of the following options is the best and most cost-effective solution to satisfy the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Use On-Demand EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes."
			},
			{
				"correct": false,
				"answer": "Use a combination of On-Demand instance and Spot instance types for both the master and core nodes. Use Spot EC2 instances for the task nodes."
			},
			{
				"correct": false,
				"answer": "Use Reserved EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes."
			},
			{
				"correct": false,
				"answer": "Use Reserved EC2 instances for the master node; On-Demand instances for the core nodes; and use Spot EC2 instances for the task nodes."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the scientists are doing a one-time processing of their data in 8 hours. Hence, a Reserved instance is not a suitable type to use as this project is not for the long term.</p><p><strong>Amazon EMR</strong> provides a managed Hadoop framework that makes it easy, fast, and cost-effective to process vast amounts of data using EC2 instances. When using Amazon EMR, you don’t need to worry about installing, upgrading, and maintaining Spark software (or any other tool from the Hadoop framework). You also don’t need to worry about installing and maintaining underlying hardware or operating systems. Instead, you can focus on your business applications and use Amazon EMR to remove the undifferentiated heavy lifting.</p><p>The central component of Amazon EMR is the <strong><em>cluster</em></strong>. A cluster is a collection of Amazon Elastic Compute Cloud (Amazon EC2) instances. Each instance in the cluster is called a <strong><em>node</em></strong>. Each node has a role within the cluster, referred to as the <strong><em>node</em></strong><em> type</em>. Amazon EMR also installs different software components on each node type, giving each node a role in a distributed application like Apache Hadoop.</p><p>The node types in Amazon EMR are as follows:</p><p><strong>Master node</strong>: A node that manages the cluster by running software components to coordinate the distribution of data and tasks among other nodes for processing. The master node tracks the status of tasks and monitors the health of the cluster. Every cluster has a master node, and it's possible to create a single-node cluster with only the master node.</p><p><strong>Core node</strong>: A node with software components that run tasks and store data in the Hadoop Distributed File System (HDFS) on your cluster. Multi-node clusters have at least one core node.</p><p><strong>Task node</strong>: A node with software components that only runs tasks and does not store data in HDFS. Task nodes are optional.</p><p>For optimizing costs and performance in choosing the instance types:</p><p><strong>Master node:</strong> Unless your cluster is very short-lived and the runs are cost-driven, avoid running your Master node on a Spot Instance. We suggest this because a Spot interruption on the Master node terminates the entire cluster. Alternatively to On-Demand, you can set up the Master node on a Spot Block. You do so by setting the defined duration of the node and failing over to On-Demand if the Spot Block capacity is unavailable.</p><p><strong>Core nodes:</strong> Avoid using Spot Instances for Core nodes if the jobs on the cluster use HDFS. That prevents a situation where Spot interruptions cause data loss for data that was written to the HDFS volumes on the instances.</p><p><strong>Task nodes:</strong> Use Spot Instances for your task nodes by selecting up to five instance types that match your hardware requirement. Amazon EMR fulfills the most suitable capacity by price and capacity availability.</p><p><img src=\"https://media.tutorialsdojo.com/sap_emr_node_types.png\"></p><p>Therefore, the correct answer is: <strong>Use On-Demand EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes.</strong></p><p>The option that says: <strong>Use a combination of On-Demand instance and Spot instance types for both the master and core nodes. Use Spot EC2 instances for the task nodes</strong> is incorrect. It is not recommended to use a Spot instance for the Master node. Also, if dealing with a large amount of data shared on the cluster, you don't want to use spot instances for the Core nodes as it may cause data loss or interruption on your processing.</p><p>The option that says: <strong>Use Reserved EC2 instances for both the master and core nodes and use Spot EC2 instances for the task nodes </strong>is incorrect. The scenario is not a long-term project so a Reserved instance is not a suitable type to use in this case.<strong><br></strong></p><p>The option that says: <strong>Use Reserved EC2 instances for the master node; On-Demand instances for the core nodes; and use Spot EC2 instances for the task nodes</strong> is incorrect. The scenario is not a long-term project so a Reserved instance is not a suitable type to use in this case.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview.html</a></p><p><a href=\"https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/\">https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/</a></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html#emr-instance-fleet-allocation-strategy</a></p><p><br></p><p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-emr/?src=udemy\">https://tutorialsdojo.com/amazon-emr/</a></p></div>"
	},
	{
		"question": "<p>An enterprise has several development and production AWS accounts managed under its AWS Organization. Consolidated billing is enabled in the organization but the management wants more visibility on the AWS Billing and Cost Management. With the sudden increase in the Amazon RDS and Amazon DynamoDB costs, the management required all CloudFormation templates to enforce a consistent tagging with cost center numbers and project ID numbers on all resources that will be provisioned. The management also wants these tags to be enforced in all existing and future DynamoDB and RDS instances.</p><p>Which of the following options is the recommended strategy to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Apply an SCP on the organizational unit that denies users from creating resources that do not have the cost center and project ID tags.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an AWS Config rule to check for any untagged resource and send a notification email to the finance team. Write a Lambda function that has a cross-account role to tag all RDS databases and DynamoDB resources on all accounts under the organization. Schedule this function to run every hour.</p>"
			},
			{
				"correct": false,
				"answer": "<p>On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports. Update existing federated roles to deny users from creating resources that do not have the cost center and project ID tags.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Tags</strong> are words or phrases that act as metadata that you can use to identify and organize your AWS resources. A resource can have up to 50 user-applied tags. It can also have read-only system tags. Each tag consists of a key and one optional value.</p><p>You can add tags to resources when you create the resource. You can use the resource's service console or API to add, change, or remove those tags one resource at a time. To add tags to—or edit or delete tags of—multiple resources at once, use <strong>Tag Editor</strong>. With Tag Editor, you search for the resources that you want to tag, and then manage tags for the resources in your search results.</p><p>For each resource, each tag key must be unique, and each tag key can have only one value. You can use tags to organize your resources, and cost allocation tags to track your AWS costs on a detailed level. After you activate cost allocation tags, AWS uses the cost allocation tags to organize your resource costs on your cost allocation report, to make it easier for you to categorize and track your AWS costs. AWS provides two types of cost allocation tags, <strong>AWS generated tags</strong> and<strong> user-defined tags</strong>. AWS, or AWS Marketplace ISV defines, creates, and applies the AWS generated tags for you, and you define, create, and apply user-defined tags. You must activate both types of tags separately before they can appear in Cost Explorer or on a cost allocation report.</p><p><img src=\"https://media.tutorialsdojo.com/sap_billing_tags.png\"></p><p><strong>User-defined tags</strong> are tags that you define, create, and apply to resources. After you have created and applied the user-defined tags, you can activate them by using the Billing and Cost Management console for cost allocation tracking. Cost Allocation Tags appear on the console after you've enabled Cost Explorer, Budgets, AWS Cost and Usage Reports, or legacy reports. After you activate the AWS services, they appear on your cost allocation report. You can then use the tags on your cost allocation report to track your AWS costs. Tags are not applied to resources that were created before the tags were created.</p><p><strong>Service control policies (SCPs)</strong> are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you to ensure your accounts stay within your organization’s access control guidelines. SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features.</p><p>Therefore, the correct answer is: <strong>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Apply an SCP on the organizational unit that denies users from creating resources that do not have the cost center and project ID tags.</strong> Tag Editor allows bulk tagging to easily tag your AWS resources. The SCP rules will enforce the company tagging policy by preventing users from creating resources that do not have the appropriate tags.</p><p>The option that says: <strong>Tag all existing resources in bulk using the Tag Editor. On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports </strong>is incorrect. This solution is incomplete. It does not prevent users from creating untagged resources in the future.</p><p>The option that says: <strong>Create an AWS Config rule to check for any untagged resource and send a notification email to the finance team. Write a Lambda function that has a cross-account role to tag all RDS databases and DynamoDB resources on all accounts under the organization. Schedule this function to run every hour </strong>is incorrect. This solution does not prevent users from creating untagged resources in the future and the AWS Config rule does not automatically tag non-compliant resources.</p><p>The option that says: <strong>On the Billing and Cost Management page, create new cost allocation tags for the cost center and project ID. Wait for at least 24 hours to allow AWS to propagate the tags and gather cost reports. Update existing federated roles to deny users from creating resources that do not have the cost center and project ID tags </strong>is incorrect. This may be possible but it will be cumbersome to edit all IAM policies across all the company AWS accounts. It is better to use an SCP applied at the organization unit level to enforce the tagging policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html#example-require-tag-on-create\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples.html#example-require-tag-on-create</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf\">https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf</a></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html</a></p><p><a href=\"https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html\">https://docs.aws.amazon.com/ARG/latest/userguide/tag-editor.html</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p></div>"
	},
	{
		"question": "<p>A global finance company has multiple data centers around the globe. Due to the ever-growing data that your company is storing, the solutions architect was instructed to set up a durable, cost-effective solution to archive sensitive data from the existing on-premises tape-based backup infrastructure to AWS Cloud.</p><p>Which of the following options is the recommended implementation to achieve the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up a Tape Gateway to back up your data in Amazon S3 with point-in-time backups as tapes which will be stored in the Virtual Tape Shelf.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a Tape Gateway to back up your data in Amazon S3 and archive it in Amazon Glacier using your existing tape-based processes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a File Gateway to back up your data in Amazon S3 and archive in Amazon Glacier using your existing tape-based processes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a Stored Volume Gateway to back up your data in Amazon S3 with point-in-time backups as EBS snapshots.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Storage Gateway</strong> connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the Amazon Web Services Cloud for scalable and cost-effective storage that helps maintain data security.</p><p>AWS Storage Gateway offers file-based file gateways (Amazon S3 File and Amazon FSx File), volume-based (Cached and Stored), and tape-based storage solutions.</p><p><strong>Tape Gateway</strong> offers a durable, cost-effective solution to archive your data in the AWS Cloud. With its virtual tape library (VTL) interface, you use your existing tape-based backup infrastructure to store data on virtual tape cartridges that you create on your tape gateway. Each tape gateway is preconfigured with a media changer and tape drives. These are available to your existing client backup applications as iSCSI devices. You add tape cartridges as you need to archive your data.</p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_tape_gateway.png\"></p><p>Therefore, the correct answer is: <strong>Set up a Tape Gateway to back up your data in Amazon S3 and archive it in Amazon Glacier using your existing tape-based processes.</strong></p><p>The option that says: <strong>Set up a Stored Volume Gateway to back up your data in Amazon S3 with point-in-time backups as EBS snapshots</strong> is incorrect. Stored Volume Gateway is not recommended for Tape archives, you should use Tape Gateway instead.</p><p>The option that says: <strong>Set up a Tape Gateway to back up your data in Amazon S3 with point-in-time backups as tapes which will be stored in the Virtual Tape Shelf</strong> is incorrect. The archival should be on Amazon Glacier for cost-effectiveness.</p><p>The option that says: <strong>Set up a File Gateway to back up your data in Amazon S3 and archive in Amazon Glacier using your existing tape-based processes</strong> is incorrect. A File Gateway is not recommended for Tape archives, you should use Tape Gateway instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-vtl-concepts\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-vtl-concepts</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/create-tape-gateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/create-tape-gateway.html</a></p><p><br></p><p><strong>AWS Storage Gateway Overview:</strong></p><p><a href=\"https://youtu.be/pNb7xOBJjHE\">https://youtu.be/pNb7xOBJjHE</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p></div>"
	},
	{
		"question": "<p>A company runs an application in a fleet of Amazon EC2 instances in the us-east-2 region. A database server is hosted on the on-premises data center which complies with the BASE (Basically Available, Soft state, Eventual consistency) model rather than the ACID (Atomicity, Consistency, Isolation, Durability) consistency model. The on-premises network has a 10 GB AWS Direct Connect connection to the Amazon VPC in us-east-2. The application relies on this database for normal operations. Whenever there are lots of database write requests, the application behavior becomes erratic.</p><p>Which of the following options should the solutions architect implement to improve the performance of the application in a cost-effective way?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Update the application to write to an Amazon DynamoDB table. Feed the table to an Amazon EMR cluster and create a map function that will update the on-premises database for every table update.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a Hadoop cluster using Amazon Elastic Map Reduce (EMR) and use the S3DistCp tool to synchronize data between the on-premises database and the Hadoop cluster.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon SQS queue and develop a consumer process to flush the queue to the on-premises database server. Update the application to enable writing to the SQS queue.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon RDS multi-AZ instance that will synchronize with the on-premises database server using Amazon EventBridge. Redirect the write operations of the application to the Amazon RDS endpoint via the Amazon Elastic Transcoder service.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Since the application relies on an eventual consistency model, there should be no problem on adding an SQS queue in front of the database.</p><p>Decoupling message queuing from the database improves database availability and enables greater message queue scalability. It also provides a more cost-effective use of the database, and mitigates backpressure created when database performance is constrained by message management.</p><p>You can use standard message queues in many scenarios, as long as your application can process messages that arrive more than once and out of order, for example:</p><p>- Decouple live user requests from intensive background work: Let users upload media while resizing or encoding it.</p><p>- Allocate tasks to multiple worker nodes: Process a high number of credit card validation requests.</p><p>- Batch messages for future processing: Schedule multiple entries to be added to a database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_database.png\"></p><p>Therefore, the correct answer is: <strong>Create an Amazon SQS queue and develop a consumer process to flush the queue to the on-premises database server. Update the application to enable writing to the SQS queue.</strong> Since the application follows the eventual consistency model, an SQS can be used to temporarily hold the write requests, while a worker process flushes the queue to the on-premises database.</p><p>The option that says: <strong>Create a Hadoop cluster using Amazon Elastic Map Reduce (EMR) and use the S3DistCp tool to synchronize data between the on-premises database and the Hadoop cluster </strong>is incorrect. S3DistCp tool is used to copy large amounts of data from Amazon S3 into HDFS. It is not suitable for synchronizing data to an on-premises database.</p><p>The option that says:<strong> Create an Amazon RDS multi-AZ instance that will synchronize with the on-premises database server using Amazon EventBridge. Redirect the write operations of the application to the Amazon RDS endpoint via the Amazon Elastic Transcoder service</strong> is incorrect. The application is using the BASE consistency model so an SQL-based database, such as an Amazon RDS, may not be compatible with the data to be written by the application. Moreover, you cannot directly synchronize your on-premises database server using Amazon EventBridge. Amazon Elastic Transcoder is simply a media transcoding service in AWS; thus, it can't be used to send write operations to your Amazon RDS endpoint.</p><p>The option that says: <strong>Update the application to write to an Amazon DynamoDB table. Feed the table to an Amazon EMR cluster and create a map function that will update the on-premises database for every table update</strong> is incorrect. This may be possible but creating a DynamoDB table with a high WCU and an EMR cluster significantly increases operational costs compared to using an SQS queue.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/architecture/modernized-database-queuing-using-amazon-sqs-and-aws-services/\">https://aws.amazon.com/blogs/architecture/modernized-database-queuing-using-amazon-sqs-and-aws-services/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><a href=\"https://aws.amazon.com/sqs/features/\">https://aws.amazon.com/sqs/features/</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
	},
	{
		"question": "<p>A company plans to release a public beta of its new video game. The release package is approximately 5GB in size. Based on previous releases and community feedback, millions of users from around the world are expected to download the new game. Currently, the company has a Linux-based, FTP site hosted on its on-premises data to allow public Internet users to download the game. However, the company wants a new solution that is cost-effective and will allow faster download performance for its users regardless of their location.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Host the FTP service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EFS volume on each instance. Place the Auto Scaling group behind a Network Load Balancer. Create an Amazon Route 53 entry pointing to the NLB. Publish the Route 53 entry as the FTP URL to allow users to download the game package.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Host the FTP service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EBS volumes on each instance. Place the Auto Scaling group behind an Application Load Balancer. Create an Amazon Route 53 entry pointing to the ALB. Publish the Route 53 entry as the FTP URL to allow users to download the game package.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. Create an Amazon CloudFront distribution with the S3 bucket as the origin. Create an Amazon Route 53 entry pointing to the CloudFront distribution. Publish the Route 53 entry as the FTP URL to allow users to download the game package.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. To improve cost-effectiveness, enable the “Requestor Pays” option for the S3 bucket. Create an Amazon Route 53 entry pointing to the S3 bucket. Publish the Route 53 entry as the FTP URL to allow users to download the game package.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use <strong>Amazon S3</strong> to host a static website. Hosting a static website on Amazon S3 delivers a highly performant and scalable website at a fraction of the cost of a traditional web server. To host a static website on Amazon S3, configure an Amazon S3 bucket for website hosting and upload your website content. Using the AWS Management Console, you can configure your Amazon S3 bucket as a static website without writing any code. Depending on your website requirements, you can also use some optional configurations, including redirects, web traffic logging, and custom error documents.</p><p>When you configure a bucket as a static website, you must enable static website hosting, configure an index document, and set permissions.</p><p>You can use <strong>Amazon CloudFront</strong> to improve the performance of your Amazon S3 website. CloudFront makes your website files (such as HTML, images, and video) available from data centers around the world (known as edge locations). When a visitor requests a file from your website, CloudFront automatically redirects the request to a copy of the file at the nearest edge location. This results in faster download times than if the visitor had requested the content from a data center that is located farther away.</p><p>CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p><p><img src=\"https://media.tutorialsdojo.com/s3-transfer-acceleration.png\"></p><p>To serve a static website hosted on Amazon S3, you can deploy a CloudFront distribution using one of these configurations:</p><p>- Using a REST API endpoint as the origin, with access restricted by an origin access identity (OAI)</p><p>- Using a website endpoint as the origin, with anonymous (public) access allowed. Helpful when creating public FTP websites to allow public users to download files.</p><p>- Using a website endpoint as the origin, with access restricted by a Referer header</p><p>- Using AWS CloudFormation to deploy a REST API endpoint as the origin, with access restricted by an OAI and a custom domain pointing to CloudFront</p><p>Therefore, the correct answer is: <strong>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. Create an Amazon CloudFront distribution with the S3 bucket as the origin. Create an Amazon Route 53 entry pointing to the CloudFront distribution. Publish the Route 53 entry as the FTP URL to allow users to download the game package. </strong>Storing the game package on an Amazon S3 bucket is very cost-effective. Using CloudFront will ensure that users will have a consistently high download performance regardless of their location.</p><p>The option that says: <strong>Host the FTP service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EBS volumes on each instance. Place the Auto Scaling group behind an Application Load Balancer. Create an Amazon Route 53 entry pointing to the ALB. Publish the Route 53 entry as the FTP URL to allow users to download the game package</strong> is incorrect. This is possible but very expensive as you have to attach EBS volumes to each instance. Additionally, this is limited to only one region. Other users from around the world may experience a slower download speed.</p><p>The option that says: <strong>Host the FTP service on an Auto Scaling group of Amazon EC2 instances. Save the game files on the mounted Amazon EFS volume on each instance. Place the Auto Scaling group behind a Network Load Balancer. Create an Amazon Route 53 entry pointing to the NLB. Publish the Route 53 entry as the FTP URL to allow users to download the game package</strong> is incorrect. This is not recommended because using EFS is more expensive than using an S3 bucket. A Network Load Balancer is also very expensive.</p><p>The option that says: <strong>Create an Amazon S3 bucket with website hosting enabled and upload the game package on it. To improve cost-effectiveness, enable the “Requestor Pays” option for the S3 bucket. Create an Amazon Route 53 entry pointing to the S3 bucket. Publish the Route 53 entry as the FTP URL to allow users to download the game package</strong> is incorrect. This is not possible because the users will need their own AWS accounts in order to download an object from a bucket that has \"Requestor Pays\" enabled.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/\">https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-serve-static-website/</a></p><p><br></p><p><strong>Check out these Amazon S3 and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A company runs a finance-related application on a fleet of Amazon EC2 instances inside a private subnet of a VPC in AWS. To access the application, the instances are behind an internet-facing Application Load Balancer (ALB). As part of security compliance, the company is required to have a solution that allows it to inspect network payloads that are being sent to the application. Analyzing the network payloads will help in reverse-engineering sophisticated network attacks that the application may experience.</p><p>Which of the following options should the solutions architect implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Go to the Amazon EC2 console and enable “Access logs” for the ALB. Send the ALB access logs to an Amazon S3 bucket for analysis.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Go to the Amazon VPC console and create a VPC flow log. Set the destination of flow log data to an Amazon S3 bucket for analysis.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure Traffic Mirroring on the elastic network interface of the EC2 instances. Send the mirrored traffic to a monitoring appliance for storage and inspection.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new AWS web ACL with blank rules and a default “Allow” action. Associate the ALB to this web ACL. Enable logging on web ACL and send them to Amazon CloudWatch Logs for analysis.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Traffic Mirroring</strong> is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of Amazon EC2 instances. You can then send the traffic to out-of-band security and monitoring appliances for:</p><p>Content inspection</p><p>Threat monitoring</p><p>Troubleshooting</p><p>The security and monitoring appliances can be deployed as individual instances or as a fleet of instances behind a Network Load Balancer with a UDP listener. Traffic Mirroring supports filters and packet truncation so that you only extract the traffic of interest to monitor by using monitoring tools of your choice.</p><p><img src=\"https://media.tutorialsdojo.com/sap_traffic_mirroring.PNG\"></p><p>Traffic Mirroring copies inbound and outbound traffic from the network interfaces that are attached to your Amazon EC2 instances. You can send the mirrored traffic to the network interface of another EC2 instance, or a Network Load Balancer that has a UDP listener. The traffic mirror source and the traffic mirror target (monitoring appliance) can be in the same VPC. Or they can be in different VPCs that are connected through intra-Region VPC peering or a transit gateway.</p><p>Therefore, the correct answer is: <strong>Configure Traffic Mirroring on the elastic network interface of the EC2 instances. Send the mirrored traffic to a monitoring appliance for storage and inspection.</strong> Traffic Mirroring can copy network traffic from an elastic network interface and send it to a monitoring appliance for inspection.</p><p>The option that says: <strong>Go to the Amazon VPC console and create a VPC flow log. Set the destination of flow log data to an Amazon S3 bucket for analysis</strong> is incorrect. This will log network data on all resources on the VPC, not just the EC2 cluster in question. Additionally, VPC flow log data only contain OSI Layer 4 (Transport) information. This does not include payload contents.</p><p>The option that says: <strong>Go to the Amazon EC2 console and enable “Access logs” for the ALB. Send the ALB access logs to an Amazon S3 bucket for analysis</strong> is incorrect. ALB access logs have similar content to HTTP/HTTPS logs, however, it only contains the request path and some headers on the logs. The payload itself is not recorded.</p><p>The option that says: <strong>Create a new AWS web ACL with blank rules and a default “Allow” action. Associate the ALB to this web ACL. Enable logging on web ACL and send them to Amazon CloudWatch Logs for analysis</strong> is incorrect. You can capture information about the web requests that are evaluated by AWS WAF, however, you can only send the logs to Amazon Kinesis Firehose, not CloudWatch Logs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/what-is-traffic-mirroring.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/traffic-mirroring-how-it-works.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-inbound-tcp.html\">https://docs.aws.amazon.com/vpc/latest/mirroring/tm-example-inbound-tcp.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company has a large Microsoft Windows Server running on a public subnet. There are EC2 instances hosted on a private subnet that allows Remote Desktop Protocol (RDP) connections to the Windows Server via port 3389. These instances enable the Microsoft Administrators to connect to the public servers and troubleshoot any server failures.</p><p>The server must always have the latest operating system upgrades to improve security and it must be accessible at any given point in time. The administrators are tasked to refactor the existing solution and manage the server patching activities effectively, even outside the regular maintenance window.</p><p>Which of the following provides the LEAST amount of administrative overhead in managing the server?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Launch a hardened machine image from the AWS Marketplace and host the server in AWS Cloud9. Set up the AWS Systems Manager Patch Manager to automatically apply system updates. Use Amazon AppStream 2.0 to act as a bastion host.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Launch the Windows Server on Amazon WorkSpaces. Use Amazon WorkSpaces Application Manager (WAM) to harden the server and configure the Windows automatic updates to occur every day.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch the server using AWS OpsWorks Stacks and implement a Chef recipe to harden the AMI automatically during instance launch. Set up a combination of Amazon EventBridge and AWS Lambda scheduled event to run the <code>Upgrade Operating System</code> stack command to apply system updates.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an AWS AppSync environment with a single EC2 instance that runs the Windows Server. Set up the environment with a custom AMI to utilize a hardened machine image that can be downloaded from AWS Marketplace. Configure the AWS Systems Manager Patch Manager to automatically apply the OS updates.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon WorkSpaces enables you to provision virtual, cloud-based Microsoft Windows or Amazon Linux desktops for your users, known as WorkSpaces. WorkSpaces eliminates the need to procure and deploy hardware or install complex software. You can quickly add or remove users as your needs change. Users can access their virtual desktops from multiple devices or web browsers.</p><p>Amazon WorkSpaces Application Manager (Amazon WAM) offers a fast, flexible, and secure way for you to deploy and manage applications for Amazon WorkSpaces with Windows. Amazon WAM accelerates software deployment, updates, patching, and retirement by packaging Microsoft Windows desktop applications into virtual containers that run as though they are installed natively.</p><p><img src=\"https://media.tutorialsdojo.com/amazon%20workspaces%20application%20manager.png\"></p><p>Amazon WAM is fully integrated with the AWS Management Console, and allows you to build an application catalog from your line-of-business applications, third-party applications that you own the license for, and applications purchased through the AWS Marketplace.</p><p>It is recommended that you regularly patch, update, and secure the operating system and applications on your WorkSpaces. You can configure your WorkSpaces to be updated by WorkSpaces during a regular maintenance window or you can update them yourself. For applications on your WorkSpaces, you can use any automatic update services provided or follow the recommendations for installing updates provided by the application vendor.</p><p>During the maintenance window, the WorkSpace installs important updates from Amazon WorkSpaces and reboots as necessary. If available, operating system updates are also installed from the OS update server that the WorkSpace is configured to use. During maintenance, your WorkSpaces might be unavailable. By default, your Windows WorkSpaces are configured to receive updates from Windows Update.</p><p>Hence, the option that says: <strong>Launch the Windows Server on Amazon WorkSpaces. Use Amazon WorkSpaces Application Manager (WAM) to harden the server and configure the Windows automatic updates to occur every day </strong>is correct.</p><p>The option that says: <strong>Launch an AWS AppSync environment with a single EC2 instance that runs the Windows Server. Set up the environment with a custom AMI to utilize a hardened machine image that can be downloaded from AWS Marketplace. Configure the AWS Systems Manager Patch Manager to automatically apply the OS updates</strong> is incorrect because the AWS AppSync service is a fully managed service for developing GraphQL APIs, and not for creating an environment with a single Window Server instance.</p><p>The option that says: <strong>Launch a hardened machine image from the AWS Marketplace and host the server in AWS Cloud9. Set up the AWS Systems Manager Patch Manager to automatically apply system updates. Use Amazon AppStream 2.0 to act as a bastion host</strong> is incorrect because you cannot serve any machine image using AWS Cloud9. You can use Amazon WorkSpaces for this particular use case.</p><p>The option that says: <strong>Launch the server using AWS OpsWorks Stacks and implement a Chef recipe to harden the AMI automatically during instance launch. Set up a combination of Amazon EventBridge and AWS Lambda scheduled event to run the </strong><code><strong>Upgrade Operating System</strong></code><strong> stack command to apply system updates</strong> is incorrect. Although this option is possible, the solution entails a lot of issues and management overhead along the way.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/wam/latest/adminguide/what_is.html\">https://docs.aws.amazon.com/wam/latest/adminguide/what_is.html</a></p><p><a href=\"https://docs.aws.amazon.com/workspaces/latest/adminguide/workspace-maintenance.html\">https://docs.aws.amazon.com/workspaces/latest/adminguide/workspace-maintenance.html</a></p><p><br></p><p><strong>Check out this Amazon Workspaces Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-workspaces/?src=udemy\">https://tutorialsdojo.com/amazon-workspaces/</a></p></div>"
	},
	{
		"question": "<p>A medical firm uses an image analysis application that extracts data from multiple images. The input stream analyzes a batch of images and for each file, it writes the result data to an output stream of files. The number of input files per day grows and peaks for a few hours in a day. The application is hosted on an Amazon EC2 instance with a large EBS volume that hosts the input data, but the results still take almost 20 hours per day to be processed.</p><p>Which of the following solutions can be implemented to reduce the processing time and improve the availability of the application?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications."
			},
			{
				"correct": true,
				"answer": "Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue."
			},
			{
				"correct": false,
				"answer": "Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications."
			},
			{
				"correct": false,
				"answer": "Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Queue Service (SQS)</strong> is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications. Building applications from individual components that each perform a discrete function improves scalability and reliability, and is best practice design for modern applications. SQS makes it simple and cost-effective to decouple and coordinate the components of a cloud application. Using SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_s3_consumption.png\"></p><p>Therefore, the correct answer is: <strong>Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue.</strong> It provides high availability and can store the massive amount of data. Auto-scaling of EC2 instances reduces the overall processing time and SQS helps in distributing the commands/tasks to the group of EC2 instances.</p><p>The option that says: <strong>Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications </strong>is incorrect because EBS is for storage and SNS is only for notification, not for scaling.</p><p>The option that says: <strong>Store I/O files in an EBS Provisioned IOPS volume, and use SNS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the length of your SQS queue </strong>is incorrect because EBS is only for block storage and not for scaling.</p><p>The option that says: <strong>Store I/O files in S3 instead and use SQS to facilitate a group of hosts working in parallel. Include the hosts in an auto scaling group that scales accordingly to the number of SNS notifications</strong> is incorrect because SNS is only for notification and not for scaling.</p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/auto-scaling-with-sqs/\">https://aws.amazon.com/blogs/aws/auto-scaling-with-sqs/</a></p><p><br></p><p><strong>Check out these AWS Auto Scaling and Amazon SQS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
	},
	{
		"question": "<p>A leading insurance firm has several new members in its development team. The solutions architect was instructed to provision access to certain IAM users who perform application development tasks in the VPC. The access should allow the users to create and configure various AWS resources such as deploying Windows EC2 servers. In addition, the users should be able to see the permissions in AWS Organizations to view information about the user's organization, including the master account email and organization limitations.</p><p>Which of the following should the solutions architect implement to follow the standard security advice of granting the least privilege?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a new IAM role and attach the <code>AdministratorAccess</code> AWS managed policy to it. Assign the IAM Role to the IAM users.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Attach the <code>PowerUserAccess</code> AWS managed policy to the IAM users.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Attach the <code>AdministratorAccess</code> AWS managed policy to the IAM users.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new IAM role and attach the <code>SystemAdministrator</code> AWS managed policy to it. Assign the IAM Role to the IAM users.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS managed policies</strong> for job functions are designed to closely align to common job functions in the IT industry. You can use these policies to easily grant the permissions needed to carry out the tasks expected of someone in a specific job function. These policies consolidate permissions for many services into a single policy that's easier to work with than having permissions scattered across many policies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_iam_managed_policies.png\"></p><p>There are a lot of available AWS Managed Policies that you can directly attach to your IAM Users, such as Administrator, Billing, Database Administrator, Data Scientist, Developer Power User, Network Administrator, Security Auditor, System Administrator and many others.</p><p>For Administrators, you can use the AWS managed policy name: <strong>AdministratorAccess</strong> if you want to provision full access to a specific IAM User. This will enable the user to delegate permissions to every service and resource in AWS as this policy grants all actions for all AWS services and for all resources in the account.</p><p>For Developer Power Users, you can use the AWS managed policy name: <strong>PowerUserAccess</strong> if you have users who perform application development tasks. This policy will enable them to create and configure resources and services that support AWS aware application development. The first statement of this policy uses the <em>NotAction</em> element to allow all actions for all AWS services and for all resources except AWS Identity and Access Management and AWS Organizations. The second statement grants IAM permissions to create a service-linked role. This is required by some services that must access resources in another service, such as an Amazon S3 bucket. It also grants Organizations permissions to view information about the user's organization, including the master account email and organization limitations.</p><p>Therefore, the correct answer is: <strong>Attach the </strong><code><strong>PowerUserAccess</strong></code><strong> AWS managed policy to the IAM users.</strong></p><p>The options that say: <strong>Attach the AdministratorAccess AWS managed policy to the IAM users </strong>and <strong>Create a new IAM role and attach the </strong><code><strong>AdministratorAccess</strong></code><strong> AWS managed policy to it. Assign the IAM Role to the IAM users</strong> are incorrect. Although an AdministratorAccess policy can meet the requirement, it is more suitable to attach a PowerUserAccess to the IAM users since this policy can provide the required access. Take note that you have to follow the standard security best practice of granting the least privilege. In addition, a managed policy can be directly attached to your IAM Users, which is one of the reasons why the latter option is incorrect.</p><p>The option that says: <strong>Create a new IAM role and attach the </strong><code><strong>SystemAdministrator</strong></code><strong> AWS managed policy to it. Assign the IAM Role to the IAM users </strong>is incorrect because the SystemAdministrator managed policy does not have AWS Organizations permissions to view information about the user's organization such as the master account email or the organization limitations. In this scenario, you have to use PowerUserAccess instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html?shortFooter=true#access_policies_job-functions_create-policies\">https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html?shortFooter=true#access_policies_job-functions_create-policies</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A leading insurance company runs its cloud operations in the Hong Kong AWS region. As part of the audit process, an auditor has been called in to view all the logs of all API requests made to the company AWS environment.</p><p>Which of the following options is the most suitable solution for the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>The company will have to contact AWS first due to the shared responsibility model before AWS can provide the necessary access to the auditor.</p>"
			},
			{
				"correct": true,
				"answer": "Turn on CloudTrail logging, and provide the auditor an IAM user with read-only permissions to the AWS resources that will be audited, including the S3 bucket containing the CloudTrail logs."
			},
			{
				"correct": false,
				"answer": "Subscribe the auditor to an SNS topic that sends notifications via email whenever CloudTrail delivers log files to S3. Do not provide the auditor access to your AWS environment."
			},
			{
				"correct": false,
				"answer": "Create an IAM Role with the required permissions for the auditor."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Identity and Access Management (IAM)</strong> is an Amazon service that helps an administrator securely control access to Amazon resources. IAM administrators control who can be <em>authenticated</em> (signed in) and <em>authorized</em> (have permissions) to use CloudTrail resources. IAM is an Amazon service that you can use with no additional charge.</p><p>You control access in Amazon by creating policies and attaching them to IAM identities or Amazon resources. A policy is an object in Amazon that, when associated with an identity or resource, defines their permissions. You can sign in as the root user or an IAM user, or you can assume an IAM role. When you then make a request, Amazon evaluates the related identity-based or resource-based policies. Permissions in the policies determine whether the request is allowed or denied. Most policies are stored in Amazon as JSON documents.</p><p>The <strong>CloudTrail IAM policy</strong> grants permissions to users who work with CloudTrail. If you need to grant different permissions to users, you can attach a CloudTrail policy to an IAM group or to a user. You can edit the policy to include or exclude specific permissions. You can also create your own custom policy. Policies are JSON documents that define the actions a user is allowed to perform and the resources that the user is allowed to perform those actions on.</p><p>Therefore, the correct answer is: <strong>Turning on CloudTrail logging and provide the auditor an IAM user with read-only permissions to the AWS resources that will be audited, including the S3 bucket containing the CloudTrail logs.</strong> You need to enable CloudTrail logging in order to generate the logs with information about all the activities related to the AWS account and resources. It also creates an IAM user that has permissions to read the logs that are stored in the S3 bucket.</p><p>The option that says: <strong>Create an IAM Role with the required permissions for the auditor</strong> is incorrect because simply creating a role is not sufficient. You need to create a \"Trail\" that will save CloudTrail logs to an S3 bucket.</p><p>The option that says: <strong>The company will have to contact AWS first due to the shared responsibility model before AWS can provide the necessary access to the auditor</strong> is incorrect because granting the auditor access to AWS resources is not AWS' responsibility. It is the AWS user or account owner's responsibility.</p><p>The option that says:<strong> Subscribe the auditor to an SNS topic that sends notifications via email whenever CloudTrail delivers log files to S3. Do not provide the auditor access to your AWS environment</strong> is incorrect because sending the logs via email is not a good architecture and is actually a security risk since it can be easily forwarded to someone else.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-sharing-logs-create-role.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-sharing-logs-create-role.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/grant-custom-permissions-for-cloudtrail-users.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/grant-custom-permissions-for-cloudtrail-users.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p></div>"
	},
	{
		"question": "<p>A multinational manufacturing company has multiple AWS accounts in multiple AWS regions across North America, Europe, and Asia. The solutions architect has been tasked to set up AWS Organizations to centrally manage policies and have full administrative control across the multiple AWS accounts owned by the company, without requiring custom scripts and manual processes.</p><p>Which of the following options is the recommended implementation to achieve this requirement with the LEAST effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up AWS Organizations by enabling trusted access to all member AWS accounts of the company. The master account will automatically have full administrative control across all member accounts.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up AWS Organizations by sending an invitation to all member accounts of the company from the master account of your organization. Create an <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS Organizations by sending an invitation to the master account of your organization from each of the member accounts of the company. Create an <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS Organizations by establishing cross-account access from the master account to all member AWS accounts of the company. The master account will automatically have full administrative control across all member accounts.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>After you create an <strong>Organization</strong> and verify that you own the email address associated with the master account, you can invite existing AWS accounts to join your organization. When you invite an account, AWS Organizations sends an invitation to the account owner, who decides whether to accept or decline the invitation. You can use the AWS Organizations console to initiate and manage invitations that you send to other accounts. You can send an invitation to another account only from the master account of your organization.</p><p>If you are the administrator of an AWS account, you also can accept or decline an invitation from an organization. If you accept, your account becomes a member of that organization. Your account can join only one organization, so if you receive multiple invitations to join, you can accept only one.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_steps.png\"></p><p>When an invited account joins your organization, you <em>do not</em> automatically have full administrator control over the account, unlike created accounts. If you want the master account to have full administrative control over an invited member account, you must create the <code>OrganizationAccountAccessRole</code> IAM role in the member account and grant permission to the master account to assume the role.</p><p>Therefore, the correct answer is: <strong>Set up AWS Organizations by sending an invitation to all member accounts of the company from the master account of your organization. Create an </strong><code><strong>OrganizationAccountAccessRole</strong></code><strong> IAM role in the member account and grant permission to the master account to assume the role.</strong></p><p>The option that says: <strong>Set up AWS Organizations by establishing cross-account access from the master account to all member AWS accounts of the company. The master account will automatically have full administrative control across all member accounts</strong> is incorrect. Cross-account access is primarily used for scenarios where you need to grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own.</p><p>The option that says: <strong>Set up AWS Organizations by sending an invitation to the master account of your organization from each of the member accounts of the company. Create an </strong><code><strong>OrganizationAccountAccessRole</strong></code><strong> IAM role in the member account and grant permission to the master account to assume the role</strong> is incorrect. It entails a lot of effort to send an individual invitation to the master account from each of the member accounts of the company. It's stated in the scenario that you should achieve this requirement with the LEAST effort and you can do this by sending an invitation to all member accounts of the company from the master account of your organization.</p><p>The option that says: <strong>Set up AWS Organizations by enabling trusted access to all member AWS accounts of the company. The master account will automatically have full administrative control across all member accounts</strong> is incorrect. A trusted access is primarily used to enable a specific AWS service (called a trusted service) to perform tasks in your organization and its accounts on your behalf.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts.html</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p></div>"
	},
	{
		"question": "<p>A company runs a live flight tracking service hosted on the AWS cloud. The application gets updated every 10 minutes with the latest flight information from every airline. The tracking website has a global audience and uses an Auto Scaling group behind an Elastic Load Balancer and an Amazon RDS database. A simple web interface is hosted as static content on an Amazon S3 bucket. The Auto Scaling group is set to trigger a scale-up event at 90% CPU utilization. The average load time of the web page is<strong> </strong>around 7 seconds but the management wants to bring it down to less than 3 seconds.</p><p>Which combination of options will make the page load time faster in the MOST cost-effective way? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create a second installation in another region, and utilize Amazon Route 53's latency-based routing feature to direct requests to the appropriate region."
			},
			{
				"correct": false,
				"answer": "<p>Replace your existing Auto Scaling group with the AWS Systems Manager State Manager which provides a more effective way to manage and scale your EC2 instances.</p>"
			},
			{
				"correct": true,
				"answer": "Have CloudFront enable caching of re-usable content from your website."
			},
			{
				"correct": true,
				"answer": "<p>Add a caching layer using Amazon ElastiCache Service to be used for storing sessions and frequent DB queries.</p>"
			},
			{
				"correct": false,
				"answer": "Scale more frequently by setting the scale up trigger of the Auto Scaling group to 30%."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use a content delivery network (CDN) like <strong>Amazon CloudFront</strong> to improve the performance of your website by securely delivering data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. To improve performance, you can simply configure your website’s traffic to be delivered over CloudFront’s globally distributed edge network by setting up a CloudFront distribution. In addition, CloudFront offers a variety of optimization options.</p><p><strong>Amazon ElastiCache</strong> allows you to seamlessly set up, run, and scale popular open-source compatible in-memory data stores in the cloud. Build data-intensive apps or boost the performance of your existing databases by retrieving data from high throughput and low latency in-memory data stores. Both Redis and MemCached are in-memory, open-source data stores. Memcached, a high-performance distributed memory cache service, is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases.</p><p>In this scenario, you can improve the page load times of your application by using a combination of Amazon ElastiCache, CloudFront, or alternatively, an upgraded RDS instance to increase the read capacity.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_elasticache.JPG\"></p><p>The option that says: <strong>Add a caching layer using Amazon ElastiCache Service to be used for storing sessions and frequent DB queries</strong> is correct. This uses ElastiCache for storing sessions as well as frequent DB queries hence, reducing the load on the database. This should help increase the read performance.</p><p>The option that says: <strong>Having CloudFront enable caching of re-usable content from your website</strong> is correct. This uses CloudFront which is a network of globally distributed edge-locations that caches the content and improves the user experience.</p><p>The option that says: <strong>Scale more frequently by setting the scale up trigger of the Auto Scaling group to 30%</strong> is incorrect. This will increase the number of web server instances but will not reduce the load on the database and hence, will not improve the read performance.</p><p>The option that says: <strong>Create a second installation in another region, and utilize Amazon Route 53's latency-based routing feature to direct requests to the appropriate region</strong> is incorrect. This will not improve read performance. In fact, this setup would add to the cost.</p><p>The option that says: <strong>Replace your existing Auto Scaling group with the AWS Systems Manager State Manager which provides a more effective way to manage and scale your EC2 instances</strong> is incorrect. The AWS Systems Manager State Manager is a secure and scalable service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. It is not a suitable solution to improve the load times of your web application. This is primarily used to control the configuration detail of your instances in your VPC as well as your servers located in your on-premises data centers, such as server configurations, anti-virus definitions, firewall settings, and many others.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/dynamic-content/\">https://aws.amazon.com/cloudfront/dynamic-content/</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/</a></p><p><br></p><p><strong>Check out these Amazon CloudFront, Amazon ElastiCache, and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company has a CRM application that uses a MySQL database hosted in Amazon RDS, and a central data warehouse that runs on Amazon Redshift. There is a batch analytics process that runs every day and reads data from RDS. During the execution of the batch analytics, the RDS utilization spikes up, which results in the CRM application becoming unresponsive.<strong> </strong>The top management dashboard must also be updated with new data right after the batch analytics processing completes. However, the dashboard is on another system running on-premises and cannot be modified directly. The only way to update the dashboard is to send an email with the new data to the dashboard system via SMTP, which will then be parsed and processed to update the dashboard with the latest data.</p><p>How would the solutions architect optimize this scenario to solve performance issues and automate the process as much as possible?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Consider using Amazon Redshift as the main OLTP transactional database instead of RDS for the batch analytics and use Redshift Spectrum to run SQL queries directly against Exabytes of structured or unstructured data in S3 without the need for unnecessary data movement. Utilize Amazon SNS to notify the on-premises system to update the dashboard.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Consider using Amazon Redshift instead of Amazon RDS as the database for the CRM application. Use Amazon SQS to notify the on-premises system to update the dashboard.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Add read replicas for the RDS database to speed up batch analytics and use Amazon SNS to notify the on-premises system to update the dashboard.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Add read replicas for the RDS database to speed up batch analytics and use Amazon SQS to notify the on-premises system to update the dashboard.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the use of <strong>Amazon RDS Read Replicas</strong> is the best option. It provides enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances. Read replicas are available in Amazon RDS for MySQL, MariaDB, Oracle, and PostgreSQL as well as Amazon Aurora.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_read_replica_promote.png\"></p><p>Therefore, the correct answer is: <strong>Adding read replicas for the RDS database to speed up batch analytics and using Amazon SNS to notify the on-premises system to update the dashboard.</strong> It uses Read Replicas which improves the read performance, and it uses SNS which automates the process of notifying the on-premises system to update the dashboard.</p><p>The option that says: <strong>Consider using Amazon Redshift as the main OLTP transactional database instead of RDS for the batch analytics and use Redshift Spectrum to run SQL queries directly against Exabytes of structured or unstructured data in S3 without the need for unnecessary data movement. Utilize Amazon SNS to notify the on-premises system to update the dashboard </strong>is incorrect because Redshift is primarily used for OLAP scenarios whereas RDS is used for OLTP scenarios. Hence, replacing RDS with Redshift is not a valid solution. Although using Redshift Spectrum to run SQL queries is valid, it is still incorrect to replace RDS with Redshift as your main OLTP database.</p><p>The option that says: <strong>Consider using Amazon Redshift instead of Amazon RDS as the database for the CRM application. Use Amazon SQS to notify the on-premises system to update the dashboard</strong> is incorrect because Redshift is used for OLAP scenarios whereas RDS is used for OLTP scenarios. Hence, replacing RDS with Redshift is not a solution.</p><p>The option that says: <strong>Adding read replicas for the RDS database to speed up batch analytics and using Amazon SQS to notify the on-premises system to update the dashboard</strong> is incorrect because SQS is not a service to be used for sending the notification.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/big-data/datalakes-and-analytics/\">https://aws.amazon.com/big-data/datalakes-and-analytics/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company has recently finished developing a web application that will soon be put into production. Before it is transferred into the production environment, a final test run must be conducted. Only the employees can access the web app - either from the corporate network or from the Internet. The manager instructed the solutions architect to ensure that the EC2 instance hosting the application server will not be exposed to the Internet.</p><p>Which of the following options is the recommended implementation to fulfill the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Launch an Elastic Load Balancer for your EC2 instances that terminates SSL to them. </p><p>2. Create a public subnet in your VPC and launch your application servers in it.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Use IPsec VPN that would allow your employees to access the network of your application servers.&nbsp;</p><p>2. Create a public subnet in your VPC and launch your application servers in it.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Configure SSL VPN on the public subnet of your VPC. </p><p>2. Install an SSL VPN client software on all employee workstations. </p><p>3. Create a private subnet in your VPC and place your application servers in it. </p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Use AWS Direct Connect to hook up your employee workstations to the VPC via a private interface. </p><p>2. Create a public subnet and place your application servers in it.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, you have a web application which is still under development but you want to enable access to it only for the employees via public Internet. In this scenario, you can implement an SSL VPN solution in which the employees can connect first and once they are authenticated, they will be granted access to the online portal. In this way, you can launch the web servers in the private subnet and still access it over the Internet via the VPN.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_vpn.png\"></p><p>Therefore, the correct answer is:</p><p><strong>1. Configure SSL VPN on the public subnet of your VPC.</strong></p><p><strong>2. Install an SSL VPN client software on all employee workstations.</strong></p><p><strong>3. Create a private subnet in your VPC and place your application servers in it.</strong></p><p>The following option is incorrect. Even though an IPSec VPN may work, your application servers are still exposed since you launched them in a public subnet:</p><p><strong>1. Use IPsec VPN that would allow your employees to access the network of your application servers.</strong></p><p><strong>2. Create a public subnet in your VPC and launch your application servers in it.</strong></p><p>The following option is incorrect because you don't need to set up a DirectConnect connection in order to meet the requirement. Additionally, it is costly to maintain this connection considering that it is not required to have a high bandwidth connection between the customer and AWS:</p><p><strong>1. Use AWS Direct Connect to hook up your employee workstations to the VPC via a private interface.</strong></p><p><strong>2. Create a public subnet and place your application servers in it.</strong></p><p>The following option is incorrect because terminating the SSL on the EC2 instance does not meet the requirement.</p><p><strong>1. Launch an Elastic Load Balancer for your EC2 instances that terminates SSL to them.</strong></p><p><strong>2. Create a public subnet in your VPC and launch your application servers in it.</strong></p><p>The application servers are still exposed as it is deployed to a public subnet.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p><p><a href=\"https://aws.amazon.com/marketplace/pp/B00MI40CAE?qid=1532168886060&amp;sr=0-1&amp;ref_=srh_res_product_title\">https://aws.amazon.com/marketplace/pp/B00MI40CAE?qid=1532168886060&amp;sr=0-1&amp;ref_=srh_res_product_title</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A company has several financial applications hosted in AWS that uses Amazon S3 buckets to store static data. The Solutions Architect recently discovered that some employees store highly classified data into S3 buckets without proper approval. To mitigate any security risks, the Architect needs to determine all possible S3 objects that contain personally identifiable information (PII) and determine whether the data has been accessed. Due to the sheer volume of data, the Architect must implement an automated solution to accomplish this important task.</p><p>Which of the following should the solutions architect implement for this scenario?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Enable Amazon Macie on the S3 buckets to automatically classify the data and detect any objects with personally identifiable information (PII). Determine if the objects with PII have been recently accessed by tracking the <code>GET</code> API calls in AWS CloudTrail.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Install the Amazon Inspector agent on the Amazon S3 buckets. Use AWS CloudTrail to determine if the objects with personally identifiable information (PII) have been recently accessed by tracking the <code>GET</code> API calls that are used to fetch these objects.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Detect personally identifiable information (PII) on the specific S3 buckets using Amazon Athena. Set up Amazon CloudWatch to determine if the objects with PII have been recently accessed by tracking the <code>GET</code> API calls that are used to download these objects.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon GuardDuty to detect personally identifiable information (PII) on the Amazon S3 buckets. Determine if the objects with PII have been recently accessed by tracking the <code>GET</code> API calls in AWS CloudTrail that are used to download these objects.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Macie</strong> is an ML-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3. Amazon Macie uses machine learning to recognize sensitive data such as personally identifiable information (PII) or intellectual property, assigns a business value, and provides visibility into where this data is stored and how it is being used in your organization.</p><p>Amazon Macie continuously monitors data access activity for anomalies and delivers alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie has the ability to detect global access permissions inadvertently being set on sensitive data, detect uploading of API keys inside source code, and verify sensitive customer data is being stored and accessed in a manner that meets their compliance standards.</p><p><img src=\"https://media.tutorialsdojo.com/sap_macie_dashboard.jpg\"></p><p>Hence, the correct answer is: <strong>Enable Amazon Macie on the S3 buckets to automatically classify the data and detect any objects with personally identifiable information (PII). Determine if the objects with PII have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls in AWS CloudTrail.</strong></p><p>The option that says: <strong>Detect personally identifiable information (PII) on the specific S3 buckets using Amazon Athena. Set up Amazon CloudWatch to determine if the objects with PII have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls that are used to download these objects</strong> is incorrect because Amazon Athena is not capable of detecting personally identifiable information (PII) in an Amazon S3 bucket. You have to use Amazon Macie instead. In addition, you have to use AWS CloudTrail to track the <code>GET</code> API calls that are used to fetch the objects with PII.</p><p>The option that says: <strong>Use Amazon GuardDuty to detect personally identifiable information (PII) on the Amazon S3 buckets. Determine if the objects with PII have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls in AWS CloudTrail that are used to download these objects </strong>is incorrect because GuardDuty is just a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. You have to use Amazon Macie instead.</p><p>The option that says: <strong>Install the Amazon Inspector agent on the Amazon S3 buckets. Use AWS CloudTrail to determine if the objects with personally identifiable information (PII) have been recently accessed by tracking the </strong><code><strong>GET</strong></code><strong> API calls that are used to fetch these objects<em> </em></strong>is incorrect because you can only install the Amazon Inspector agent to EC2 instances and not to S3 buckets. Amazon Inspector is basically an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. You have to use Amazon Macie instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html\">https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html</a></p><p><a href=\"https://aws.amazon.com/macie/faq/\">https://aws.amazon.com/macie/faq/</a></p><p><a href=\"https://docs.aws.amazon.com/macie/index.html\">https://docs.aws.amazon.com/macie/index.html</a></p><p><br></p><p><strong>Check out this Amazon Macie Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-macie/?src=udemy\">https://tutorialsdojo.com/amazon-macie/</a></p></div>"
	},
	{
		"question": "<p>A company recently adopted a modern design for its legacy application. The new application is now suitable for native cloud deployments so the CI/CD pipelines need to be updated as well. The following deployment requirements are needed to support the new application:</p><p>- The pipeline should support deployments of new versions several times every hour.</p><p>- The pipeline should be able to quickly rollback to the previous application version if any problems are encountered on the new version.</p><p>Which of the following options is the recommended solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Re-provision a new environment for every deployment using AWS Systems Manager. Create an Amazon EC2 user data script to download the latest application artifact from an Amazon S3 bucket. Use a weighted routing policy on Amazon Route 53 to slowly shift traffic to the newer version.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Package the newer application version on AMIs including the needed configurations. Reconfigure the CI/CD pipeline to deploy this AMI by replacing the current Amazon EC2 instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Reconfigure the pipeline to create a Staging environment on AWS Elastic Beanstalk. Deploy the newer version on the Staging environment. Swap the Staging and Production environment URLs to shift traffic to the newer version.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Package the newer application version on AMIs including the needed configurations. Update the Launch Template of the Auto Scaling group and trigger a scale-out to use this new AMI. Ensure that the configured termination policy is to delete the old instances using the previous AMI.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Elastic Beanstalk</strong> provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, Immutable, and Traffic splitting) and options that let you configure the batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's a scalable environment, it uses rolling deployments.</p><p>You can use the AWS Elastic Beanstalk console to upload an updated source bundle and deploy it to your Elastic Beanstalk environment or redeploy a previously uploaded version. The following list provides summary information about the different deployment policies and adds related considerations.</p><p><strong>All at once</strong> – The quickest deployment method. Suitable if you can accept a short loss of service, and if quick deployments are important to you. With this method, Elastic Beanstalk deploys the new application version to each instance.</p><p><strong>Rolling</strong> – Avoids downtime and minimizes reduced availability, at a cost of a longer deployment time. Suitable if you can't accept any period of completely lost service. With this method, your application is deployed to your environment one batch of instances at a time.</p><p><strong>Rolling with additional batch</strong> – Avoids any reduced availability, at a cost of an even longer deployment time compared to the Rolling method. Suitable if you must maintain the same bandwidth throughout the deployment. With this method, Elastic Beanstalk launches an extra batch of instances, then performs a rolling deployment.</p><p><strong>Immutable</strong> – A slower deployment method that ensures your new application version is always deployed to new instances, instead of updating existing instances. It also has the additional advantage of a quick and safe rollback in case the deployment fails.</p><p><strong>Traffic splitting</strong> – A canary testing deployment method. Suitable if you want to test the health of your new application version using a portion of incoming traffic while keeping the rest of the traffic served by the old application version.</p><p>For deployments that depend on resource configuration changes or a new version that can't run alongside the old version, you can launch a new environment with the new version and perform a CNAME swap for a <strong>blue/green deployment</strong>.</p><p>Because AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time. You can avoid this downtime by performing a blue/green deployment, where you deploy the new version to a separate environment, and then <strong>swap CNAMEs of the two environments</strong> to redirect traffic to the new version instantly. With this method, you can have two independent environments and you can quickly switch between the version by swapping the URLs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_elastic_beanstalk_swap_env_urls.png\"></p><p>Therefore, the correct answer is: <strong>Reconfigure the pipeline to create a Staging environment on AWS Elastic Beanstalk. Deploy the newer version on the Staging environment. Swap the Staging and Production environment URLs to shift traffic to the newer version.</strong> This is a blue/green deployment on Elastic Beanstalk. You can deploy a newer version without affecting the current version and quickly rollback by just swapping the URLs again.</p><p>The option that says: <strong>Package the newer application version on AMIs including the needed configurations. Reconfigure the CI/CD pipeline to deploy this AMI by replacing the current Amazon EC2 instances </strong>is incorrect. This is a possible deployment procedure, however, the rollback procedure will take too long because it will need to replace the current EC2 instances again.</p><p>The option that says: <strong>Re-provision a new environment for every deployment using AWS Systems Manager. Create an Amazon EC2 user data script to download the latest application artifact from an Amazon S3 bucket. Use a weighted routing policy on Amazon Route 53 to slowly shift traffic to the newer version</strong> is incorrect. AWS Systems Manager cannot re-provision a new environment for your application. The rollback for this procedure will take longer too, as there is a need to re-create newer instances again to run the EC2 user data script.</p><p>The options that says: <strong>Package the newer application version on AMIs including the needed configurations. Update the Launch Template of the Auto Scaling group and trigger a scale-out to use this new AMI. Ensure that the configured termination policy is to delete the old instances using the previous AMI</strong> is incorrect. This is also a possible deployment, however, it is not recommended for this scenario. With this solution, you will have to re-deploy the older AMI in case of a rollback which takes more time compared to just swapping the environment URLs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html</a></p><p><br></p><p><strong>Check out the AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p></div>"
	},
	{
		"question": "<p>A leading telecommunications company is moving all of its mission-critical, multi-tier applications to AWS. At present, their architecture is composed of desktop client applications and several servers that are all located in their on-premises data center. The application-tier is using a MySQL database that is hosted on a single VM while both the presentation and business logic layers are distributed across multiple VMs. There has been a lot of reports that their users, who access the applications remotely, are experiencing increased connection latency and slow load times.</p><p>Which of the following is the MOST cost-effective solution to improve the uptime of the application with MINIMAL change and improve the overall user experience?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Using Amazon WorkSpaces, set up and allocate a workspace for each user to improve the overall user experience. Migrate the MySQL database from your VM to a self-hosted MySQL database in a large EC2 instance. Host the application and presentation layers in Amazon ECS containers behind an Application Load Balancer.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use Amazon AppStream 2.0 to centrally manage your desktop applications and improve the overall user experience. Migrate the MySQL database from your VM to Amazon Aurora. Host the application and presentation layers in an Auto Scaling group on EC2 instances behind an Application Load Balancer.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon ElastiCache to improve the overall user experience of your desktop applications. Directly migrate the MySQL database from your VM to a DynamoDB database. Host the application and presentation layers in AWS Fargate containers behind an Application Load Balancer.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a new CloudFront web distribution to improve the overall user experience of your desktop applications. Migrate the MySQL database from your VM to a Redshift cluster. Host the application and presentation layers in ECS containers behind a Network Load Balancer.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon AppStream 2.0</strong> is a fully managed application streaming service. You centrally manage your desktop applications on AppStream 2.0 and securely deliver them to any computer. You can easily scale to any number of users across the globe without acquiring, provisioning, and operating hardware or infrastructure. AppStream 2.0 is built on AWS, so you benefit from a data center and network architecture designed for the most security-sensitive organizations. Each user has a fluid and responsive experience with your applications, including GPU-intensive 3D design and engineering ones, because your applications run on virtual machines (VMs) optimized for specific use cases and each streaming session automatically adjusts to network conditions.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_appstream_overview.png\"></p><p>Enterprises can use AppStream 2.0 to simplify application delivery and complete their migration to the cloud. Educational institutions can provide every student access to the applications they need for class on any computer. Software vendors can use AppStream 2.0 to deliver trials, demos, and training for their applications with no downloads or installations. They can also develop a full software-as-a-service (SaaS) solution without rewriting their application.</p><p>Therefore, the correct answer is the option that says: <strong>Use Amazon AppStream 2.0 to centrally manage your desktop applications and improve the overall user experience. Migrate the MySQL database from your VM to Amazon Aurora. Host the application and presentation layers in an Auto Scaling group on EC2 instances behind an Application Load Balancer.</strong></p><p>The option that says: <strong>Set up a new CloudFront web distribution to improve the overall user experience of your desktop applications. Migrate the MySQL database from your VM to a Redshift cluster. Host the application and presentation layers in ECS containers behind a Network Load Balancer</strong> is incorrect. It is not suitable to migrate your MySQL database to a Redshift cluster.</p><p>The option that says: <strong>Use Amazon ElastiCache to improve the overall user experience of your desktop applications. Directly migrate the MySQL database from your VM to a DynamoDB database. Host the application and presentation layers in AWS Fargate containers behind an Application Load Balancer</strong> is incorrect. You cannot directly migrate your MySQL database, which is relational in type, to DynamoDB which is a NoSQL database or non-relational. The use of Amazon ElastiCache alone will not significantly improve the overall user experience since this service is primarily used for caching.</p><p>The option that says: <strong>Using Amazon WorkSpaces, set up and allocate a workspace for each user to improve the overall user experience. Migrate the MySQL database from your VM to a self-hosted MySQL database in a large EC2 instance. Host the application and presentation layers in Amazon ECS containers behind an Application Load Balancer</strong> is incorrect. Amazon WorkSpaces is primarily used as a managed, secure Desktop-as-a-Service (DaaS) solution and it costs a lot to implement. Take note that in the scenario, it was mentioned that they are using desktop applications but there was no mention about the need for Windows or Linux desktops which is why the use of Amazon WorkSpaces is incorrect.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/appstream2/\">https://aws.amazon.com/appstream2/</a></p><p><a href=\"https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html\">https://docs.aws.amazon.com/appstream2/latest/developerguide/what-is-appstream.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company is running its enterprise resource planning application in AWS that handles supply chain, order management, and delivery tracking. The architecture has a set of RESTful web services that enable third-party companies to search for data that will be consumed by their respective applications. The public web services consist of several AWS Lambda functions. DynamoDB is used for its database-tier and is integrated with an Amazon ES domain, which stores the indexes and supports the search feature. A Solutions Architect has been instructed to ensure that in the event of a failed deployment, there should be no downtime, and a system should be in place to prevent subsequent deployments. The service must strictly maintain full capacity during API deployment without any reduced compute capacity to avoid degradation of the service. </p><p>Among the options below, which can the Architect use to meet the requirements in the MOST efficient way?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the DynamoDB tables, Lambda functions, and Amazon ES domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>All at Once</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS CloudFormation, launch the Amazon DynamoDB tables, AWS Lambda functions, and Amazon ES domain in your AWS VPC. Launch the application to an Amazon S3 static web hosting and enable cross-region replication.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS CloudFormation, launch the Amazon DynamoDB tables, AWS Lambda functions, and Amazon ES domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Immutable</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Do an in-place deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the Amazon DynamoDB tables, Lambda functions, and Amazon ES domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to <code>Rolling</code>.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Elastic Beanstalk</strong> provides several options for how deployments are processed, including deployment policies (All at once, Rolling, Rolling with additional batch, and Immutable) and options that let you configure batch size and health check behavior during deployments. By default, your environment uses all-at-once deployments. If you created the environment with the EB CLI and it's an automatically scaling environment (you didn't specify the <code>--single</code> option), it uses rolling deployments.</p><p>With rolling deployments, Elastic Beanstalk splits the environment's EC2 instances into batches and deploys the new version of the application to one batch at a time, leaving the rest of the instances in the environment running the old version of the application. During a rolling deployment, some instances serve requests with the old version of the application, while instances in completed batches serve other requests with the new version.</p><p><img src=\"https://media.tutorialsdojo.com/sap_elastic_beanstalk_immutable_configuration.png\"></p><p>To maintain full capacity during deployments, you can configure your environment to launch a new batch of instances before taking any instances out of service. This option is known as a rolling deployment with an additional batch. When the deployment completes, Elastic Beanstalk terminates the additional batch of instances.''</p><p>Immutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.</p><p>Therefore, the correct answer is: <strong>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS CloudFormation, launch the Amazon DynamoDB tables, AWS Lambda functions, and Amazon ES domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Immutable</strong></code><strong>.</strong></p><p>The option that says: <strong>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the DynamoDB tables, Lambda functions, and Amazon ES domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>All at Once</strong></code><strong> </strong>is incorrect. This policy deploys the new version to all instances simultaneously which means that the instances in your environment are out of service for a short time while the deployment occurs.</p><p>The option that says: <strong>Do an in-place deployment on all upcoming changes using AWS CodeDeploy. Using AWS SAM, launch the Amazon DynamoDB tables, Lambda functions, and Amazon ES domain in your AWS VPC. Host the web application in AWS Elastic Beanstalk and set the deployment policy to </strong><code><strong>Rolling</strong></code><strong> </strong>is incorrect. This policy will deploy the new version in batches where each batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.</p><p>The option that says: <strong>Do a blue/green deployment on all upcoming changes using AWS CodeDeploy. Using AWS CloudFormation, launch the Amazon DynamoDB tables, AWS Lambda functions, and Amazon ES domain in your AWS VPC. Launch the application to an Amazon S3 static web hosting and enable cross-region replication</strong> is incorrect. You can't host a dynamic web application in Amazon S3. A better solution would be to launch your application in Elastic Beanstalk.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html#welcome-deployment-overview</a></p><p><a href=\"https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html\">https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-immutable.html</a></p><p><br></p><p><strong>Check out this AWS Elastic Beanstalk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-beanstalk/?src=udemy\">https://tutorialsdojo.com/aws-elastic-beanstalk/</a></p></div>"
	},
	{
		"question": "<p>A leading financial company runs its application in an Amazon ECS Cluster. The application processes a large stream of intraday data and stores the generated result in a DynamoDB database. To comply with the financial regulatory policy, the solutions architect was tasked to design a system that will detect new entries in the DynamoDB table then automatically run tests to verify the results using a Lambda function.</p><p>Which of the following options can satisfy the company requirement with minimal configuration?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Detect the new entries in the DynamoDB table using Systems Manager Automation then automatically invoke the Lambda function for processing.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Run the Lambda function using SNS each time the ECS Cluster successfully processes financial data.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a CloudWatch Alarm to automatically trigger the Lambda function whenever a new entry is created in the DynamoDB table.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up DynamoDB Streams to detect the new entries and automatically trigger the Lambda function.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon DynamoDB</strong> is integrated with <strong>AWS Lambda</strong> so that you can create <em>triggers</em>—pieces of code that automatically respond to events in DynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.</p><p>If you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_lambda_triggers.png\"></p><p>You can create a Lambda function which can perform a specific action that you specify, such as sending a notification or initiating a workflow. For instance, you can set up a Lambda function to simply copy each stream record to persistent storage, such as EFS or S3, to create a permanent audit trail of write activity in your table.</p><p>Suppose you have a mobile gaming app that writes to a <code>TutorialsDojoCourses</code> table. Whenever the <code>TopCourse</code> attribute of the <code>TutorialsDojoScores</code> table is updated, a corresponding stream record is written to the table's stream. This event could then trigger a Lambda function that posts a congratulatory message on a social media network. (The function would simply ignore any stream records that are not updates to <code>TutorialsDojoCourses</code> or that do not modify the <code>TopCourse</code> attribute.)</p><p>Therefore, the correct answer is: <strong>Set up DynamoDB Streams to detect the new entries and automatically trigger the Lambda function.</strong> In this way, the requirement can be met with minimal configuration change. DynamoDB streams can be used as an event source to automatically trigger Lambda functions whenever there is a new entry.</p><p>The option that says: <strong>Run the Lambda function using SNS each time the ECS Cluster successfully processes financial data</strong> is incorrect. You don't need to create an SNS topic just to invoke Lambda functions. You can simply enable DynamoDB streams to meet the requirement with less configuration.</p><p>The option that says: <strong>Set up a CloudWatch Alarm to automatically trigger the Lambda function whenever a new entry is created in the DynamoDB table</strong> is incorrect. CloudWatch Alarms only monitor service metrics, not changes in DynamoDB table data.</p><p>The option that says: <strong>Detect the new entries in the DynamoDB table using Systems Manager Automation then automatically invoking the Lambda function for processing</strong> is incorrect. The Systems Manager Automation service is primarily used to simplify common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. It does not have the capability to detect new entries in a DynamoDB table.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB cheat sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A medical research team in a university is working to analyze historical patient records. The outcome is an intuitive mobile app that shows the user's overall health record and medications. Currently, the highly sensitive health records of the patients s are stored in an Amazon EC2 instance with an attached EBS data volume. As part of the security compliance, it is mandated that all of the data stored in the cloud infrastructure are properly secured and encrypted.</p><p>Which of these options would allow the solutions architect to encrypt the data at rest? (Select THREE.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Use native data encryption drivers present at the file system level of encrypting your data."
			},
			{
				"correct": true,
				"answer": "<p>Encrypt your data prior to storing them on EBS.</p>"
			},
			{
				"correct": true,
				"answer": "Take advantage of third-party volume encryption software."
			},
			{
				"correct": false,
				"answer": "All EBS volumes after provisioning are encrypted by default."
			},
			{
				"correct": false,
				"answer": "Apply SSL/TLS for all your services running on the server."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon EBS</strong> offers a straight-forward encryption solution of data at rest , data in transit, and all volume backups. Amazon EBS encryption is supported by all volume types, and includes built-in key management infrastructure without having you to build, maintain, and secure your own keys.</p><p>Amazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure. When you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted:</p><p>- Data at rest inside the volume</p><p>- All data moving between the volume and the instance</p><p>- All snapshots created from the volume</p><p>- All volumes created from those snapshots</p><p>Alternatively, you can encrypt the data at rest by using either a native data encryption, a third party encrypting tool, or just encrypt the data before storing on the volume.</p><p>You can create Amazon Machine Images (AMIs) that make use of encrypted EBS boot volumes and use the AMIs to launch EC2 instances. The stored data is encrypted, as is the data transfer path between the EBS volume and the EC2 instance. The data is decrypted on the hypervisor of that instance on an as-needed basis, then stored only in memory.</p><p>Therefore, the following options are correct:</p><p><strong>- Take advantage of third-party volume encryption software.</strong></p><p><strong>- Encrypt your data prior to storing them on EBS.</strong></p><p><strong>- Using native data encryption drivers present at the file system level of encrypting your data.</strong></p><p>The option that says: <strong>Apply SSL/TLS for all your services running on the server</strong> is incorrect. SSL/TLS is used for the security of the data in transit, not at rest.</p><p>The option that says: <strong>All EBS volumes after provisioning are encrypted by default</strong> is incorrect. Newly created EBS volumes are not encrypted by default unless you manually specify it.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/data-at-rest-encryption-with-amazon-ebs.html\">https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/data-at-rest-encryption-with-amazon-ebs.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 and Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A tech company is about to undergo a financial audit. It has been planned to use a third-party web application that needs to have certain AWS access to issue several API commands. It will discover Amazon EC2 resources running within the enterprise's account. The company has internal security policies that require any outside access to its environment to conform to the principles of least privilege. The solutions architect must ensure that the credentials used by the third-party vendor cannot be used by any other third party. The third-party vendor also has an AWS account where it runs its web application and it already provided a unique customer ID, including their AWS account number.</p><p>Which of the following options would allow the solutions architect to give permissions to the third-party vendor in compliance with the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create an IAM user in the enterprise account that has permissions allowing only the actions required by the third-party application. Also generate a new access key and secret key from the user to be given to the third-party provider."
			},
			{
				"correct": true,
				"answer": "<p>Create a new IAM role for the 3rd-party vendor. Add a permission policy that only allows the actions required by the third party application. Also, add a trust policy with a <code>Condition</code> element for the <code>ExternalId</code> context key. The Condition must test the <code>ExternalId</code> context key to ensure that it matches the unique customer ID from the 3rd party vendor.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Connect to allow the third-party application to access your AWS resources. In the AWS Connect configuration, input the <code>ExternalId</code> context key to ensure that it matches the unique customer ID of the 3rd party vendor.</p>"
			},
			{
				"correct": false,
				"answer": "Provide your own access key and secret key to the third-party software."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>At times, you need to give third party access to your AWS resources (delegate access). One important aspect of this scenario is the External ID, optional information that you can use in an IAM role trust policy to designate who can assume the role.</p><p>To use an external ID, update a role trust policy with the external ID of your choice. Then, when someone uses the AWS CLI or AWS API to assume that role, they must provide the external ID.</p><p>For example, let's say that you decide to hire a third-party company called Boracay Corp to monitor your AWS account and help optimize costs. In order to track your daily spending, Boracay Corp needs to access your AWS resources. Boracay Corp also monitors many other AWS accounts for other customers.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iam_assume_role_cross_account.png\"></p><p>Do not give Boracay Corp access to an IAM user and its long-term credentials in your AWS account. Instead, use an IAM role and its temporary security credentials. An IAM role provides a mechanism to allow a third party to access your AWS resources without needing to share long-term credentials (for example, an IAM user's access key).</p><p>You can use an IAM role to establish a trusted relationship between your AWS account and the Boracay Corp account. After this relationship is established, a member of the Boracay Corp account can call the AWS STS AssumeRole API to obtain temporary security credentials. The Boracay Corp members can then use the credentials to access AWS resources in your account.</p><p>When a user, a resource, an application, or any service needs to access any AWS service or resource, always opt to create an appropriate role that has the least privileged access or only the required access, rather than using any other credentials such as keys.</p><p>Therefore, the correct answer is: <strong>Create a new IAM role for the 3rd-party vendor. Add a permission policy that only allows the actions required by the third party application. Also, add a trust policy with a </strong><code><strong>Condition</strong></code><strong> element for the </strong><code><strong>ExternalId</strong></code><strong> context key. The Condition must test the </strong><code><strong>ExternalId</strong></code><strong> context key to ensure that it matches the unique customer ID from the 3rd party vendor.</strong></p><p>The option that says: <strong>Use Amazon Connect to allow the third-party application to access your AWS resources. In the AWS Connect configuration, input the </strong><code><strong>ExternalId</strong></code><strong> context key to ensure that it matches the unique customer ID of the 3rd party vendor </strong>is incorrect because Amazon Connect is simply an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost. You should use an IAM Role in this scenario instead of Amazon Connect.</p><p>The option that says: <strong>Provide your own access key and secret key to the third-party software</strong> is incorrect because you should never share your access and secret keys.</p><p>The option that says: <strong>Create an IAM user in the enterprise account that has permissions allowing only the actions required by the third-party application and generating a new access key and secret key from the user to be given to the third-party provider</strong> is incorrect because when a user is created, its security credentials are stored in the EC2 which can be compromised, and the creation of the appropriate role is always the better solution rather than creating a user.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company wants to improve the security of its cloud resources by ensuring that all running EC2 instances were launched from pre-approved AMIs only, which are set by the Security team. Their Development team has an agile CI/CD process which should not be stalled by the new automated solution that they’ll implement. Any new application release must be deployed first before the solution could analyze if it is using a pre-approved AMI or not.</p><p>Which of the following options enforces the required controls with the LEAST impact on the development process? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set up a scheduled Lambda function to search through the list of running EC2 instances within your VPC and determine if any of these are based on unauthorized AMIs. Afterward, publish a new message to an SNS topic to inform the Security team that this occurred and then terminate the EC2 instance.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up Amazon Inspector to do regular scans using a custom assessment template to determine if the EC2 instance is based upon a pre-approved AMI. Terminate the instances and inform the Security team by email about the security breach.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up AWS Config rules to determine any launches of EC2 instances based on non-approved AMIs and then trigger an AWS Lambda function to automatically terminate the instance. Afterward, publish a message to an SNS topic to inform the Security team about the occurrence.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up the required policies, roles, and permissions to a centralized IT Operations team, which will manually process the security approval steps to ensure that EC2 instances are only launched from pre-approved AMIs.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\"></p><p>AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p><strong>AWS Config</strong> provides <strong>AWS managed rules</strong>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS manage rule which checks whether running instances are using specified AMIs. You can also use a Lambda function which is scheduled to run regularly to scan all of the running EC2 instances in your VPC and check if there is an instance that was launched using an unauthorized AMI. Hence, the following options are the correct answers:</p><p><strong>- Set up AWS Config rules to determine any launches of EC2 instances based on non-approved AMIs and then trigger an AWS Lambda function to automatically terminate the instance. Afterward, publish a message to an SNS topic to inform the Security team about the occurrence.</strong></p><p><strong>- Set up a scheduled Lambda function to search through the list of running EC2 instances within your VPC and determine if any of these are based on unauthorized AMIs. Afterward, publish a new message to an SNS topic to inform the Security team that this occurred and then terminate the EC2 instance.</strong></p><p>The option that says: <strong>Set up the required policies, roles, and permissions to a centralized IT Operations team, which will manually process the security approval steps to ensure that EC2 instances are only launched from pre-approved AMIs</strong> is incorrect because having manual information security approval will impact the development process. A better solution is to implement an automated process using AWS Config and a scheduled AWS Lambda function.</p><p>The option that says: <strong>Set up IAM policies to restrict the ability of users to launch EC2 instances based on a specific set of pre-approved AMIs which were tagged by the Security team</strong> is incorrect because setting up an IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs which could impact their CI/CD process. The scenario clearly says that the solution should not have any interruption in the company's development process.</p><p>The option that says:<strong> Set up Amazon Inspector to do regular scans using a custom assessment template to determine if the EC2 instance is based upon a pre-approved AMI. Terminate the instances and inform the Security team by email about the security breach</strong> is incorrect because the Amazon Inspector service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p></div>"
	},
	{
		"question": "<p>A company has a web service portal on which users can perform read and write operations to structured data. The company wants to refactor the current application and leverage AWS-managed services to have more scalability and higher availability. To ensure optimal user experience, the service is expected to respond to short but significant system load spikes. The service must be highly available and fault-tolerant in the event of a regional AWS failure.</p><p>Which of the following options is the suitable solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use DocumentDB to store the structured data. Create an edge-optimized Amazon API Gateway and AWS Lambda-based web service, and set it as the origin of a global Amazon CloudFront distribution. Add the company’s domain as an alternate name on the CloudFront distribution. Create an Amazon Route 53 Alias record pointed to the CloudFront distribution.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a primary Amazon S3 bucket to store the structured data. Enable S3 Cross-Region Replication to sync objects to the backup region. Create an Amazon API Gateway and AWS Lambda-based web service on each region, and set them as the origin for the two Amazon CloudFront distributions. Add the company’s domain as an alternate name on the CloudFront distributions. Create Amazon Route 53 Alias records pointed to each distribution using a failover routing policy.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon DynamoDB global table to store the structured data on two Regions. Use on-demand capacity mode to allow DynamoDB scaling. Run the web service on an Auto Scaling Amazon ECS Fargate cluster on each region. Place each Fargate cluster behind their own Application Load Balancer (ALB). Create Amazon Route 53 Alias records pointed to each ALB in using latency routing policy with health checks enabled.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon Aurora global database to store the structured data in two Regions. Configure Auto Scaling replicas on both regions. Run the web service on an Auto Scaling group of Amazon EC2 instances on both regions using the user data script to download the application code. Place each Auto Scaling group behind their own Application Load Balancer (ALB). Create a single Amazon Route 53 Alias record pointed to each ALB in using a multi-value answer routing policy with health checks enabled.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon DynamoDB</strong> is a fast, fully-managed NoSQL database service that makes it simple and cost effective to store and retrieve any amount of data, and serve any level of request traffic. DynamoDB helps offload the administrative burden of operating and scaling a highly-available distributed database cluster. This storage alternative meets the latency and throughput requirements of highly demanding applications by providing single-digit millisecond latency and predictable performance with seamless throughput and storage scalability.</p><p>DynamoDB stores structured data in tables, indexed by primary key, and allows low-latency read and write access to items ranging from 1 byte up to 400 KB. DynamoDB supports three data types (number, string, and binary), in both scalar and multi-valued sets. It supports document stores such as JSON, XML, or HTML in these data types. Tables do not have a fixed schema, so each data item can have a different number of attributes. The primary key can either be a single-attribute hash key or a composite hash-range key.</p><p><strong>Global tables</strong> build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_global_table.JPG\"></p><p>DynamoDB global tables are ideal for massively scaled applications with globally dispersed users. In such an environment, users expect very fast application performance. Global tables provide automatic multi-active replication to AWS Regions worldwide. They enable you to deliver low-latency data access to your users no matter where they are located.</p><p><strong>AWS Fargate</strong> is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS). Fargate removes the need to provision and manage servers, lets you specify and pay for resources per application, and improves security through application isolation by design. Fargate allocates the right amount of compute, eliminating the need to choose instances and scale cluster capacity. Fargate runs each task or pod in its own kernel providing the tasks and pods their own isolated compute environment.</p><p>Therefore, the correct answer is: <strong>Create an Amazon DynamoDB global table to store the structured data on two Regions. Use on-demand capacity mode to allow DynamoDB scaling. Run the web service on an Auto Scaling Amazon ECS Fargate cluster on each region. Place each Fargate cluster behind their own Application Load Balancer (ALB). Create Amazon Route 53 Alias records pointed to each ALB in using latency routing policy with health checks enabled. </strong>DynamoDB global table offers a multi-active database on multiple regions. AWS Fargate clusters scale up or down significantly faster than EC2 instances because they are only containers and can be provisioned quickly. Latency routing policy with health checks enabled ensures that users are sent to the fastest healthy region based on their location.</p><p>The option that says: <strong>Use DocumentDB to store the structured data. Create an edge-optimized Amazon API Gateway and AWS Lambda-based web service, and set it as the origin of a global Amazon CloudFront distribution. Add the company’s domain as an alternate name on the CloudFront distribution. Create an Amazon Route 53 Alias record pointed to the CloudFront distribution</strong> is incorrect. DocumentDb does not offer a native cross-region replication or multi-region operation. You can only automate snapshots to another region which can take some time to restore in the event of regional failure.</p><p>The option that says: <strong>Create a primary Amazon S3 bucket to store the structured data. Enable S3 Cross-Region Replication to sync objects to the backup region. Create an Amazon API Gateway and AWS Lambda-based web service on each region, and set them as the origin for the two Amazon CloudFront distributions. Add the company’s domain as an alternate name on the CloudFront distributions. Create Amazon Route 53 Alias records pointed to each distribution using a failover routing policy</strong> is incorrect. Although you can store structured and unstructured data, there is a delay of up to 15 minutes in Cross-Region Replication. In the event of a failover, some data might not have been replicated yet on the secondary region.</p><p>The option that says: <strong>Create an Amazon Aurora global database to store the structured data in two Regions. Configure Auto Scaling replicas on both regions. Run the web service on an Auto Scaling group of Amazon EC2 instances on both regions using the user data script to download the application code. Place each Auto Scaling group behind their own Application Load Balancer (ALB). Create a single Amazon Route 53 Alias record pointed to each ALB in using a multi-value answer routing policy with health checks enabled</strong> is incorrect. Scaling EC2 instances can take a few minutes because the EBS volumes need to be provisioned and the OS needs to load along with the user script. This scaling time might not be quick enough for the expected short but significant spikes on the system load.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html\">https://docs.aws.amazon.com/whitepapers/latest/big-data-analytics-options/amazon-dynamodb.html</a></p><p><a href=\"https://aws.amazon.com/dynamodb/global-tables/\">https://aws.amazon.com/dynamodb/global-tables/</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html</a></p><p><a href=\"https://aws.amazon.com/fargate/\">https://aws.amazon.com/fargate/</a></p><p><br></p><p><strong>Check out these Amazon DynamoDB and AWS Fargate Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p></div>"
	},
	{
		"question": "<p>A company is running a serverless backend API service on AWS. It has several AWS Lambda functions written in Python and an Amazon API Gateway that is configured to invoke the functions. The company wants to secure the API endpoint by ensuring that only authorized IAM users or roles can access the Amazon API Gateway endpoint. The Solutions Architect was also tasked to provide the ability to inspect each request end-to-end to check the latency of the request and to generate service maps.</p><p>Which of the following implementation will fulfill the above company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Write a separate AWS Lambda function that will act as a custom authorizer. For every call to the API gateway, require the client to pass the access key and secret key. Use the Lambda function to validate the key/secret pair against a valid IAM user. Trace and analyze each user request on API Gateway by using AWS X-Ray.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Ensure that the API Gateway resource is secured by only returning the company’s domain in Access-Control-Allow-Origin headers and enabling Cross-origin resource sharing (CORS). Create the IAM users or roles that have the <code>execute-api:Invoke</code> permission to the ARN of the API resource. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure authorization to use <code>AWS_IAM</code> for the API Gateway method. Create the IAM users or roles that have the <code>execute-api:Invoke</code> permission to the ARN of the API resource. Enable request signing with AWS Signature for every call to the API endpoint. Trace and analyze each user request on API Gateway by using AWS X-Ray.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Generate a new client certificate on Amazon API Gateway. Distribute this certificate to all AWS users or roles that require access to the API endpoint. Ensure that each user will pass the client certification for every request made to the API endpoint. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>API Gateway</strong> supports multiple mechanisms for controlling and managing access to your API.</p><p>You can use the following mechanisms for authentication and authorization:</p><p><strong>Resource policies</strong> let you create resource-based policies to allow or deny access to your APIs and methods from specified source IP addresses or VPC endpoints.</p><p><strong>Standard AWS IAM roles and policies</strong> offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs, as well as who can invoke them.</p><p><strong>IAM tags</strong> can be used together with IAM policies to control access.</p><p><strong>Endpoint policies for interface VPC endpoints</strong> allow you to attach IAM resource policies to interface VPC endpoints to improve the security of your private APIs.</p><p><strong>Lambda authorizers</strong> are Lambda functions that control access to REST API methods using bearer token authentication—as well as information described by headers, paths, query strings, stage variables, or context variables request parameters. Lambda authorizers are used to control who can invoke REST API methods.</p><p><strong>Amazon Cognito user pools</strong> let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.</p><p><img src=\"https://media.tutorialsdojo.com/API_GATEWAY_AWS_IAM.png\"></p><p>You can enable IAM authentication for an API method in the API Gateway console. Then, you can use IAM policies and resource policies to designate permissions for your API's users. Here are the general steps to implement this:</p><p>In the API Gateway console, enable IAM authentication for your API method by going to <strong>Settings</strong> &gt; <strong>Authorization</strong>, and choosing <strong>AWS_IAM</strong>.</p><p>Deploy the API to ensure the changes are applied.</p><p>Grant API authorization to a group of IAM users by creating an IAM policy document with the required permissions.</p><p>For the IAM policy, ensure that it allows the <code>execute-api:Invoke</code> action and the resource is set to the ARN of the API gateway method.</p><p>Attach this policy to the required IAM users or IAM roles.</p><p><strong>AWS X-Ray</strong> helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. AWS X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application’s underlying components. You can use X-Ray to analyze both applications in development and in production, from simple three-tier applications to complex microservices applications consisting of thousands of services.</p><p>Therefore, the correct answer is: <strong>Configure authorization to use </strong><code><strong>AWS_IAM</strong></code><strong> for the API Gateway method. Create the IAM users or roles that have the </strong><code><strong>execute-api:Invoke</strong></code><strong> permission to the ARN of the API resource. Enable request signing with AWS Signature for every call to the API endpoint. Trace and analyze each user request on API Gateway by using AWS X-Ray.</strong></p><p>The option that says: <strong>Ensure that the API Gateway resource is secured by only returning the company’s domain in Access-Control-Allow-Origin headers and enabling Cross-origin resource sharing (CORS). Create the IAM users or roles that have the </strong><code><strong>execute-api:Invoke</strong></code><strong> permission to the ARN of the API resource. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs</strong> is incorrect because CORS does not provide any IAM authentication capability. Although API Gateway can send execution logs to CloudWatch Logs, you can only view the logs with it for monitoring purposes. It does not provide end-to-end tracing or inspection for the request.</p><p>The option that says: <strong>Write a separate AWS Lambda function that will act as a custom authorizer. For every call to the API gateway, require the client to pass the access key and secret key. Use the Lambda function to validate the key/secret pair against a valid IAM user. Trace and analyze each user request on API Gateway by using AWS X-Ray </strong>is incorrect. Creating a custom Lambda authorizer will require more work compared to just using the AWS_IAM authorizer. Sending AWS access and secret key as part of each HTTPS call is not recommended from a security standpoint since these are sensitive security credentials.</p><p>The option that says: <strong>Generate a new client certificate on Amazon API Gateway. Distribute this certificate to all AWS users or roles that require access to the API endpoint. Ensure that each user will pass the client certification for every request made to the API endpoint. Trace and analyze each user request on API Gateway using Amazon CloudWatch Logs </strong>is incorrect. Using client certificates for authentication does not provide any IAM authentication capability and although API Gateway can send execution logs to CloudWatch Logs, the logs do not provide end-to-end tracing or inspection for request.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-iam-policy-examples-for-api-execution.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-iam-policy-examples-for-api-execution.html</a></p><p><a href=\"https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-authentication-api-gateway/</a></p><p><br></p><p><strong>Check out the AWS X-Ray and Amazon API Gateway Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/aws-x-ray/?src=udemy\">https://tutorialsdojo.com/aws-x-ray/</a></p></div>"
	},
	{
		"question": "<p>A company recently patched a vulnerability in its web application hosted on AWS. The solutions architect was tasked to improve the security of the company’s AWS resources as well as secure the web applications from common web vulnerabilities and cyber attacks. One example is a Distributed Denial of Service attack (DDoS) in which there is numerous incoming traffic coming from many different locations that simultaneously target the company web application and floods the network with bogus requests.</p><p>Which of the following options are recommended strategies for reducing DDoS attack surface and minimizing the blast radius in the cloud infrastructure? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Configure the Network Access Control Lists (ACLs) to only allow the required ports to your network. Identify and block common DDoS request patterns to effectively mitigate a DDoS attack by using AWS WAF.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Add Elastic Load Balancing and Auto Scaling to your EC2 instances to improve availability and scalability. Use extra large EC2 instances to accommodate a surge of incoming traffic caused by a DDoS attack and utilize AWS Systems Manager Session Manager to filter all client-side web sessions to your instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Allow versioning in your S3 bucket. Ensure that the OS of all of your EC2 instances are properly patched using Systems Manager Patch Manager.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Strictly implement Multi-Factor Authentication (MFA) in AWS. Use a combination of AWS Systems Manager State Manager, AWS Config, and Trusted Advisor to fortify your AWS resources.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Always add a security group that only allows certain ports and authorized servers and protects your origin servers by putting it behind a CloudFront distribution. Enable AWS Shield Advanced which provides enhanced DDoS attack detection and monitoring for application-layer traffic to your AWS resources.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Another important consideration when architecting on AWS is to limit the opportunities that an attacker may have to target your application. For example, if you do not expect an end-user to directly interact with certain resources, you will want to make sure that those resources are not accessible from the Internet. Similarly, if you do not expect end-users or external applications to communicate with your application on certain ports or protocols, you will want to make sure that traffic is not accepted. This concept is known as attack surface reduction. Resources that are not exposed to the Internet are more difficult to attack, which limits the options an attacker might have to target the availability of your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_waf_shield_cloudfront.png\"></p><p><strong>AWS Shield</strong> is a managed DDoS protection service that is available in two tiers: Standard and Advanced. AWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency. AWS Shield Standard is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost. When you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks. Customers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.</p><p>The following options are both correct as they are best practices for reducing the DDoS attack surface which then limit the extent to which your application is exposed:</p><p><strong>1. Always add a security group that only allows certain ports and authorized servers and protect your origin servers by putting it behind a CloudFront distribution. Enable AWS Shield Advanced which provides enhanced DDoS attack detection and monitoring for application-layer traffic to your AWS resources.</strong></p><p><strong>2. Configure the Network Access Control Lists (ACLs) to only allow the required ports to your network. Identify and block common DDoS request patterns to effectively mitigate a DDoS attack by using AWS WAF.</strong></p><p>Using AWS Shield together with AWS WAF rules provide a comprehensive DDoS attack mitigation strategy.</p><p>The option that says:<strong><em> </em>Add Elastic Load Balancing and Auto Scaling to your EC2 instances to improve availability and scalability. Use extra large EC2 instances to accommodate a surge of incoming traffic caused by a DDoS attack and utilize AWS Systems Manager Session Manager to filter all client-side web sessions to your instances</strong> is incorrect. Although it improves the scalability of your network in case of an ongoing DDoS attack, it simply absorbs the heavy application layer traffic and doesn't minimize the attack surface in your cloud architecture. In addition, AWS Systems Manager Session Manager is primarily used to provide secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys, but not to filter client-side web sessions.</p><p>The options that say: <strong>Allow versioning in your S3 bucket. Ensure that the OS of all of your EC2 instances are properly patched using Systems Manager Patch Manager</strong> and <strong>strictly implement Multi-Factor Authentication (MFA) in AWS. Use a combination of AWS Systems Manager State Manager, AWS Config, and Trusted Advisor to fortify your AWS resources </strong>are incorrect because MFA, as well as the Versioning feature in S3, don't minimize the DDoS attack surface area. Although it is recommended that all of your instances are properly patched using the Systems Manager Patch Manager, it is still not enough to protect your cloud infrastructure against DDoS attacks. In addition, the AWS Systems Manager State Manager is just a configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This service will not help you minimize the blast radius of a security attack.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/\">https://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/</a></p><p><a href=\"https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf\">https://d1.awsstatic.com/whitepapers/Security/DDoS_White_Paper.pdf</a></p><p><a href=\"https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf\">https://d0.awsstatic.com/whitepapers/DDoS_White_Paper_June2015.pdf</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company has multiple database servers hosted on extra-large Reserved Amazon EC2 instances which are all deployed to a private subnet. A single NAT instance is in place to allow the servers to fetch data from the Internet. The solutions architect noticed that whenever there is a new database patch update, the processing takes a lot of time which results in request time-outs. As a workaround, the developers just manually re-run the database patch update on the servers that failed to complete the process the first time.</p><p>What could be the possible root cause of the issue and what steps should the solutions architect implement to solve it?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "There is no Virtual Private Gateway attached to the VPC that links up to the Customer Gateway of the database provider. Simply add the missing gateway and the issue will be resolved"
			},
			{
				"correct": false,
				"answer": "The database servers are not in a Placement Group, which means that the inter-instance communications are not optimal. This is causing the timeout issue. Place all the database servers on either a Spread or a Cluster type Placement group to fix the problem."
			},
			{
				"correct": false,
				"answer": "There is no Internet Gateway (IGW) attached to the VPC. Simply add an IGW and the issue will be resolved."
			},
			{
				"correct": true,
				"answer": "The timeout behavior of a NAT instance is that, when there is a connection time out, it sends a FIN packet to resources behind the NAT instance to close the connection. It does not attempt to continue the connection which is why some database updates are failing. For better performance, use a NAT Gateway instead. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>NAT gateway</strong> is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances.</p><p>The NAT gateway replaces the source IPv4 address of the instances with the private IP address of the NAT gateway. When sending response traffic to the instances, the NAT device translates the addresses back to the original source IPv4 addresses.</p><p>When you create a NAT gateway, you specify one of the following connectivity types:</p><p><strong>Public</strong> – (Default) Instances in private subnets can connect to the internet through a public NAT gateway, but cannot receive unsolicited inbound connections from the internet. You create a public NAT gateway in a public subnet and must associate an elastic IP address with the NAT gateway at creation. You route traffic from the NAT gateway to the internet gateway for the VPC. Alternatively, you can use a public NAT gateway to connect to other VPCs or your on-premises network. In this case, you route traffic from the NAT gateway through a transit gateway or a virtual private gateway.</p><p><strong>Private</strong> – Instances in private subnets can connect to other VPCs or your on-premises network through a private NAT gateway. You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. You cannot associate an elastic IP address with a private NAT gateway. You can attach an internet gateway to a VPC with a private NAT gateway, but if you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic.</p><p>Take note of the following difference between a NAT Instance and a NAT Gateway when handling a timeout:</p><p><strong>NAT Instance</strong> - When there is a connection time out, a NAT instance sends a FIN packet to resources behind the NAT instance to close the connection.</p><p><strong>NAT Gateway</strong> - When there is a connection time out, a NAT gateway returns an RST packet to any resources behind the NAT gateway that attempt to continue the connection (it does not send a FIN packet).</p><p><img src=\"https://media.tutorialsdojo.com/sap_nat_subnet.png\"></p><p>Therefore, the correct answer is: <strong>The timeout behavior of a NAT instance is that, when there is a connection time out, it sends a FIN packet to resources behind the NAT instance to close the connection. It does not attempt to continue the connection which is why some database updates are failing. For better performance, use a NAT Gateway instead.</strong></p><p>The option that says: <strong>There is no Internet Gateway (IGW) attached to the VPC. Simply add an IGW and the issue will be resolved</strong> is incorrect. If there is no Internet Gateway attached to the VPC, any communication to the outside internet will fail, which is not the case here.</p><p>The option that says: <strong>There is no Virtual Private Gateway attached to the VPC that links up to the Customer Gateway of the database provider. Simply add the missing gateway and the issue will be resolved</strong> is incorrect. A Virtual Private Gateway is used for VPN connections. This will not resolve the issue for this scenario.</p><p>The option that says: <strong>The database servers are not in a Placement Group, which means that the inter-instance communications are not optimal. This is causing the timeout issue. Place all the database servers on either a Spread or a Cluster type Placement group to fix the problem</strong> is incorrect. Placement groups are just allocation of servers to be as close together as possible. This does not improve the performance of the NAT instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A startup currently runs a web application on an extra-large Amazon EC2 instance. The application allows users to upload and download various pdf files from a private Amazon S3 bucket using a pre-signed URL. The web application checks if the file being requested actually exists in the S3 bucket before generating the URL.</p><p>In this scenario, how should the solutions architect configure the web application to access the Amazon S3 bucket securely?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>1. Create an IAM role with a policy that allows listing and uploading of the objects in the S3 bucket. Launch the EC2 instance with the IAM role. </p><p>2. Program your web application to retrieve the temporary security credentials from the EC2 instance metadata.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Create an IAM role with a policy that allows listing of the objects in the S3 bucket. Launch the EC2 instance with the IAM role.</p><p>2. Program your web application to retrieve the temporary security credentials from the EC2 instance user data.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Create an IAM user with the appropriate permissions allowing access and listing of all of the objects of the S3 bucket. Associate the EC2 instance with the IAM user. </p><p>2. Program your web application to retrieve the user credentials from the EC2 instance metadata. </p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Store your access keys inside the EC2 instance. </p><p>2. Program your web application to retrieve the AWS credentials from the instance to interact with the objects in the S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Applications that run on an EC2 instance must include AWS credentials in their AWS API requests. You could store AWS credentials directly within the EC2 instance and allow applications in that instance to use those credentials. But you would then have to manage the credentials and ensure that they securely pass the credentials to each instance and update each EC2 instance when it's time to rotate the credentials. That's a lot of additional work. Instead, you can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. When you use a role, you don't have to distribute long-term credentials (such as a user name and password or access keys) to an EC2 instance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role_s3_bucket.png\"></p><p>Instead, the role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests.</p><p>When the application runs, it obtains temporary security credentials from Amazon EC2 instance metadata. These are temporary security credentials that represent the role and are valid for a limited period of time to access various AWS resources. You can fetch the temporary security credentials from the instance by requesting it from this endpoint:</p><p><code>curl http://169.254.169.254/latest/meta-data/iam/security-credentials/s3access</code></p><p>In this particular scenario, you have to use a combination of IAM Role and EC2 instance metadata to provide the web application the required access it needs to access the S3 bucket. Hence, the correct answer is the following option:</p><p><strong>1. Create an IAM role with a policy that allows listing and uploading of the objects in the S3 bucket. Launch the EC2 instance with the IAM role.</strong></p><p><strong>2. Program your web application to retrieve the temporary security credentials from the EC2 instance metadata.</strong></p><p><br></p><p>The following option is incorrect because it is a security vulnerability to store the AWS credentials in your EC2 instances:</p><p><strong>1. Store your access keys inside the EC2 instance.</strong></p><p><strong>2. Program your web application to retrieve the AWS credentials from the instance to interact with the objects in the S3 bucket.</strong></p><p>Anyone who has access to that EC2 instance can find your AWS credentials and hence, this is not a recommended option.</p><p><br></p><p>The following option is incorrect because you should have used an IAM Role instead of an IAM User:</p><p><strong>1. Create an IAM user with the appropriate permissions allowing access and listing of all of the objects of the S3 bucket. Associate the EC2 instance with the IAM user.</strong></p><p><strong>2. Program your web application to retrieve the user credentials from the EC2 instance metadata.</strong></p><p><br></p><p>The following option is incorrect because you should retrieve the IAM role's credentials from the EC2 instance metadata and not from the user data:</p><p><strong>1. Create an IAM role with a policy that allows listing of the objects in the S3 bucket. Launch the EC2 instance with the IAM role.</strong></p><p><strong>2. Program your web application to retrieve the temporary security credentials from the EC2 instance user data.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company runs a suite of web applications in AWS. The application is hosted in an Auto Scaling group of On-Demand Amazon EC2 instances behind an Application Load Balancer that handles traffic from multiple web domains. The solutions architect is responsible for securing the system by allowing multiple domains to serve SSL traffic without the need to re-authenticate and re-provision a new certificate whenever a new domain name is added. This change of architecture from HTTP to HTTPS will help improve the SEO and Google search ranking of the web application.</p><p>Which of the following options are valid solutions to meet the above requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Add a Subject Alternative Name (SAN) for each additional domain to your certificate.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a Classic Load Balancer instead of an Application Load Balancer. Upload all SSL certificates of the domains and use Server Name Indication (SNI).</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a new CloudFront web distribution and configure it to serve HTTPS requests using dedicated IP addresses in order to associate your alternate domain names with a dedicated IP address in each CloudFront edge location.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a wildcard certificate to handle multiple sub-domains and different domains.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>SNI Custom SSL</strong> relies on the SNI extension of the Transport Layer Security protocol, which allows multiple domains to serve SSL traffic over the same IP address by including the hostname which the viewers are trying to connect to.</p><p>You can host multiple TLS secured applications, each with its own TLS certificate, behind a single load balancer. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. These features are provided at no additional charge.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_sni_custom_ssl.gif\"></p><p>You can use your own SSL certificates with <strong>Amazon CloudFront</strong> at no additional charge with Server Name Indication (SNI) Custom SSL. SNI is supported by most modern browsers, and provides an efficient way to deliver content over HTTPS using your own domain and SSL certificate. Amazon CloudFront delivers your content from each edge location and offers the same security as the Dedicated IP Custom SSL feature.</p><p>The option that says: <strong>Upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI)</strong> is correct. You can upload all SSL certificates of the domains in the ALB using the console and bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client using Server Name Indication (SNI).</p><p>The option that says: <strong>Create a new CloudFront web distribution and configure it to serve HTTPS requests using dedicated IP addresses in order to associate your alternate domain names with a dedicated IP address in each CloudFront edge location</strong> is correct. You can configure Amazon CloudFront to require viewers to interact with your content over an HTTPS connection using the HTTP to HTTPS Redirect feature. If you configure CloudFront to serve HTTPS requests using SNI, CloudFront associates your alternate domain name with an IP address for each edge location. The IP address to your domain name is determined during the SSL/TLS handshake negotiation and isn’t dedicated to your distribution.<strong><br></strong></p><p>The option that says: <strong>Use a wildcard certificate to handle multiple sub-domains and different domains</strong> is incorrect. A wildcard certificate can only handle multiple sub-domains but not different domain names.</p><p>The option that says: <strong>Add a Subject Alternative Name (SAN) for each additional domain to your certificate</strong> is incorrect. Although using Subject Alternative Name (SAN) is correct, you will still have to reauthenticate and reprovision your certificate every time you add a new domain. One of the requirements in the scenario is that you should not have to reauthenticate and reprovision your certificate hence, this solution is incorrect.</p><p>The option that says: <strong>Use a Classic Load Balancer instead of an Application Load Balancer. Upload all SSL certificates of the domains and use Server Name Indication (SNI)</strong> is incorrect because a Classic Load Balancer does not support SNI.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\">https://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-https-dedicated-ip-or-sni.html#cnames-https-dedicated-ip</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>SNI Custom SSL vs Dedicated IP Custom SSL:</strong></p><p><a href=\"https://tutorialsdojo.com/sni-custom-ssl-vs-dedicated-ip-custom-ssl/?src=udemy\">https://tutorialsdojo.com/sni-custom-ssl-vs-dedicated-ip-custom-ssl/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A company plans to migrate its on-premises workload to the AWS cloud. The solutions architect has been tasked to perform a Total Cost of Ownership (TCO) analysis and prepare a cost-optimized migration plan for the systems hosted in your on-premises network to AWS. It is required to collect detail about configuration, usage, and behavioral data from the on-premises servers to help better understand the current workloads before doing the migration.</p><p>Which of the following option is the recommended solution that should be implemented to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use the AWS Application Discovery Service to gather data about your on-premises data center and perform the TCO analysis.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS SAM service to move your data to AWS which will also help you perform the TCO analysis.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Migration Hub service to collect data from each server in your on-premises data center and perform the TCO analysis.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Server Migration Service (SMS) to migrate VM servers to AWS and collect data required to complete your TCO analysis.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Application Discovery Service</strong> helps enterprise customers plan migration projects by gathering information about their on-premises data centers.</p><p>Planning data center migrations can involve thousands of workloads that are often deeply interdependent. Server utilization data and dependency mapping are important early first steps in the migration process. AWS Application Discovery Service collects and presents configuration, usage, and behavior data from your servers to help you better understand your workloads.</p><p><img src=\"https://media.tutorialsdojo.com/sap_application_dicovery.png\"></p><p>The collected data is retained in encrypted format in an AWS Application Discovery Service data store. You can export this data as a CSV file and use it to estimate the Total Cost of Ownership (TCO) of running on AWS and to plan your migration to AWS. In addition, this data is also available in AWS Migration Hub, where you can migrate the discovered servers and track their progress as they get migrated to AWS.</p><p>Therefore, the correct answer is: <strong>Use the AWS Application Discovery Service to gather data about your on-premises data center and perform the TCO analysis.</strong></p><p>The option that says: <strong>Use the AWS SAM service to move your data to AWS which will also help you perform the TCO analysis</strong> is incorrect. The AWS Serverless Application Model (AWS SAM) service is just an open-source framework that you can use to build serverless applications on AWS. It is not a suitable migration service to be used for your on-premises systems.</p><p>The option that says: <strong>Use the AWS Migration Hub service to collect data from each server in your on-premises data center and performing the TCO analysis</strong> is incorrect. AWS Migration Hub simply provides a single location to track the progress of application migrations across multiple AWS and partner solutions. Using Migration Hub just allows you to choose the AWS and partner migration tools that best fits your needs while providing visibility into the status of migrations across your portfolio of applications. Although AWS Application Discovery Service can be integrated with AWS Migration Hub, this service alone is not enough to meet the requirement in this scenario.</p><p>The option that says: <strong>Use the AWS Server Migration Service (SMS) to migrate VM servers to AWS and collecting data required to complete your TCO analysis</strong> is incorrect. Although AWS Server Migration Service (SMS) is a migration tool, it does not collect configuration, usage, and behavior data from your on-premises servers unlike AWS Application Discovery Service.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/application-discovery/\">https://aws.amazon.com/application-discovery/</a></p><p><a href=\"https://aws.amazon.com/server-migration-service/?nc=sn&amp;loc=1\">https://aws.amazon.com/server-migration-service/?nc=sn&amp;loc=1</a></p><p><a href=\"https://aws.amazon.com/cloud-migration/\">https://aws.amazon.com/cloud-migration/</a></p><p><br></p><p><strong>Check out this AWS Server Migration Service (SMS) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-server-migration-service-sms/?src=udemy\">https://tutorialsdojo.com/aws-server-migration-service-sms/</a></p><p><br></p><p><strong>AWS Migration Services Overview:</strong></p><p><a href=\"https://youtu.be/yqNBkFMnsL8\">https://youtu.be/yqNBkFMnsL8</a></p></div>"
	},
	{
		"question": "<p>The www.tutorialsdojonews.com website is using the WordPress platform that runs on a fleet of Amazon EC2 instances behind an application load balancer to deliver news around the globe. There are a lot of customers complaining about the slow loading time of the website. The solutions architect has created a CloudFront distribution and set the ALB as the origin to improve the read performance. After several days, the IT Security team reported that the setup is not secure and it should enable end-to-end HTTPS connections from the user's browser to the origin via CloudFront.</p><p>Which of the following options should the solutions architect implement to satisfy the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. Generate a new SSL certificate on AWS Certificate Manager and use it as the CloudFront distribution and origin certificate.</p>"
			},
			{
				"correct": false,
				"answer": "Use third-party CA certificate on both the origin and CloudFront."
			},
			{
				"correct": false,
				"answer": "<p>Configure CloudFront to use its default certificate. Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. For the origin, generate a new SSL certificate on AWS Certificate Manager.</p>"
			},
			{
				"correct": false,
				"answer": "Use a self-signed certificate in both the origin and CloudFront."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\"></p><p>If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store.</p><p>Therefore, the correct answer is: <strong>Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. Generate a new SSL certificate on AWS Certificate Manager and use it as the CloudFront distribution and origin certificate.</strong> You can use ACM to generate a valid SSL certificate for your ALB and CloudFront distribution.</p><p>The option that says: <strong>Use a third-party CA certificate on both the origin and CloudFront</strong> is incorrect. You can use a third-party CA certificate on both the custom origin and CloudFront, however, it is cost-effective to just use an ACM-generated SSL as it also supports automatic renewal.</p><p>The options that says:<strong> Use a self-signed certificate in both the origin and CloudFront</strong> is incorrect. The website is hosted in Amazon EC2 and the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec, or other third-party providers.</p><p>The option that says: <strong>Configure CloudFront to use its default certificate. Configure the CloudFront distribution to redirect HTTP to HTTPS protocol. For the origin, generate a new SSL certificate on AWS Certificate Manager </strong>is incorrect. You cannot use the default certificate in CloudFront since the website is using a custom domain (www.tutorialsdojonews.com). You can generate a valid SSL certificate for your domain on AWS Certificate Manager.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A data analytics company is running simulations on a high-performance computing (HPC) cluster in AWS. The compute node and storage are tightly coupled to achieve the best performance possible. The running simulations on the cluster produce thousands of large files stored on an Amazon EFS share that is shared across 200 Amazon EC2 instances. Several more simulation jobs need to be run on the cluster so the number of nodes has been increased to 1000 instances. However, the bigger cluster performed below the expectations of the company. The Solutions Architect was tasked to implement a solution that will achieve maximum performance from the HPC cluster.</p><p>Which of the following options are the recommended actions to achieve this? (Select THREE.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Improve the performance by placing all the compute nodes as close to each other. Re-launch all the Amazon EC2 instances within a single Availability Zone in a cluster placement group.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Improve the network performance of each node by using Amazon EC2 instances with an Elastic Fabric Adapter (EFA) network interface.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Improve the network performance of each node by attaching multiple network interfaces to the Amazon EC2 instances. This ensures that the network bandwidth is not a bottleneck.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Improve the performance and scalability of the HPC cluster by spreading the Amazon EC2 instances into multiple Availability Zones. Improve the storage performance by using Amazon EBS volumes configured on RAID 0 mode. This allows for much higher IOPS compared to Amazon EFS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Improve the storage performance by using Amazon EBS Throughput Optimized volumes configured on RAID 0 mode. This allows for much higher IOPS compared to Amazon EFS.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Improve the storage performance by implementing Amazon FSx for Lustre instead of using Amazon EFS.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the shared Amazon EFS storage and the spreading of the compute nodes on multiple AZs cause sluggishness on both the storage and network performance of the HPC cluster. For a tightly-coupled HPC cluster, you want the Amazon EC2 instances to be launched on a single Availability Zone.</p><p><img src=\"https://media.tutorialsdojo.com/sap_az_placement_group.png\"></p><p>When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p><p><strong>Cluster</strong> – packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is <strong>typical of HPC applications.</strong></p><p><strong>Partition</strong> – spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, and Kafka.</p><p><strong>Spread</strong> – strictly places a small group of instances across distinct underlying hardware to reduce correlated failures.</p><p><strong>Elastic Fabric Adapter (EFA)</strong> is a network interface for Amazon EC2 instances that enables customers to run applications requiring high levels of inter-node communications at scale on AWS. Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, which is critical to scaling these applications. EFA’s unique OS bypass networking mechanism provides a low-latency, low-jitter channel for inter-instance communications. This enables your tightly-coupled HPC or distributed machine learning applications to scale to thousands of cores, making your applications run faster.</p><p><img src=\"https://media.tutorialsdojo.com/sap_fsx_for_lustre.jpg\"></p><p><strong>Amazon FSx for Lustre</strong> makes it easy and cost-effective to launch and run the popular, high-performance Lustre file system. You use Lustre for workloads where speed matters, such as machine learning, high-performance computing (HPC), video processing, and financial modeling. Amazon FSx file systems provide up to multiple GB/s of throughput and hundreds of thousands of IOPS. The specific amount of throughput and IOPS that your workload can drive on your file system depends on the throughput capacity and storage capacity configuration of your file system, along with the nature of your workload, including the size of the active working set.</p><p>The open-source Lustre file system is designed for applications that require fast storage—where you want your storage to keep up with your computing capacity. Lustre was built to solve the problem of quickly and cheaply processing the world's ever-growing datasets. It's a widely used file system designed for the fastest computers in the world. It provides submillisecond latencies, up to hundreds of Gbps of throughput, and up to millions of IOPS.</p><p>Therefore, the correct answers are:</p><p><strong>- Improve the performance by placing all the compute nodes as close to each other. Re-launch all the Amazon EC2 instances within a single Availability Zone in a cluster placement group.</strong></p><p><strong>- Improve the network performance of each node by using Amazon EC2 instances with an Elastic Fabric Adapter (EFA) network interface.</strong></p><p><strong>- Improve the storage performance by implementing Amazon FSx for Lustre instead of using Amazon EFS.</strong></p><p>The option that says: <strong>Improve the network performance of each node by attaching multiple network interfaces to the Amazon EC2 instances. This ensures that the network bandwidth is not a bottleneck</strong> is incorrect. Although this may increase the network performance of the individual compute node, the Amazon EFS could still be a bottleneck because there are too many instances accessing the EFS shared storage simultaneously. A better solution is to use Amazon FSx for Lustre which supports multiple GB/s of throughput and hundreds of thousands of IOPS.</p><p>The option that says: <strong>Improve the performance and scalability of the HPC cluster by spreading the Amazon EC2 instances into multiple Availability Zones</strong> is incorrect. The HPC cluster relies on tight communication between the compute nodes and the storage solution. Spreading the EC2 instances into multiple Availability Zones will lower the performance of the cluster because the underlying physical servers will have to communicate with each other over long distances.</p><p>The option that says: <strong>Improve the storage performance by using Amazon EBS Throughput Optimized volumes configured on RAID 0 mode. This allows for much higher IOPS compared to Amazon EFS</strong> is incorrect. The cluster relies on shared storage across all the compute nodes. A Throughput Optimized EBS volume cannot be shared between EC2 instances. The Amazon EBS Multi-Attach feature is only applicable for Provisioned IOPS SSD volumes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><a href=\"https://aws.amazon.com/hpc/efa/\">https://aws.amazon.com/hpc/efa/</a></p><p><a href=\"https://aws.amazon.com/fsx/lustre/\">https://aws.amazon.com/fsx/lustre/</a></p><p><a href=\"https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html\">https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html</a></p><p><br></p><p><strong>Check out the Amazon FSx Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-fsx/?src=udemy\">https://tutorialsdojo.com/amazon-fsx/</a></p></div>"
	},
	{
		"question": "<p>A large software company has an on-premises LDAP server and a web application hosted on its VPC in AWS. The solutions architect has established an IPSec VPN connection between the AWS VPC and the company’s on-premises network. The company wants to enable employees to access the web application and other AWS resources using the same corporate account used inside the company network.</p><p>Which of the following actions should the solutions architect implement to achieve the company requirements? (SELECT TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create an identity broker that authenticates against STS to assume an IAM role to generate temporary AWS security credentials. For user authentication, configure the web application to call the identity broker to get AWS temporary security credentials."
			},
			{
				"correct": true,
				"answer": "<p>Configure the web application to authenticate against the on-premises LDAP server and retrieve the name of an IAM role associated with the user. The application then calls the STS to assume that IAM role. The application can use the temporary credentials to access any AWS resource.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Integrate the on-premises LDAP server with IAM so the users can log into IAM using their corporate LDAP credentials. Once authenticated, they can use the temporary credentials to access any AWS resource.</p>"
			},
			{
				"correct": true,
				"answer": "Launch an identity broker that authenticates against LDAP server and then calls STS to get IAM federated user credentials. Configure the web application to call the identity broker that you created to get IAM federated user credentials with access to the appropriate AWS service."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>If your identity store is not compatible with SAML 2.0, then you can build a custom identity broker application to perform a similar function. The broker application authenticates users, requests temporary credentials for users from AWS, and then provides them to the user to access AWS resources.</p><p>To enable corporate employees to access the company's AWS resources, you can develop a custom identity broker application. The application verifies that employees are signed into the existing identity and authentication system of the company (which might use LDAP, Active Directory, or another system). The identity broker application then obtains temporary security credentials for the employees. To get temporary security credentials, the identity broker application calls either the <code>AssumeRole</code> or <code>GetFederationToken</code> actions in STS to obtain temporary security credentials.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap.png\"></p><p>The option that says: <strong>Configure the web application to authenticate against the on-premises LDAP server and retrieve the name of an IAM role associated with the user. The application then calls the STS to assume that IAM role. The application can use the temporary credentials to access any AWS resource</strong> is correct as it properly authenticates users using LDAP, gets the security token from STS, and then accesses the S3 bucket using the temporary credentials.</p><p>The option that says: <strong>Launch an identity broker that authenticates against LDAP server and then calls STS to get IAM federated user credentials. Configure the web application to call the identity broker that you created to get IAM federated user credentials with access to the appropriate AWS service</strong> is correct as it properly provides access to the users using STS and IAM. You can develop an identity broker that authenticates users against LDAP then gets the temporary security token from STS, which can be used to access the S3 bucket using the IAM federated user credentials.</p><p>The option that says: <strong>Create an identity broker that authenticates against STS to assume an IAM role to generate temporary AWS security credentials. For user authentication, configure the web application to call the identity broker to get AWS temporary security credentials</strong> is incorrect as the users need to be authenticated using LDAP first and not via STS. In addition, the temporary credentials to log into AWS should be provided by STS and not the identity broker.</p><p>The option that says: <strong>Integrate the on-premises LDAP server with IAM so the users can log into IAM using their corporate LDAP credentials. Once authenticated, they can use the temporary credentials to access any AWS resource</strong> is incorrect as you cannot use the LDAP credentials to log into IAM.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A leading commercial bank has a hybrid network architecture and is extensively using AWS for its day-to-day operations. The bank uses an Amazon S3 bucket to store sensitive bank records. It has versioning enabled and does not have any encryption. The new solutions architect for the company was asked to implement Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C) for the Amazon S3 bucket to ensure data inside it is secured both at rest and in transit.</p><p>Which of the following options should the solutions architect implement to achieve the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>For Amazon S3 REST API calls, use the following HTTP Request Headers:\n\n<code>x-amz-server-side​-encryption​-customer-algorithm</code> \n<code>x-amz-server-side​-encryption​-customer-key</code>\n<code>x-amz-server-side​-encryption​-customer-key-MD5</code></p>"
			},
			{
				"correct": false,
				"answer": "<p>For presigned URLs, specify the algorithm using the <code>x-amz-server-side​-encryption​-customer-key-MD5</code> request header</p>"
			},
			{
				"correct": false,
				"answer": "Only use the S3 console to upload and update objects with SSE-C encryption."
			},
			{
				"correct": false,
				"answer": "Use WSS (WebSocket Secure)"
			},
			{
				"correct": true,
				"answer": "<p>For presigned URLs, specify the algorithm using the <code>x-amz-server-side​-encryption​-customer-algorithm</code> request header</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Server-side encryptio</strong>n is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption as it writes to disks, and decryption when you access your objects. Therefore, you don't need to maintain any code to perform data encryption and decryption. The only thing you do is manage the encryption keys you provide.</p><p>When you upload an object, Amazon S3 uses the encryption key you provide to apply AES-256 encryption to your data and removes the encryption key from memory. Amazon S3 will reject any requests made over HTTP when using SSE-C. For security reasons, it is recommended that you consider any key you send erroneously using HTTP to be compromised. You should discard the key and rotate as appropriate.</p><p>At the time of object creation—that is, when you are uploading a new object or making a copy of an existing object—you can specify if you want Amazon S3 to encrypt your data by adding the <code>x-amz-server-side-encryption</code> header to the request. Set the value of the header to the encryption algorithm <code>AES256</code> that Amazon S3 supports. Amazon S3 confirms that your object is stored using server-side encryption by returning the response header <code>x-amz-server-side-encryption</code>.</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_sse_encryption.png\"></p><p>Therefore, the following options are correct:</p><p><strong>- For Amazon S3 REST API calls, you have to include the following HTTP Request Headers:</strong></p><p><code><strong>x-amz-server-side-encryption-customer-algorithm</strong></code></p><p><code><strong>x-amz-server-side-encryption-customer-key</strong></code></p><p><code><strong>x-amz-server-side-encryption-customer-key-MD5</strong></code></p><p><strong>- For presigned URLs, you should specify the algorithm using the </strong><code><strong>x-amz-server-side-encryption-customer-algorithm</strong></code><strong> request header.</strong></p><p>The option that says: <strong>Using WSS (WebSocket Secure)</strong> is incorrect as you have to use HTTPS and not WSS.</p><p>The option that says: <strong>Specifying the algorithm using the </strong><code><strong>x-amz-server-side​-encryption​-customer-key-MD5</strong></code><strong> request header for presigned URLs </strong>is incorrect. You should use the <code>x-amz-server-side-encryption-customer-algorithm</code> request header instead.</p><p>The option that says:<strong> Only use the S3 console to upload and update objects with SSE-C encryption</strong> is incorrect because you should use S3 REST APIs instead of the S3 web console.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-use-the-rest-api-to-encrypt-s3-objects-by-using-aws-kms/\">https://aws.amazon.com/blogs/security/how-to-use-the-rest-api-to-encrypt-s3-objects-by-using-aws-kms/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-s3-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/specifying-s3-encryption.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A startup is building a mobile app and a custom GraphQL API backend that lets people post photos and videos of road potholes, faulty street lights, bridge damages, and other issues in the public infrastructure with 100-character summaries. The data gathered by the system will be used by the department of public works to facilitate fast resolution. The developers used a javascript-based React Native mobile framework so that it would run on various mobile and tablet devices. The app will be connecting to a custom GraphQL API that will be responsible for storing the photos and videos in an Amazon S3 bucket and will also access a DynamoDB table to store the summaries. The developers have recently deployed the mobile app prototype but it was found that there is an availability issue with the custom GraphQL API. To proceed with the project, the team decided to remove the API and instead, re-model the mobile app so that it will directly connect to both DynamoDB and S3 as well as handle user authentication.</p><p>Which of the following options provides the most cost-effective and scalable architecture for this project?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Set up a web identity federation using the AssumeRoleWithSAML API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Set up a web identity federation using Cognito and social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Configure the IAM role in Cognito to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Set up a web identity federation using the AssumeRole API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.\n</p><p>2. Create an IAM user for that provider and set up permissions for the IAM user to allow access to S3 and DynamoDB. \n</p><p>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With <strong>web identity federatio</strong>n, you don't need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. They can receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account. Using an IdP helps you keep your AWS account secure, because you don't have to embed and distribute long-term security credentials with your application.</p><p>In this scenario, you have a mobile app that needs to have access to the DynamoDB and S3 bucket. You can achieve this by using Web Identity Federation with AssumeRoleWithWebIdentity API which provides temporary security credentials and an IAM role.</p><p><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\"></p><p>Thus, the correct answer here is the following option:</p><p><strong>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect because you cannot use AssumeRole API and an IAM user in this scenario:</p><p><strong>1. Set up a web identity federation using the AssumeRole API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM user for that provider and set up permissions for the IAM user to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect because you should have used AssumRoleWithWebIdentity instead of AssumeRoleWithSAML API:</p><p><strong>1. Set up a web identity federation using the AssumeRoleWithSAML API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS temporary security credentials to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect because it is a security risk to store and use the AWS access and secret keys from the mobile app itself:</p><p><strong>1. Set up a web identity federation using the AssumeRoleWithWebIdentity API of STS and register with social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Create an IAM role for that provider and set up permissions for the IAM role to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p>The following option is incorrect. Even though the use of Cognito is valid, it is wrong to store and use the AWS access and secret keys from the mobile app itself. This is a security risk and you should use the temporary security credentials instead:</p><p><strong>1. Set up a web identity federation using Cognito and social identity providers like Amazon, Google, Facebook or any other OpenID Connect (OIDC)-compatible IdP.</strong></p><p><strong>2. Configure the IAM role in Cognito to allow access to S3 and DynamoDB.</strong></p><p><strong>3. The mobile app will use the AWS access and secret keys to store the photos and videos to an S3 bucket and persist the summaries to the DynamoDB database.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company hosts its application on several Amazon EC2 instances inside a VPC. A known security vulnerability was discovered in the outdated Operating System of the company's EC2 fleet. The solutions architect is responsible for mitigating the vulnerability as soon as possible to safeguard your systems from various cybersecurity attacks. In addition, it is also required to record all of the changes to patches and association compliance statuses.</p><p>Which of the following options is the easiest way to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up Amazon QuickSight and Kibana to apply, monitor, and visualize the patch statuses of all EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the EC2 fleet to automatically install the security OS patch every week on the provided maintenance window.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Systems Manager and Amazon ES to manage, record, and deploy the security patches for the OS for the entire fleet of EC2 instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use AWS Systems Manager and AWS Config to manage, record, and deploy the security patches for the OS for the entire fleet of EC2 instances.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Patch Manager</strong> automates the process of patching managed instances with security-related updates. For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), Amazon Linux, and Amazon Linux 2. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_maintenance_window.png\"></p><p>Since you are also required to record all of the changes to patch and association compliance statuses, you can use <strong>AWS Config</strong> to meet this requirement. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations.</p><p>Therefore, the correct answer is: <strong>Use AWS Systems Manager and AWS Config to manage, record, and deploy the security patches for the OS for the entire fleet of EC2 instances.</strong></p><p>The option that says: <strong>Configure the EC2 fleet to automatically install the security OS patch every week on the provided maintenance window</strong> is incorrect because the EC2 Spot Fleet does not have a built-in function to automatically install the security OS patch every week on the provided maintenance window.</p><p>The option that says: <strong>Set up Amazon QuickSight and Kibana to apply, monitor, and visualize the patch statuses of all EC2 instances</strong> is incorrect. Amazon QuickSight and Kibana are primarily used for data visualization and not for patch management. You can use Amazon Elasticsearch (ES) with Kibana but this service is not suitable for this scenario.</p><p>The option that says: <strong>Use AWS Systems Manager and Amazon ES to manage, record, and deploy the security patches for the OS for the entire fleet of EC2 instances</strong> is incorrect. Amazon Elasticsearch Service (Amazon ES) is just an AWS-managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Elasticsearch is a popular open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. This service is not helpful in this scenario since the task is to manage the security patches of your EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-patch.html</a></p><p><a href=\"https://aws.amazon.com/config/\">https://aws.amazon.com/config/</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A photo-sharing website uses a CloudFront distribution with a default name (dtut0r1al5doj0.cloudfront.net) to distribute its static contents. It uses an ELB in front of an Auto Scaling group of Spot EC2 instances deployed across two Availability Zones. The website has a poor search ranking in Google as it doesn't use a secure HTTPS/SSL on its site.</p><p>Which of the following are valid options in order to require HTTPS for communication between the viewers and CloudFront? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use a self-signed certificate in the ELB.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure CloudFront to use its default SSL/TLS certificate by changing the <code>Viewer Protocol Policy</code> setting for one or more cache behaviors to require HTTPS communication.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a self-signed SSL/TLS certificate in the ELB which is stored in a private S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set the <code>Viewer Protocol Policy&lt;</code> to use <code>Redirect HTTP to HTTPS</code> or <code>HTTPS Only</code>.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the ELB to use its default SSL/TLS certificate.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>If you're using the domain name that <strong>CloudFront</strong> assigned to your distribution, such as dtut0ria1sd0jo.cloudfront.net, you can change the Viewer Protocol Policy setting for one or more cache behaviors to require HTTPS communication by setting it as either <code>Redirect HTTP to HTTPS</code> or <code>HTTPS Only</code>. In that configuration, CloudFront provides its default SSL/TLS certificate.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\"></p><p>If your origin is an Elastic Load Balancing load balancer, you can use a certificate provided by AWS Certificate Manager (ACM). You can also use a certificate that is signed by a trusted third-party certificate authority and imported into ACM. Note that you can't use a self-signed certificate for HTTPS communication between CloudFront and your origin.</p><p>Therefore, the following options are correct:</p><p><strong>- Set the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> to use </strong><code><strong>Redirect HTTP to HTTPS</strong></code><strong> or </strong><code><strong>HTTPS Only</strong></code><strong>.</strong></p><p><strong>- Configure CloudFront to use its default SSL/TLS certificate by changing the </strong><code><strong>Viewer Protocol Policy</strong></code><strong> setting for one or more cache behaviors to require HTTPS communication.</strong></p><p>The option that says: <strong>Use a self-assigned SSL/TLS certificate in the ELB which is stored in a private S3 bucket</strong> is incorrect because you don't need to add an SSL certificate if you only require HTTPS for communication between the viewers and CloudFront. You should only do this if you require HTTPS between your origin and CloudFront. In addition, you can't use a self-signed certificate in this scenario even though it is stored in a private S3 bucket. You need to use either a certificate from ACM or a third-party certificate.</p><p>The option that says: <strong>Use a self-signed certificate in the ELB</strong> is incorrect. As explained in the previous paragraph, adding an SSL certificate in the ELB is not required. Additionally, using a self-signed certificate for public websites is not recommended.</p><p>The option that says: <strong>Configure the ELB to use its default SSL/TLS certificate </strong>is incorrect. There is no default SSL certificate in ELB, unlike what we have in CloudFront (*.cloudfront.net). As previously explained, adding an SSL certificate in the ELB is not required.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html#using-https-cloudfront-to-origin-certificate</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A leading mobile game company has an application running on Elastic Beanstalk that continuously collects player-game interactions and player's behavior then feeds the data into an Amazon Kinesis stream. A second Elastic Beanstalk app generates key performance indicators (KPIs) into a DynamoDB table and powers the game leaderboard. After a few weeks, there has been a technical problem in the Kinesis data stream which resulted in data loss for your application. </p><p>Which of the following is the most efficient and most scalable option to prevent any data loss for this application?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use the second AWS Elastic Beanstalk app to store a backup of Kinesis data onto an EBS volume, and then create snapshots from your EBS volumes."
			},
			{
				"correct": false,
				"answer": "Use Data Pipeline to replicate your DynamoDB tables into another region."
			},
			{
				"correct": false,
				"answer": "Launch a second Amazon Kinesis stream in another Availability Zone then use Data Pipeline to replicate data across Kinesis streams."
			},
			{
				"correct": true,
				"answer": "Launch a third Elastic Beanstalk app that uses the Amazon Kinesis S3 connector or Amazon Kinesis Data Firehose to archive the data from Kinesis into an S3 bucket."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Kinesis Data Firehose</strong> is the easiest way to reliably load streaming data into data stores and analytics tools. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk, enabling near real-time analytics with existing business intelligence tools and dashboards you’re already using today. It is a fully managed service that automatically scales to match the throughput of your data and requires no ongoing administration. It can also batch, compress, transform, and encrypt the data before loading it, minimizing the amount of storage used at the destination and increasing security.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_firehose_overview.png\"></p><p>In this scenario, you have to use either an Amazon Kinesis S3 connector or an Amazon Kinesis Data Firehose that archives the data from Kinesis into an S3 bucket.</p><p>Therefore, the correct answer is: <strong>Launch a third Elastic Beanstalk app that uses the Amazon Kinesis S3 connector or Amazon Kinesis Data Firehose to archive the data from Kinesis into an S3 bucket.</strong></p><p>The option that says: <strong>Use Data Pipeline to replicate your DynamoDB tables into another region</strong> is incorrect because the Data Pipeline service does not support data streams unlike Kinesis, and replicating your DynamoDB tables into another region does not solve the root cause of the data loss.</p><p>The option that says: <strong>Launch a second Amazon Kinesis stream in another Availability Zone then using Data Pipeline to replicate data across Kinesis streams</strong> is incorrect because replicating the data across Kinesis streams will only provide redundancy but will not solve the root cause of the data loss issue.</p><p>The option that says: <strong>Use the second AWS Elastic Beanstalk app to store a backup of Kinesis data onto an EBS volume, and then creating snapshots from your EBS volumes</strong> is incorrect because storing the data on EBS volumes is not a scalable solution.</p><p><br></p><p><strong>Reference:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><a href=\"https://aws.amazon.com/blogs/big-data/persist-streaming-data-to-amazon-s3-using-amazon-kinesis-firehose-and-aws-lambda/\">https://aws.amazon.com/blogs/big-data/persist-streaming-data-to-amazon-s3-using-amazon-kinesis-firehose-and-aws-lambda/</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
	},
	{
		"question": "<p>A company has a critical application running on an Auto Scaling group of Amazon EC2 instances. The application CI/CD pipelines are created on AWS CodePipeline and all of the relevant AWS resources are defined in AWS CloudFormation templates. During deployments, the Auto Scaling group spawns new instances and the user data script downloads the new artifact from a central Amazon S3 bucket. With several code updates during the development cycle, a recent update on the CloudFormation templates has caused a major application downtime.</p><p>Which of the following solutions should the Solutions Architect implement to reduce the chances of downtime during deployments?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Update the CloudFormation templates to include cfn helper scripts. This will detect and report conditions during deployments to ensure that only healthy deployments are continued. Create test plans for the quality assurance team to ensure that changes are tested on a non-production environment before applying to production.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Check the CloudFormation templates for errors with the help of plugins on the integrated development environment (IDE). Ensure that the templates are valid using AWS CLI. Include cfn helper scripts on the deployment code to detect and report for errors. Deploy on a non-production environment and perform manual testing before applying changes to production.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a blue/green deployment pattern on AWS CodeDeploy using CloudFormation to update the user data deployment scripts. Manually login to the instances and perform tests to verify that the deployment is successful and the application is running as expected.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Add an AWS CodeBuild stage on the deployment pipeline to automatically test on a non-production environment. Leverage change sets on AWS CloudFormation to preview changes before applying to production. Set up a blue/green deployment pattern on AWS CodeDeploy to deploy changes on a separate environment and to quickly rollback if needed.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CodeBuild</strong> is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools.</p><p>You can automate your release process by using <strong>AWS CodePipeline</strong> to test your code and run your builds with AWS CodeBuild. This involves two main steps:</p><p>- Create a continuous delivery (CD) pipeline with CodePipeline that automates builds with CodeBuild.</p><p>- Add test and build automation with CodeBuild to an existing pipeline in CodePipeline.</p><p>When you need to update a <strong>CloudFormation</strong> stack, understanding how your changes will affect running resources before you implement them can help you update stacks with confidence. <strong>Change sets</strong> allow you to preview how proposed changes to a stack might impact your running resources, for example, whether your changes will delete or replace any critical resources, AWS CloudFormation makes the changes to your stack only when you decide to execute the change set, allowing you to decide whether to proceed with your proposed changes or explore other changes by creating another change set.</p><p><strong>AWS CodeDeploy</strong> is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it possible to automate the deployment of code to either Amazon EC2 or on-premises instances. AWS CodeDeploy supports blue/green deployments. AWS CodeDeploy offers two ways to perform blue/green deployments:</p><p>- In the first approach, AWS CodeDeploy makes a copy of an Auto Scaling group. It, in turn, provisions new Amazon EC2 instances, deploys the application to these new instances, and then redirects traffic to the newly deployed code.</p><p>- In the second approach, you use instance tags or an Auto Scaling group to select the instances that will be used for the green environment. AWS CodeDeploy then deploys the code to the tagged instances.</p><p>In the following figure, the release manager uses the workstation instance to push a new version of the application to AWS CodeDeploy and starts a blue-green deployment. AWS CodeDeploy creates a copy of the Auto Scaling group. It launches two new web server instances just like the original two. AWS CodeDeploy installs the new version of the application and then redirects the load balancer to the new instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codebuild_blue_gree_deploy.png\"></p><p>Therefore, the correct answer is: <strong>Add an AWS CodeBuild stage on the deployment pipeline to automatically test on a non-production environment. Leverage change sets on AWS CloudFormation to preview changes before applying to production. Set up a blue/green deployment pattern on AWS CodeDeploy to deploy changes on a separate environment and to quickly rollback if needed.</strong> With AWS CodeBuild on your pipeline, you can add automated tests to verify that the artifact is working as expected. CloudFormation change sets allow you to preview proposed changes on templates before you apply them. AWS CodeDeploy can use a blue/green deployment strategy to have a separate deployment environment and easy rollback procedures.</p><p>The option that says: <strong>Update the CloudFormation templates to include cfn helper scripts. This will detect and report conditions during deployments to ensure that only healthy deployments are continued. Create test plans for the quality assurance team to ensure that changes are tested on a non-production environment before applying to production</strong> is incorrect. This may be possible, however, it will rely on another team to do the testing. It is better to set up AWS CodeBuild to automate this verification which also reduces any human intervention or errors.</p><p>The option that says: <strong>Check the CloudFormation templates for errors with the help of plugins on the integrated development environment (IDE). Ensure that the templates are valid using AWS CLI. Include cfn helper scripts on the deployment code to detect and report for errors. Deploy on a non-production environment and perform manual testing before applying changes to production</strong> is incorrect. Using plugins on IDEs only detects syntax errors on your CloudFormation codes. It won't prevent the user from creating logical errors on the template that may have drastic effects on the current environment.</p><p>The option that says: <strong>Set up a blue/green deployment pattern on AWS CodeDeploy using CloudFormation to update the user data deployment scripts. Manually login to the instances and perform tests to verify that the deployment is successful and the application is running as expected</strong> is incorrect. This is possible but not recommended since manual intervention is prone to human error. You should create automated testing instead to verify the application for every deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/\">https://aws.amazon.com/blogs/devops/performing-bluegreen-deployments-with-aws-codedeploy-and-auto-scaling-groups/</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/how-to-create-pipeline.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html</a></p><p><a href=\"https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/\">https://aws.amazon.com/blogs/devops/automating-your-api-testing-with-aws-codebuild-aws-codepipeline-and-postman/</a></p><p><br></p><p><strong>Check out these AWS CodeBuild, AWS CodeDeploy, and AWS CloudFormation Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codebuild/?src=udemy\">https://tutorialsdojo.com/aws-codebuild/</a></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>AWS CloudFormation - Templates, Stacks, Change Sets:</strong></p><p><a href=\"https://youtu.be/9Xpuprxg7aY\">https://youtu.be/9Xpuprxg7aY</a></p></div>"
	},
	{
		"question": "<p>An online learning portal that provides educational video courses is deployed in AWS and is using CloudFront to distribute its images, videos, files, and other static contents to its users. Recently, they introduced a new, member-only access to some of its top-rated courses. They want to provide access to multiple private files of their online courses only to their paying subscribers without having to change their current URLs.</p><p>What should you do to satisfy the given requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Enable field-level encryption when serving your content.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure your web distribution to serve the private content using Signed URLs.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure your web distribution to serve the private content using Signed Cookies.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set the Origin Protocol Policy of your CloudFront web distribution to <code>Match Viewer</code>.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudFront</strong> signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content. If you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following.</p><p>Use <strong>signed URLs</strong> for the following cases:</p><p>- You want to use an RTMP distribution. Signed cookies aren't supported for RTMP distributions.</p><p>- You want to restrict access to individual files, for example, an installation download for your application.</p><p>- Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.</p><p>Use <strong>signed cookies</strong> for the following cases:</p><p>- You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of a website.</p><p>- You don't want to change your current URLs.</p><p>Therefore, the correct answer is: <strong>Configure your web distribution to serve the private content using Signed Cookies.</strong></p><p>The option that says: <strong>Set the Origin Protocol Policy of your CloudFront web distribution to </strong><code><strong>Match Viewer</strong></code><strong> </strong>is incorrect. The <code>Match Viewer</code> is an Origin Protocol Policy which configures CloudFront to communicate with your origin using HTTP or HTTPS, depending on the protocol of the viewer request. CloudFront caches the object only once even if viewers make requests using both HTTP and HTTPS protocols.</p><p>The option that says: <strong>Configure your web distribution to serve the private content using Signed URLs</strong> is incorrect. Signed URLs, as shown on the above explanation, is mainly used for providing access to individual files.</p><p>The option that says: <strong>Enable field-level encryption when serving your content</strong> is incorrect. Field-Level Encryption only allows you to securely upload user-submitted sensitive information to your web servers. It does not provide access to download multiple private files.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>S3 Pre-signed URLs vs CloudFront Signed URLs vs Origin Access Identity (OAI)</strong></p><p><a href=\"https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/?src=udemy\">https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai/</a></p></div>"
	},
	{
		"question": "<p>A visual effects studio has over 40-TB worth of video files stored in the company's on-premises tape library. The tape drives are managed by a Media Asset Management (MAM) solution. The video files contain a variety of footage which includes faces, objects, sceneries, cars, and many others. The company wants to automatically build a metadata library for the video files based on these objects. This will then be used as a catalog for the search feature of the MAM solution. The company already has a catalog of people’s photos and names that appeared on the video footage. The company wants to migrate all the video files of the MAM solution to AWS so a Direct Connect connection was provisioned from the on-premises data center to AWS to facilitate this.</p><p>Which of the following is the MOST suitable implementation that will meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Copy the video files directly to an Amazon S3 bucket. Create an Amazon EC2 instance that will run GluonCV libraries to generate metadata information from the video files in the S3 bucket. Store the catalog of people’s faces and names in the Amazon EBS volume to be used by GluonCV. After processing the videos, push the generated metadata to the MAM solution search catalog.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the MAM solution to extract the video files from the current tape archives and move them to an Amazon S3 bucket using AWS DataSync. Use an Amazon SageMaker Jupyter notebook instance to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Amazon SageMaker to pull the video files from the S3 bucket, retrieve the generated metadata, and then push it to the MAM solution search catalog.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Provision an AWS Storage Gateway – file gateway appliance on the on-premises data center. Configure the MAM solution to extract the video files from the current tape archives and move them to the file gateway share which is then synced to Amazon S3. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Rekognition to pull the video files from the S3 bucket, retrieve the generated metadata and then push it to the MAM solution search catalog.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a stream in Amazon Kinesis Video Streams that will ingest the videos from the MAM system and store the videos to an Amazon S3 bucket. Configure the MAM solution to stream the videos into Kinesis Video Streams. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Set up a stream consumer that will retrieve the generated metadata and then push it to the MAM solution search catalog.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Storage Gateway</strong> connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure. You can use the service to store data in the AWS Cloud for scalable and cost-effective storage that helps maintain data security.</p><p>AWS Storage Gateway offers file-based, volume-based, and tape-based storage solutions. A file gateway supports a file interface into Amazon Simple Storage Service (Amazon S3) and combines a service and a virtual software appliance. By using this combination, you can store and retrieve objects in Amazon S3 using industry-standard file protocols such as Network File System (NFS) and Server Message Block (SMB). A file gateway simplifies file storage in Amazon S3, integrates to existing applications through industry-standard file system protocols, and provides a cost-effective alternative to on-premises storage. It also provides low-latency access to data through transparent local caching.</p><p><img src=\"https://media.tutorialsdojo.com/sap_file_gateway_how_it_works.png\"></p><p><strong>Amazon Rekognition</strong> makes it easy to add image and video analysis to your applications using proven, highly scalable, deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos, as well as detect any inappropriate content.</p><p>Amazon Rekognition also provides highly accurate facial analysis and facial search capabilities that you can use to detect, analyze, and compare faces for a wide variety of user verification, people counting, and public safety use cases.</p><p>Therefore, the correct answer is: <strong>Provision an AWS Storage Gateway – file gateway appliance on the on-premises data center. Configure the MAM solution to extract the video files from the current tape archives and move them to the file gateway share which is then synced to Amazon S3. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Rekognition to pull the video files from the S3 bucket, retrieve the generated metadata and then push it to the MAM solution search catalog.</strong></p><p>The option that says:<strong> Configure the MAM solution to extract the video files from the current tape archives and move them to an Amazon S3 bucket using AWS DataSync. Use an Amazon SageMaker Jupyter notebook instance to build a collection based on the videos by using the catalog of people’s faces and names. Create an AWS Lambda function that will invoke Amazon SageMaker to pull the video files from the S3 bucket, retrieve the generated metadata and then push it to the MAM solution search catalog</strong> is incorrect. An Amazon SageMaker notebook instance is simply a machine learning (ML) compute instance running the Jupyter Notebook App. It is not capable of doing image detection or recognition. Jupyter notebooks are primarily used to prepare your data and write code to train models. A better solution for this is to use Amazon Rekognition.</p><p>The option that says: <strong>Create a stream in Amazon Kinesis Video Streams that will ingest the videos from the MAM system and store the videos to an Amazon S3 bucket. Configure the MAM solution to stream the videos into Kinesis Video Streams. Use Amazon Rekognition to build a collection based on the videos by using the catalog of people’s faces and names. Set up a stream consumer that will retrieve the generated metadata and then push it to the MAM solution search catalog</strong> is incorrect. This is not cost-effective and will require more changes to the existing MAM solution. In addition, it is not stated in the scenario that the MAM solution supports video streaming directly to Kinesis Video Stream. Most of the time, you need to set up a custom Kinesis Video Streams Producer client in order to send data to a Kinesis Video Stream.</p><p>The option that says: <strong>Copy the video files directly to an Amazon S3 bucket. Create an Amazon EC2 instance that will run GluonCV libraries to generate metadata information from the video files in the S3 bucket. Store the catalog of people’s faces and names in the Amazon EBS volume to be used by GluonCV. After processing the videos, push the generated metadata to the MAM solution search catalog</strong> is incorrect. This is not cost-effective and will require more management overhead in maintaining and configuring the GluonCV libraries on the EC2 instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\">https://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html</a></p><p><a href=\"https://aws.amazon.com/rekognition/\">https://aws.amazon.com/rekognition/</a></p><p><a href=\"https://aws.amazon.com/rekognition/media-analysis/\">https://aws.amazon.com/rekognition/media-analysis/</a></p><p><br></p><p><strong>Check out these Amazon Rekognition and Storage Gateway Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p></div>"
	},
	{
		"question": "<p>A retail company has several subsidiaries with offices located in different countries in Southeast Asia. Each subsidiary has an AWS account that is used for hosting the company retail website, which is customized per country. The parent company wants to have better control on all the AWS accounts as well as visibility on the costs incurred for each account. The Solutions Architect has been tasked to implement a solution that will satisfy the following requirements:</p><p> - Provide a cost breakdown report for each subsidiary AWS account.</p><p> - Have a single AWS invoice for all the subsidiary AWS accounts.</p><p> - Provide full administration privileges on each subsidiary AWS account, regardless of the parent company’s policy.</p><p> - Have the ability to restrict the services and features that can be used on each subsidiary AWS account, as defined by the parent company’s policy.</p><p>Which of the following actions should the Solutions Architect take in order to fulfill the requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create an AWS Organization on the parent company's AWS account and invite all the subsidiary AWS accounts. On the AWS Billing and Cost Management console of the parent account, ensure that consolidated billing is enabled.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an AWS account for the parent company and create an AWS organization for each of the subsidiaries. Invite each of the subsidiary AWS Accounts to join their respective AWS organization on the parent company.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Define service quotas that will restrict services and features depending on the permissions set by the parent company policy. Apply this service quota to each subsidiary AWS account.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Define Service Control Policy (SCP) documents to only allow services and features defined by the parent company policy. Apply the necessary SCP for each subsidiary AWS account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an AWS account for the parent company and create a single AWS Organization with the Consolidated Billing features set. Invite each of the subsidiary AWS accounts to join the AWS Organization of the parent company.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Organizations</strong> helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts.</p><p>AWS Organizations is integrated with other AWS services so you can define central configurations, security mechanisms, audit requirements, and resource sharing across accounts in your organization.</p><p>Once you’ve created the organization and verified your email, you can create or invite other accounts into your organization, categorize the accounts into Organizational Units (OUs), create service control policies (SCPs), and take advantage of the Organizations' features from supported AWS services. <strong>Service control policies (SCPs)</strong> are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for all accounts in your organization. SCPs help you ensure that your accounts stay within your organization’s access control guidelines.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_overview.png\"></p><p>You can use the <strong>consolidated billing</strong> feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts or multiple Amazon Internet Services Pvt. Ltd (AISPL) accounts. Every organization in AWS Organizations has a management account that pays the charges of all the member accounts.</p><p>Consolidated billing has the following benefits:</p><p><strong>One bill</strong> – You get one bill for multiple accounts.</p><p><strong>Easy tracking</strong> – You can track the charges across multiple accounts and download the combined cost and usage data.</p><p><strong>Combined usage</strong> – You can combine the usage across all accounts in the organization to share the volume pricing discounts, Reserved Instance discounts, and Savings Plans. This can result in a lower charge for your project, department, or company than with individual standalone accounts.</p><p><strong>No extra fee</strong> – Consolidated billing is offered at no additional cost.</p><p><img src=\"https://tutorialsdojo-media.s3.us-east-2.amazonaws.com/aws_organizations_scp.png\"></p><p>With consolidated billing, the management account is billed for all charges of the member accounts. However, unless the organization is changed to support all features in the organization (not consolidated billing features only) and member accounts are explicitly restricted by policies, each member account is otherwise independent of the other member accounts. For example, the owner of a member account can sign up for AWS services, access resources, and use AWS Premium Support unless the management account restricts those actions. Each account owner continues to use their own IAM user name and password, with account permissions assigned independently of other accounts in the organization.</p><p><img src=\"https://media.tutorialsdojo.com/AWS%20Organizations_Consolidated%20Billing%20Features%20only.png\"></p><p>The option that says: <strong>Create an AWS Organization on the parent company's AWS account and invite all the subsidiary AWS accounts. On the AWS Billing and Cost Management console of the parent account, ensure that consolidated billing is enabled</strong> is correct. Consolidated billing allows the management account owner to have only one invoice for all accounts in the organization. And by default, each member account is independent of the other member accounts, so each subsidiary has full administration privileges unless controlled by the parent account.</p><p>The option that says: <strong>Define Service Control Policy (SCP) documents to only allow services and features defined by the parent company policy. Apply the necessary SCP for each subsidiary AWS account</strong> is correct. This satisfies the requirement for restricting access to the subsidiary AWS accounts as defined by the parent AWS account.</p><p>The option that says: <strong>Create an AWS account for the parent company and create an AWS Organization for each of the subsidiaries. Invite each of the subsidiary AWS accounts to join their respective AWS organization on the parent company</strong> is incorrect. You only have to create a single organization and link the member accounts.</p><p>The option that says: <strong>Define service quotas that will restrict services and features depending on the permissions set by the parent company policy. Apply this service quota to each subsidiary AWS account</strong> is incorrect. Applying service quota will not restrict the member accounts from using AWS services or features that are not permitted by the parent account. Service quota only restricts how much you can use for a particular service.</p><p>The option that says: <strong>Create an AWS account for the parent company and create a single AWS Organization with the Consolidated Billing features set. Invite each of the subsidiary AWS accounts to join the AWS Organization of the parent company </strong>is incorrect. Although creating an AWS organization is necessary, using only the Consolidated Billing features set is not enough to satisfy the requirements. Even though \"All Features\" is enabled by default, this will be overridden if you enable only the \"Consolidated Billing\" feature. This means that you cannot use the SCP to your member AWS accounts anymore. You need to enable \"All features\" on the AWS Organization to be able to create and apply SCP for each subsidiary.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a></p><p><a href=\"https://aws.amazon.com/organizations/getting-started/\">https://aws.amazon.com/organizations/getting-started/</a></p><p><a href=\"https://aws.amazon.com/organizations/getting-started/best-practices/\">https://aws.amazon.com/organizations/getting-started/best-practices/</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-all-features.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p></div>"
	},
	{
		"question": "<p>A cryptocurrency trading platform uses a Lambda function which has recently been integrated with DynamoDB Streams as its event source. Whenever there is a new deployment, the incoming traffic to the function must be shifted in two increments using CodeDeploy. Ten percent of the incoming traffic should be shifted to the new version and then the remaining 90 percent should be deployed five minutes later. It is also required to trace the event source that invoked the Lambda function including the downstream calls that the function made.</p><p>Which of the following options should the solutions architect implement to satisfy this requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Configure a <code>Rolling with additional batch</code> deployment configuration for your Lambda function and use X-Ray to trace the event source and downstream calls.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure an <code>All-at-once</code> deployment configuration for your Lambda function and use AWS Config to trace the event source and downstream calls.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure a <code>Canary</code> deployment configuration for your Lambda function. Enable active tracing to integrate AWS X-Ray to your AWS Lambda function.&nbsp; </p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure a <code>Linear</code> deployment configuration for your Lambda function and use AWS Config to trace the event source and downstream calls.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy.</p><p>When you deploy to an AWS Lambda compute platform, the deployment configuration specifies the way traffic is shifted to the new Lambda function versions in your application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codedeploy_lambda.png\"></p><p>In a Canary deployment configuration, the traffic is shifted in <strong>two</strong> increments. You can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.</p><p>Therefore, the correct answer is: <strong>Configure a </strong><code><strong>Canary</strong></code><strong> deployment configuration for your Lambda function. Enable active tracing to integrate AWS X-Ray to your AWS Lambda function.</strong></p><p>The option that says: <strong>Configure an </strong><code><strong>All-at-once</strong></code><strong> deployment configuration for your Lambda function and using AWS Config to trace the event source and downstream calls</strong> is incorrect. If you use this deployment configuration, the traffic is shifted from the original Lambda function to the updated Lambda function version all at once. In addition, you can't use AWS Config to trace the event source and downstream calls. You have to use X-Ray instead and this can be done by simply enabling active tracing.</p><p>The option that says: <strong>Configure a </strong><code><strong>Linear</strong></code><strong> deployment configuration for your Lambda function and using AWS Config to trace the event source and downstream calls</strong> is incorrect. A deployment configuration will cause the traffic to be shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.</p><p>The option that says: <strong>Configure a </strong><code><strong>Rolling with additional batch</strong></code><strong> deployment configuration for your Lambda function and using X-Ray to trace the event source and downstream calls</strong> is incorrect. Although X-Ray can be used to trace the event source and downstream calls,Rolling with additional batch is only applicable in Elastic Beanstalk and not for Lambda.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html</a></p><p><a href=\"https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html</a></p><p><br></p><p><strong>Check out this AWS CodeDeploy Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codedeploy/?src=udemy\">https://tutorialsdojo.com/aws-codedeploy/</a></p></div>"
	},
	{
		"question": "<p>A financial services company uses hardware security modules (HSMs) to generate encryption master keys. Since the company application logs include personally identifiable information, encryption is required as part of regulatory compliance. The application logs are going to be stored on a central Amazon S3 bucket and should be encrypted at rest. The security team wants to use the company HSMs to generate the CMK material for encryption on the S3 bucket.</p><p>Which of the following options should the solutions architect implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Using AWS CLI, create a new CMK with AWS-provided key material and use AWS_KMS as the origin of the key. Overwrite this CMK with a generated key from the on-premises HSMs by using the public key and import token provided by AWS. Set a 1-year duration for the CMK automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Request to provision an AWS Direct Connect connection from the on-premises data center to AWS VPC. Ensure that the network addresses do not overlap. Apply an Amazon S3 bucket policy on the central logging bucket to allow only encrypted object uploads. Configure the application to generate a unique CMK for each logging event by querying the on-premises HSMs through the Direct Connection network.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Using AWS CLI, create a new CMK with no key material and use EXTERNAL as the origin of the key. Generate a key from the on-premises HSMs and import it as CMK using the public key and import token from AWS. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new AWS CloudHSM cluster and set it as the key material source in AWS Key Management Service (KMS) when you generate a new CMK. Set a 1-year duration for the CMK automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can protect data at rest in Amazon S3 by using three different modes of server-side encryption: <strong>SSE-S3</strong>, <strong>SSE-C</strong>, or <strong>SSE-KMS</strong>.</p><p><strong>- SSE-S3</strong> requires that Amazon S3 manage the data and the encryption keys.</p><p><strong>- SSE-C</strong> requires that you manage the encryption key.</p><p><strong>- SSE-KMS</strong> requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS.</p><p><strong>Server-side encryption</strong> is the encryption of data at its destination by the application or service that receives it. <strong>AWS Key Management Service (AWS KMS)</strong> is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.</p><p>If you use CMKs, you use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create CMKs, define the policies that control how CMKs can be used, and audit their usage to prove that they are being used correctly. You can use these CMKs to protect your data in Amazon S3 buckets. When you use SSE-KMS encryption with an S3 bucket, the AWS KMS CMK must be in the same Region as the bucket. If you want to use a customer managed CMK for SSE-KMS, create the CMK before you configure SSE-KMS. Then, when you configure SSE-KMS for your bucket, specify the existing customer managed CMK.</p><p>Creating your own customer managed CMK gives you more flexibility and control. For example, you can create, rotate, and disable customer managed CMKs. You can also define access controls and audit the customer managed CMKs that you use to protect your data.</p><p>Below is the process of creating/importing a CMK in AWS KMS.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kms_cloud_hsm_steps.png\"></p><p>Create a customer master key (CMK) in AWS KMS that has no key material associated.</p><p>Download the import wrapping key and import token from KMS.</p><p>Import the wrapping key provided by KMS into the HSM.</p><p>Create a 256 bit symmetric key on AWS CloudHSM.</p><p>Use the imported wrapping key to wrap the symmetric key.</p><p>Import the symmetric key into AWS KMS using the import token from step 2.</p><p>Terminate your HSM, which triggers a backup. Delete or leave your cluster, depending on your needs.</p><p>Therefore, the correct answer is: <strong>Using AWS CLI, create a new CMK with no key material and use EXTERNAL as the origin of the key. Generate a key from the on-premises HSMs and import it as CMK using the public key and import token from AWS. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads.</strong> You can use the company HSMs as an independent CMK source and import them to AWS KMS by creating a CMK with no material and using EXTERNAL as the origin. You can then apply an S3 bucket policy to further restrict unencrypted object uploads.</p><p>The option that says: <strong>Create a new AWS CloudHSM cluster and set it as the key material source in AWS Key Management Service (KMS) when you generate a new CMK. Set a 1-year duration for the CMK automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads</strong> is incorrect. You must use the on-premises HSMs as the source for the CMKs so you should not create your own AWS CloudHSM cluster and generate a CMK.</p><p>The option that says: <strong>Request to provision an AWS Direct Connect connection from the on-premises data center to AWS VPC. Ensure that the network addresses do not overlap. Apply an Amazon S3 bucket policy on the central logging bucket to allow only encrypted object uploads. Configure the application to generate a unique CMK for each logging event by querying the on-premises HSMs through the Direct Connection network</strong> is incorrect. This is not a practical solution as using a Direct Connect connection for minimal traffic is not recommended. Additionally, this requires you to reconfigure the logging application.</p><p>The option that says: <strong>Using AWS CLI, create a new CMK with AWS-provided key material and use AWS_KMS as the origin of the key. Overwrite this CMK with a generated key from the on-premises HSMs by using the public key and import token provided by AWS. Set a 1-year duration for the CMK automatic key rotation. Apply an Amazon S3 bucket policy on the central logging bucket to require AWS KMS as the encryption source and deny unencrypted object uploads</strong> is incorrect. You should create a CMK with no key materials with EXTERNAL origin to be able to import your keys from the on-premises HSMs. If you use AWS-provided key material and AWS_KMS as origin, you won't be able to import and overwrite the CMK.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/security/how-to-byok-bring-your-own-key-to-aws-kms-for-less-than-15-00-a-year-using-aws-cloudhsm/\">https://aws.amazon.com/blogs/security/how-to-byok-bring-your-own-key-to-aws-kms-for-less-than-15-00-a-year-using-aws-cloudhsm/</a></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html</a></p><p><br></p><p><strong>Check out these AWS KMS and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-key-management-service-aws-kms/?src=udemy\">https://tutorialsdojo.com/aws-key-management-service-aws-kms/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A company runs its legacy web application in its on-premises data center. The solutions architect has been tasked to move the legacy web application in a virtual machine running inside the data center to the Amazon VPC. However, this application requires a private and dedicated connection to a number of servers hosted on the on-premises network in order for it to work.</p><p>Which combination of options provides the most suitable way to configure the web application running inside the VPC to reach back and access its internal dependencies on the company’s on-premises network? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "An AWS Direct Connect link between the VPC and the network housing the internal services. "
			},
			{
				"correct": false,
				"answer": "An Internet Gateway to allow a VPN connection. "
			},
			{
				"correct": false,
				"answer": "<p>Set up a Transit VPC between your on-premises data center and your VPC.</p>"
			},
			{
				"correct": true,
				"answer": "A network device in your data center that supports Border Gateway Protocol (BGP) and BGP MD5 authentication."
			},
			{
				"correct": false,
				"answer": "An Elastic IP address on the VPC instance. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Direct Connect</strong> links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. One end of the cable is connected to your router, the other to an AWS Direct Connect router.</p><p><img src=\"https://media.tutorialsdojo.com/sap_awsDirectConnect.jpg\"></p><p>With this connection, you can create <em>virtual interfaces</em> directly to public AWS services (for example, to Amazon S3) or to Amazon VPC, bypassing Internet service providers in your network path. An AWS Direct Connect location provides access to AWS in the Region with which it is associated. You can use a single connection in a public Region or AWS GovCloud (US) to access public AWS services in all other public Regions.</p><p>The option that says: <strong>A network device in your data center that supports Border Gateway Protocol (BGP) and BGP MD5 authentication</strong> is correct as a network device that supports Border Gateway Protocol (BGP) and BGP MD5 authentication is needed to establish a Direct Connect link from your data center to your VPC.</p><p>The option that says: <strong>An AWS Direct Connect link between the VPC and the network housing the internal services</strong> is correct because Direct Connect sets up a dedicated connection between on-premises data center and Amazon VPC, and provides you with the ability to connect your on-premises servers with the instances in your VPC.</p><p>The option that says: <strong>An Internet Gateway to allow a VPN connection</strong> is incorrect as you normally create a VPN connection based on a customer gateway and a virtual private gateway (VPG) in AWS.</p><p>The option that says: <strong>An Elastic IP address on the VPC instance</strong> is incorrect because EIPs are not needed as the instances in the VPC can communicate with on-premises servers via their private IP address.</p><p>The option that says: <strong>Setting up a Transit VPC between your on-premises data center and your VPC</strong> is incorrect. Although a Transit VPC can establish a connection to your VPC and on-premises data center, this option is not suitable for this scenario. Remember that a Transit VPC just simplifies network management and minimizes the number of connections required to connect multiple VPCs and remote networks.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html#ConnectionRequest\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/getting_started.html#ConnectionRequest</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p></div>"
	},
	{
		"question": "<p>A company has a multi-tier web application hosted in AWS. It leverages Amazon CloudFront to reliably scale and quickly serve requests from users around the world. After several months in operation, the company received user complaints of slow response time from the web application. The monitoring team reported that the CloudFront cache hit ratio metric is steadily dropping for the past months. This metric indicates that there are inconsistent query strings on user requests and queries that contain upper-case or mixed-case letters. These requests cause CloudFront to send unnecessary origin queries.</p><p>Which of the following actions will increase the cache hit ratio of the CloudFront distribution?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Launch a reverse proxy inside the application VPC to intercept the requests going to the origin instances. Process the query parameters to sort them by name and convert them to lowercase letters before forwarding them to the instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reconfigure the CloudFront distribution to remove the caching behavior based on query string parameters. This will cache the requests regardless of the order or case of the query parameters.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reconfigure the CloudFront distribution to ensure that the “case insensitive” option is enabled for processing query string parameters.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Write a Lamda@Edge function that will normalize the query parameters by sorting them in alphabetical order and converting them into lower case. Deploy this function with the CloudFront distribution and set “viewer request” as the trigger to invoke the function.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For each query string parameter that your web application forwards to <strong>CloudFront</strong>, CloudFront forwards requests to your origin for every parameter value and caches a separate version of the object for every parameter value. This is true even if your origin always returns the same object regardless of the parameter value. For multiple parameters, the number of requests and the number of objects multiply. For example, if requests for an object include two parameters that each have three different values, CloudFront caches six versions of that object. AWS recommends that you configure CloudFront to cache based only on the query string parameters for which your origin returns different versions, and that you carefully consider the merits of caching based on each parameter.</p><p>If you configure CloudFront to cache based on query string parameters, you can improve caching if you do the following:</p><p>- Configure CloudFront to forward only the query string parameters for which your origin will return unique objects.</p><p>- Use the same case (uppercase or lowercase) for all instances of the same parameter. For example, if one request contains <code><strong>parameter1=A</strong></code> and another contains <code><strong>parameter1=a</strong></code>, CloudFront forwards separate requests to your origin when a request contains <code><strong>parameter1=A</strong></code> and when a request contains <code><strong>parameter1=a</strong></code>. CloudFront then separately caches the corresponding objects returned by your origin separately even if the objects are identical. If you use just <strong>A</strong> or <strong>a</strong>, CloudFront forwards fewer requests to your origin.</p><p>- List parameters in the same order. As with differences in case, if one request for an object contains the query string <code><strong>parameter1=a&amp;parameter2=b</strong></code> and another request for the same object contains <code><strong>parameter2=b&amp;parameter1=a</strong></code>, CloudFront forwards both requests to your origin and separately caches the corresponding objects even if they're identical. If you always use the same order for parameters, CloudFront forwards fewer requests to your origin.</p><p><strong>Lambda@Edge</strong> is an extension of AWS Lambda, a compute service that lets you execute functions that customize the content that CloudFront delivers. You can author Node.js or Python functions in one Region, US-East-1 (N. Virginia), and then execute them in AWS locations globally that are closer to the viewer, without provisioning or managing servers. Lambda@Edge scales automatically, from a few requests per day to thousands per second. Processing requests at AWS locations closer to the viewer instead of on origin servers significantly reduces latency and improves the user experience.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\"></p><p>With Lambda@Edge, you can improve your cache hit ratio by making the following changes to query strings before CloudFront forwards requests to your origin:</p><p>- Alphabetize key-value pairs by the name of the parameter.</p><p>- Change the case of key-value pairs to lowercase.</p><p>Therefore, the correct answer is: <strong>Write a Lamda@Edge function that will normalize the query parameters by sorting them in alphabetical order and converting them into lower case. Deploy this function with the CloudFront distribution and set “viewer request” as the trigger to invoke the function. </strong>With the \"viewer request\" set as the trigger, the Lambda@Edge function will normalize the query string before CloudFront processes it. CloudFront will then see the matching cache item for the normalized request, thus increasing the cache hit ratio.</p><p>The option that says: <strong>Reconfigure the CloudFront distribution to remove the caching behavior based on query string parameters. This will cache the requests regardless of the order or case of the query parameters</strong> is incorrect. This will ignore any query parameters and will cache all requests which will cause CloudFront to return incorrect cached items to users.</p><p>The option that says: <strong>Launch a reverse proxy inside the application VPC to intercept the requests going to the origin instances. Process the query parameters to sort them by name and convert them to lowercase letters before forwarding them to the instances</strong> is incorrect. This will not increase the cache hit ratio because CloudFront already forwarded the request to the origin as the proxy processes it which defeats the purpose of having a CloudFront cache.</p><p>The option that says: <strong>Reconfigure the CloudFront distribution to ensure that the “case insensitive” option is enabled for processing query string parameters</strong> is incorrect. CloudFront is case-sensitive when caching objects. There is no \"case-insensitive\" option in CloudFront. You will have to normalize your query parameters if you want all requests to be in lower-case.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-normalize-query-string-parameters</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html</a></p><p><a href=\"https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html\">https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html</a></p><p><a href=\"https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html#cache-hit-ratio-query-string-parameters\">https://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html#cache-hit-ratio-query-string-parameters</a></p><p><br></p><p><strong>Check out these Amazon CloudFront and AWS Lambda Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><a href=\"https://tutorialsdojo.com/aws-lambda/?src=udemy\">https://tutorialsdojo.com/aws-lambda/</a></p></div>"
	},
	{
		"question": "<p>A company hosts an internal web portal on a fleet of Amazon EC2 instances that allows access to confidential files stored in an encrypted Amazon S3 bucket. Because the files contain sensitive information, the company does not want any files to traverse the public Internet. Bucket access should be restricted to only allow the web portal’s EC2 instances. To comply with the requirements, the Solutions Architect created an Amazon S3 VPC endpoint and associated it with the web portal’s VPC.</p><p>Which of the following actions should the Solutions Architect take to fully comply with the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket on the current region. Apply an Amazon S3 bucket policy that only allows access from the VPC private subnets. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Create an IAM role that grants access to the S3 bucket and attach it to the application EC2 instances. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint and those using the IAM role.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Apply an Amazon S3 bucket policy that includes the <code>aws:SourceIp</code> condition to deny all access except those coming from the application EC2 instances IP addresses. Update the route table for the VPC to ensure that the VPC endpoint is associated only with the application instances subnets.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you create an <strong>interface or gateway endpoint</strong>, you can attach an endpoint policy to it that controls access to the service to which you are connecting. Endpoint policies must be written in JSON format. Not all services support endpoint policies.</p><p>A <strong>VPC endpoint policy</strong> is an IAM resource policy that you attach to an endpoint when you create or modify the endpoint. If you do not attach a policy when you create an endpoint, we attach a default policy for you that allows full access to the service. If a service does not support endpoint policies, the endpoint allows full access to the service. An endpoint policy does not override or replace IAM user policies or service-specific policies (such as S3 bucket policies). It is a separate policy for controlling access from the endpoint to the specified service.</p><p>You cannot attach more than one policy to an endpoint. However, you can modify the policy at any time. If you do modify a policy, it can take a few minutes for the changes to take effect.</p><p>Your endpoint policy can be like any IAM policy; however, take note of the following:</p><p>- Your policy must contain a Principal element.</p><p>- The size of an endpoint policy cannot exceed 20,480 characters.</p><p>When you create or modify an endpoint, you specify the VPC route tables that are used to access the service via the endpoint. A route is automatically added to each of the route tables with a destination that specifies the AWS prefix list ID of the service (<code>pl-xxxxxxxx</code>), and a target with the endpoint ID (<code>vpce-xxxxxxxx</code>).</p><p>The following example bucket policy blocks traffic to the bucket unless the request is from specified VPC endpoints (<code>aws:sourceVpce</code>):</p><pre class=\"prettyprint linenums\">{\n&nbsp; \"Id\": \"VPCe\",\n&nbsp; \"Version\": \"2012-10-17\",\n&nbsp; \"Statement\": [\n&nbsp;&nbsp;&nbsp; {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"Sid\": \"VPCe\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"Action\": \"s3:*\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"Effect\": \"Deny\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"Resource\": [\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ],\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"Condition\": {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"StringNotEquals\": {\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"aws:SourceVpce\": [\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"vpce-1111111\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"vpce-2222222\"\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ]\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; },\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"Principal\": \"*\"\n&nbsp;&nbsp;&nbsp; }\n&nbsp; ]\n}</pre><p>- To use this policy with the <code>aws:sourceVpce</code> condition, you must attach a VPC endpoint for Amazon S3. The VPC endpoint must be attached to the route table of the EC2 instance's subnet, and in the same AWS Region as the bucket.</p><p>- To allow users to perform S3 actions on the bucket from the VPC endpoints or IP addresses, you must explicitly allow the user-level permissions. You can explicitly allow user-level permissions on either an AWS Identity and Access Management (IAM) policy or another statement in the bucket policy.</p><p>Therefore, the correct answer is: <strong>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Create an IAM role that grants access to the S3 bucket and attach it to the application EC2 instances. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint and those using the IAM role.</strong> This ensures that traffic to the S3 bucket are all coming from the VPC endpoint and that the application EC2 instances are the only ones allowed to access it.</p><p>The option that says: <strong>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket. Apply an Amazon S3 bucket policy that only allows access from the VPC endpoint. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list </strong>is incorrect. The gateway prefix list ID should be added to the route table in the VPC to allow access for the specific subnet, and not on the NACL.</p><p>The option that says: <strong>Apply an Amazon S3 bucket policy that includes the </strong><code><strong>aws:SourceIp</strong></code><strong> condition to deny all access except those coming from the application EC2 instances IP addresses. Update the route table for the VPC to ensure that the VPC endpoint is associated only with the application instances subnets</strong> is incorrect. The <code>aws:SourceIp</code> is used for specifying external IP addresses (from the public Internet or from within the VPC). You cannot use the <code>aws:SourceIp</code> condition in your bucket policies for Amazon S3 requests coming from a VPC endpoint. When you associate a VPC endpoint to your VPC, the route tables are automatically updated to include the AWS prefix list ID.</p><p>The option that says: <strong>Create a VPC endpoint policy that restricts access to the specific Amazon S3 bucket on the current region. Apply an Amazon S3 bucket policy that only allows access from the VPC private subnets. Update the VPC’s Network Access Control List (NACL) to deny other EC2 instances from accessing the gateway prefix list</strong> is incorrect. You cannot input subnet IDs as restrictions on the bucket policies. You should use VPC endpoint or source IPs instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-access.html#vpc-endpoint-policies</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/\">https://aws.amazon.com/premiumsupport/knowledge-center/block-s3-traffic-vpc-ip/</a></p><p><br></p><p><strong>Check out these Amazon VPC and Amazon S3 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A major telecommunications company is planning to set up a disaster recovery solution for its Amazon Redshift cluster which is being used by its online data analytics application. Database encryption is enabled on their clusters using AWS KMS and it is required that the recovery site should be at least 500 miles from their primary cloud location.</p><p>Which of the following is the most suitable solution to meet these requirements and to make its architecture highly available?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Develop a scheduled job using AWS Lambda which will regularly take a snapshot of the Redshift cluster and copy it to another region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>In your Redshift cluster, enable the cross-region snapshot copy feature to copy snapshots to another region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new AWS CloudFormation stack that will deploy the cluster in another region and will regularly back up the data to an S3 bucket, configured with cross-region replication. In case of an outage in the primary region, just use the snapshot from the S3 bucket and then start the cluster.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a <code>snapshot copy grant</code> for a master key in the destination region and enable cross-region snapshots in your Redshift cluster to copy snapshots of the cluster to another region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Snapshots</strong> are point-in-time backups of a cluster. There are two types of snapshots: automated and manual. Amazon Redshift stores these snapshots internally in Amazon S3 by using an encrypted Secure Sockets Layer (SSL) connection. Amazon Redshift automatically takes incremental snapshots that track changes to the cluster since the previous automated snapshot.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_cross_region_snapshot.png\"></p><p>Automated snapshots retain all of the data required to restore a cluster from a snapshot. You can take a manual snapshot any time. When you restore from a snapshot, Amazon Redshift creates a new cluster and makes the new cluster available before all of the data is loaded, so you can begin querying the new cluster immediately. The cluster streams data on demand from the snapshot in response to active queries, then loads the remaining data in the background.</p><p>When you launch an <strong>Amazon Redshift</strong> cluster, you can choose to encrypt it with a master key from the AWS Key Management Service (AWS KMS). AWS KMS keys are specific to a region. If you want to enable cross-region snapshot copy for an AWS KMS-encrypted cluster, you must configure a <em>snapshot copy grant</em> for a master key in the destination region so that Amazon Redshift can perform encryption operations in the destination region.</p><p>Therefore, the correct answer is: <strong>Set up a </strong><code><strong>snapshot copy grant</strong></code><strong> for a master key in the destination region and enable cross-region snapshots in your Redshift cluster to copy snapshots of the cluster to another region.</strong></p><p>The option that says: <strong>Create a new AWS CloudFormation stack that will deploy the cluster in another region and will regularly back up the data to an S3 bucket, configured with cross-region replication. In case of an outage in the primary region, just use the snapshot from the S3 bucket and then start the cluster</strong> is incorrect. Using a combination of CloudFormation and a separate S3 bucket entails a lot of configuration and set up compared with just enabling cross-region snapshot copy in your Redshift cluster.</p><p>The option that says: <strong>Develop a scheduled job using AWS Lambda which will regularly take a snapshot of the Redshift cluster and copy it to another region</strong> is incorrect. It is not recommended to use AWS Lambda to copy data on your Redshift cluster to another region. You simply have to enable cross-region snapshot copy in your Redshift cluster in order to meet the requirement.</p><p>The option that says: <strong>In your Redshift cluster, enable the cross-region snapshot copy feature to copy snapshots to another region</strong> is incorrect. Although it is right to use the cross-region snapshot copy feature, you still have to configure a snapshot copy grant for a master key in the destination region so that Amazon Redshift can perform encryption operations in the destination region.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-crossregioncopy-configure\">https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#snapshot-crossregioncopy-configure</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a></p><p><br></p><p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company stores confidential files on an Amazon S3 bucket. There was a recent production incident in the company in which the files that are stored in an S3 bucket were accidentally made public. This has caused data leakage that affected the company revenue. The management has instructed the solutions architect to come up with a solution to safeguard the S3 bucket. The solution should only allow private files to be uploaded to the S3 bucket and no file should have a public read or public write access.</p><p>Which of the following options should the solutions architect implement to meet the above requirements with MINIMAL effort?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up a policy that restricts all <code>s3:PutObject</code> actions of the user to have a <code>private</code> canned ACL only which prohibits any public access to the uploaded objects.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS Organizations and create a new Service Control Policy (SCP) that will deny public objects from being uploaded to the Amazon S3 bucket. Attach the SCP to the AWS account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the <code>s3-bucket-public-read-prohibited</code> and <code>s3-bucket-public-write-prohibited</code> managed rules in AWS Config to restrict all users from uploading publicly accessible and writable files to the S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Enable Amazon S3 Block Public Access in the S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon S3 provides Block Public Access settings for buckets and accounts to help you manage public access to Amazon S3 resources. By default, new buckets and objects don't allow public access, but users can modify bucket policies or object permissions to allow public access. Amazon S3 Block Public Access provides settings that override these policies and permissions so that you can limit public access to these resources.</p><p>With Amazon S3 Block Public Access, account administrators and bucket owners can easily set up centralized controls to limit public access to their Amazon S3 resources that are enforced regardless of how the resources are created.</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_disable_public_access.png\"></p><p>Therefore, the correct answer is: <strong>Enable Amazon S3 Block Public Access in the S3 bucket.</strong> It provides a way to meet the requirements with minimal effort.</p><p>When Amazon S3 receives a request to access a bucket or an object, it determines whether the bucket or the bucket owner's account has a Block Public Access setting. If there is an existing Block Public Access setting that prohibits the requested access, then Amazon S3 rejects the request. Amazon S3 Block Public Access provides four settings. These settings are independent and can be used in any combination, and each setting can be applied to a bucket or to an entire AWS account.</p><p>If a bucket has Block Public Access settings that are different from its owner's account, Amazon S3 applies the most restrictive combination of the bucket-level and account-level settings. Thus, when Amazon S3 evaluates whether an operation is prohibited by a Block Public Access setting, it rejects any request that would violate either a bucket-level or an account-level setting.</p><p>The option that says: <strong>Set up a policy that restricts all </strong><code><strong>s3:PutObject</strong></code><strong> actions of the user to have a </strong><code><strong>private</strong></code><strong> canned ACL only which prohibits any public access to the uploaded objects</strong> is incorrect. Although this solution is possible, it entails a lot of effort to set up an IAM policy that restricts the user from uploading public objects. Using the Amazon Block Public Access is a more suitable solution for this scenario.</p><p>The option that says: <strong>Use the </strong><code><strong>s3-bucket-public-read-prohibited</strong></code><strong> and </strong><code><strong>s3-bucket-public-write-prohibited</strong></code><strong> managed rules in AWS Config to restrict all users from uploading publicly accessible and writable files to the S3 bucket</strong> is incorrect. This solution with AWS Config will only notify you and your team of public objects in the S3 bucket. It would not be able to restrict any user from uploading public objects.</p><p>The option that says: <strong>Set up AWS Organizations and create a new Service Control Policy (SCP) that will deny public objects from being uploaded to the Amazon S3 bucket, then attaching the SCP to the AWS account</strong> is incorrect. Although you can satisfy the requirement using a service control policy (SCP), it still entails a lot of effort to implement. Remember that the scenario asks you to meet the requirements with minimal effort. Enabling the Amazon S3 Block Public Access in the S3 bucket is still the easiest one to implement. An SCP is primarily used to determine what services and actions can be delegated by administrators to the users and roles in the accounts that the SCP is applied to.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A travel booking company runs its main web application on the AWS cloud. Its trip planner website provides timetables, travel alerts, and other public transportation information for trains, buses, ferries, and trams. The front-end tier is composed of an ALB in front of an Auto Scaling group of Amazon EC2 instances deployed across 3 Availability Zones and a Multi-AZ RDS for its database tier. When there are sporting events and popular concerts to be held in a city, the usage of the trip planner application spikes which causes the application servers to reach utilization of over 90%. The solutions architect must ensure that the website can quickly recover in the event that one of its Availability Zones failed during its peak usage.</p><p>Which of the following is the most cost-effective architectural design that should be implemented for this website to maintain high availability?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Increase the capacity and scaling thresholds of the Auto Scaling group to allow the application servers to scale up across all Availability Zones, which will lower the aggregate utilization of the EC2 instances. Use Reserved Instances to handle the steady-state load and a combination of On-Demand and Spot Instances to process the peak load. When the peak usage is over, scale down the number of the On-Demand and Spot instances."
			},
			{
				"correct": false,
				"answer": "To have the most cost-effective architecture, replace all of the Reserved and On-Demand EC2 instances with Spot instances across all Availability Zones. Configure an Auto Scaling group in one of the AZs for scalability."
			},
			{
				"correct": false,
				"answer": "<p>Deploy six Reserved and Spot EC2 Instances in each of the 3 Availability Zones. In this way, the remaining two Availability Zones can handle the load left behind by the Availability Zone that went down.</p>"
			},
			{
				"correct": false,
				"answer": "Deploy one On-Demand EC2 instance and two Spot EC2 Instances in each of the 3 Availability Zones. In case that one Availability Zone fails, the remaining two Availability Zones can handle the peak load."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Remember that Spot Instances are the most cost-effective type of instances that you can choose. However, this is not suitable for applications with steady state usage.</p><p>Amazon EC2 Spot instances allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price. Spot instances are recommended for:</p><p>- Applications that have flexible start and end times</p><p>- Applications that are only feasible at very low compute prices</p><p>- Users with urgent computing needs for large amounts of additional capacity</p><p><img src=\"https://media.tutorialsdojo.com/sap_spot_instances_overview.png\"></p><p>On-Demand instances are recommended for:</p><p>- Users that prefer the low cost and flexibility of Amazon EC2 without any up-front payment or long-term commitment</p><p>- Applications with short-term, spiky, or unpredictable workloads that cannot be interrupted</p><p>- Applications being developed or tested on Amazon EC2 for the first time</p><p>Reserved Instances are recommended for:</p><p>- Applications with <strong>steady state</strong> usage</p><p>- Applications that may require reserved capacity</p><p>- Customers that can commit to using EC2 over a 1 or 3 year term to reduce their total computing costs</p><p>The option that says: <strong>Increase the capacity and scaling thresholds of the Auto Scaling group to allow the application servers to scale up across all Availability Zones, which will lower the aggregate utilization of the EC2 instances. Use Reserved Instances to handle the steady-state load and a combination of On-Demand and Spot Instances to process the peak load. When the peak usage is over, scale down the number of the On-Demand and Spot instances</strong> is correct because by using Auto Scaling, you allow the application servers to scale up across all Availability Zones and handle the additional load. The combination of On-Demand and Spot Instances to process the peak load is a cost-effective solution because when the peak usage is over, you can scale down the number of your instances to save costs.</p><p>The option that says: <strong>Deploy six Reserved and Spot EC2 Instances in each of the 3 Availability Zones. In this way, the remaining two Availability Zones can handle the load left behind by the Availability Zone that went down</strong> is incorrect because having 6 Reserved Instances are still costly even if you add a Spot instance on each Availability Zone. You should also use an Auto Scaling group in order to properly scale your compute resources in accordance with your incoming traffic.</p><p>The option that says: <strong>Deploy one On-Demand EC2 instance and two Spot EC2 Instances in each of the 3 Availability Zones. In case that one Availability Zone fails, the remaining two Availability Zones can handle the peak load</strong> is incorrect because although this is a cost-effective solution, it doesn't use Auto Scaling and hence, the availability of the website is at risk. If the On-Demand instance fails then the Availability Zone is only left with 2 Spot instances, which would not be able to handle the steady state usage.</p><p>The option that says: <strong>To have the most cost-effective architecture, replace all of the Reserved and On-Demand EC2 instances with Spot instances across all Availability Zones. Configure an Auto Scaling group in one of the AZs for scalability</strong> is incorrect because although this is the most cost-effective solution, it is also the most unstable one considering that a Spot instance can be interrupted and hence, the availability of the website is compromised.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/ec2/pricing\">https://aws.amazon.com/ec2/pricing</a></p><p><a href=\"https://aws.amazon.com/ec2/spot/\">https://aws.amazon.com/ec2/spot/</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A company uses an AWS CloudFormation template to deploy its three-tier web application on the AWS Cloud. The CloudFormation template contains a custom AMI value used by the Auto Scaling group of Amazon EC2 instances. Every new version of the application corresponds to a new AMI that needs to be deployed. The company doesn’t want any downtime during the deployment process. The Solutions Architect has been tasked to implement a solution that will streamline its AMI deployment process by doing the following steps:</p><p> - Update the CloudFormation template to refer to the new AMI.</p><p> - Launch new EC2 instances from the new AMI by using the calling the <code>UpdateStack</code> API to replace the&nbsp;old EC2 instances.</p><p>Which of the following actions should the Solutions Architect take to achieve the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Update the CloudFormation template <code>AWS::AutoScaling::LaunchConfiguration</code> resource section and specify a <code>DeletionPolicy</code> attribute with <code>MinSuccessfulInstancesPercent</code> of 50.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Copy the updated template and deploy it to a new CloudFormation stack. After its successful deployment, update the Amazon Route 53 records to point to the new stack and delete the old stack.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new CloudFormation change set to view the changes in the new version of the template. Verify that the correct AMI is listed on the change before executing the change set.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Update the CloudFormation template <code>AWS::AutoScaling::AutoScalingGroup</code> resource section and specify an <code>UpdatePolicy</code> attribute with an <code>AutoScalingRollingUpdate</code>.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudFormation</strong> gives you an easy way to model a collection of related AWS and third-party resources, provision them quickly and consistently, and manage them throughout their lifecycles, by treating infrastructure as code. You can use a template to create, update, and delete an entire stack as a single unit, as often as you need to, instead of managing resources individually.</p><p>The <code><strong>AWS::AutoScaling::AutoScalingGroup</strong></code> resource defines an Amazon EC2 Auto Scaling group, which is a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_overview.png\"></p><p>When you update the launch template or launch configuration for an Auto Scaling group, this update action does not deploy any change across the running Amazon EC2 instances in the Auto Scaling group. All new instances will get the updated configuration, but existing instances continue to run with the configuration that they were originally launched with. This works the same way as any other Auto Scaling group.</p><p>You can add an <code><strong>UpdatePolicy</strong></code> attribute to your stack to perform rolling updates (or replace the group) when a change has been made to the group. Alternatively, you can force a rolling update on your instances at any time after updating the stack by starting an instance refresh.</p><p>The <code>UpdatePolicy</code> on the <code>AutoScalingGroup</code> will automatically execute the rolling deployment of the new AMI instances when the Cloudformation template is updated.</p><p>To specify how AWS CloudFormation handles rolling updates for an Auto Scaling group, use the <code><strong>AutoScalingRollingUpdate</strong></code> policy. Rolling updates enable you to specify whether AWS CloudFormation updates instances that are in an Auto Scaling group in batches or all at once. For example, suppose you have updated the <code>MaxBatchSize</code> in your stack template's <code>UpdatePolicy</code> from 1 to 10. This allows you to perform updates without causing downtime to your currently running application.</p><p>Therefore, the correct answer is: <strong>Update the CloudFormation template </strong><code><strong>AWS::AutoScaling::AutoScalingGroup</strong></code><strong> resource section and specify an </strong><code><strong>UpdatePolicy</strong></code><strong> attribute with an </strong><code><strong>AutoScalingRollingUpdatepolicy</strong></code><strong>.</strong></p><p>The option that says: <strong>Create a new CloudFormation change set to view the changes in the new version of the template. Verify that the correct AMI is listed on the change before executing the change set</strong> is incorrect. Although this is possible, the existing EC2 instances won't be replaced immediately this way. The change set will only update the Launch Configuration with the new AMI and will only be applied to the newly spawned instances, excluding the existing ones.</p><p>The option that says: <strong>Update the CloudFormation template </strong><code><strong>AWS::AutoScaling::LaunchConfiguration</strong></code><strong> resource section and specify a </strong><code><strong>DeletionPolicy</strong></code><strong> attribute with </strong><code><strong>MinSuccessfulInstancesPercentof</strong></code><strong> 50</strong> is incorrect. This option should have used a <code>CreationPolicy</code> attribute instead of a <code>DeletionPolicy</code> attribute because there is no <code>MinSuccessfulInstancesPercentof</code> element in the <code>DeletionPolicy</code> attribute.</p><p>The option that says: <strong>Copy the updated template and deploy it to a new CloudFormation stack. After its successful deployment, update the Amazon Route 53 records to point to the new stack and delete the old stack</strong> is incorrect. Although this is possible, it involves an extra step that requires updating the Route 53 entry for every deployment. This does not satisfy the deployment requirement of just calling the UpdateStack API to replace the instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-group.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_UpdateStack.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_UpdateStack.html</a></p><p><br></p><p><strong>Check out the Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>An analytics company provides big data services to various clients worldwide. For performance-testing activities, a Big Data Analytics application is using an Elastic MapReduce cluster which will only be run once. The cluster is designed to ingest 20 TB of data with a total of 30 EC2 instances and is expected to run for about 48 hours.</p><p>Which of the following options is the most cost-effective architecture to implement for this scenario without sacrificing data integrity?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "For both the master and core nodes, use Reserved EC2 instances. For the task nodes, use Spot EC2 instances."
			},
			{
				"correct": true,
				"answer": "For both the master and core nodes, use On-Demand EC2 instances. For the task nodes, use Spot EC2 instances. "
			},
			{
				"correct": false,
				"answer": "Use On-Demand instances for the core nodes. Use Reserved EC2 instances for the master node and Spot EC2 instances for the task nodes."
			},
			{
				"correct": false,
				"answer": "Use a combination of On-Demand instance and Spot Instance types for both the master and core nodes. Use On-Demand EC2 instances for the task nodes."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you set up a cluster, you choose a purchasing option for EC2 instances. You can choose On-Demand Instances, Spot Instances, or both. With On-Demand Instances, you pay for compute capacity by the hour. Optionally, you can have these On-Demand Instances use Reserved Instance or Dedicated Instance purchasing options.</p><p>With Reserved Instances, you make a one-time payment for an instance to reserve capacity. Dedicated Instances are physically isolated at the host hardware level from instances that belong to other AWS accounts. After you purchase a Reserved Instance, if all of the following conditions are true, Amazon EMR uses the Reserved Instance when a cluster launches:</p><p>- An On-Demand Instance is specified in the cluster configuration that matches the Reserved Instance specification.</p><p>- The cluster is launched within the scope of the instance reservation (the Availability Zone or Region).</p><p>- The Reserved Instance capacity is still available.</p><p><strong>Spot Instances</strong> in <strong>Amazon EMR</strong> provide an option for you to purchase Amazon EC2 instance capacity at a reduced cost as compared to On-Demand purchasing. The disadvantage of using Spot Instances is that instances may terminate unpredictably as prices fluctuate.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_emr_kerberos.png\"></p><p>With the uniform instance group configuration, you can have up to a total of 48 task instance groups. The ability to add instance groups in this way allows you to mix EC2 instance types and pricing options, such as On-Demand Instances and Spot Instances. This gives you the flexibility to respond to workload requirements in a cost-effective way.</p><p>With the instance fleet configuration, the ability to mix instance types and purchasing options are built-in, so there is only one task instance fleet.</p><p>Because Spot Instances are often used to run task nodes, Amazon EMR has default functionality for scheduling YARN jobs so that running jobs do not fail when task nodes running on Spot Instances are terminated. Amazon EMR does this by allowing application master processes to run only on core nodes. The application master process controls running jobs and needs to stay alive for the life of the job.</p><p>On-Demand instances cost more than Spot instances, which is why it is better to use Spot Instances for the task nodes to save costs. However, using Spot Instances for the master and core nodes is not recommended since a Spot Instance can potentially become unavailable during the 48-hour process.</p><p>Hence, this option is the correct answer: <strong>For both the master and core nodes, use On-Demand EC2 instances. For the task nodes, use Spot EC2 instances.</strong></p><p>Remember that in this scenario, the processing will only take 48 hours. That is why it is not suitable to use Reserved Instances since the minimum reservation for this instance purchasing option is 1 year.</p><p>Therefore, the following options are incorrect:</p><p><strong>- Use On-Demand instances for the core nodes. Use Reserved EC2 instances for the master node and Spot EC2 instances for the task nodes.</strong></p><p><strong>- For both the master and core nodes, use Reserved EC2 instances. For the task nodes, use Spot EC2 instances.</strong></p><p>The option that says: <strong>Use a combination of On-Demand instance and Spot Instance types for both the master and core nodes. Use On-Demand EC2 instances for the task nodes is</strong> incorrect because using On-Demand EC2 instances for the task nodes is not cost-efficient. A better setup is to use Spot Instances for the task nodes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11</a></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html</a></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html</a></p><p><br></p><p><strong>Check out this Amazon EMR Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-emr/?src=udemy\">https://tutorialsdojo.com/amazon-emr/</a></p></div>"
	},
	{
		"question": "<p><br>A company is developing an application that will allow biologists from around the world to submit plant genomic information and share it with other biologists. The application will expect several submissions every minute and will push about 8KB of genomic data every second to the data platform. This data needs to be processed and analyzed to provide meaningful information back to the biologists. The following are the requirements for the data platform:</p><p> - The inbound genomic data must be processed near-real-time and provide analytics.</p><p> - The received data must be stored in a flexible, parallel, and durable manner.</p><p> - After processing the data, the resulting output must be delivered to a data warehouse.</p><p>Which of the following options should the Solutions Architect implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Leverage Amazon API Gateway to accept the inbound data and send it to an Amazon SQS queue. Write an AWS Lambda function that will process the messages on the SQS queue. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a delivery stream on Amazon Kinesis Data Firehose to deliver the inbound data to an Amazon S3 bucket. Use a Kinesis client to analyze the stored data. After processing, send the results to an Amazon RDS instance.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store all inbound data files directly to an Amazon S3 bucket. Use Amazon Kinesis with Amazon SQS to analyze the data stored in the S3 bucket. After processing, send the results to an Amazon Redshift cluster.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a stream in Amazon Kinesis Data Streams to collect the inbound data. Use a Kinesis client to analyze the genomic data. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Kinesis Data Streams (KDS)</strong> is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. The data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.</p><p>You can make your streaming data available to multiple real-time analytics applications, to Amazon S3, or to AWS Lambda within 70 milliseconds of the data being collected. KDS is highly durable as it performs synchronous replication of your streaming data across three Availability Zones in an AWS Region and stores that data for up to 365 days to provide multiple layers of protection from data loss.</p><p>You can have your Kinesis Applications run real-time analytics on high frequency event data such as sensor data collected by Kinesis Data Streams, which enables you to gain insights from your data at a frequency of minutes instead of hours or days.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_streams_overview.png\"></p><p>One of the methods of developing custom consumer applications that can process data from KDS data streams is to use the <strong>Kinesis Client Library (KCL)</strong>. KCL helps you consume and process data from a Kinesis data stream by taking care of many of the complex tasks associated with distributed computing. These include load balancing across multiple consumer application instances, responding to consumer application instance failures, checkpointing processed records, and reacting to resharding.</p><p><strong>Amazon Redshift</strong> is a fully managed, petabyte-scale data warehouse service in the cloud. An Amazon Redshift data warehouse is a collection of computing resources called nodes, which are organized into a group called a cluster. Each cluster runs an Amazon Redshift engine and contains one or more databases. With Redshift, you can query and combine exabytes of structured and semi-structured data across your data warehouse, operational database, and data lake using standard SQL.</p><p>Therefore, the correct answer is: <strong>Create a stream in Amazon Kinesis Data Streams to collect the inbound data. Use a Kinesis client to analyze the genomic data. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster.</strong> Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. Your data can be made available to real-time analytics applications and then saved to Amazon Redshift for data warehousing.</p><p>The option that says: <strong>Create a delivery stream on Amazon Kinesis Data Firehose to deliver the inbound data to an Amazon S3 bucket. Use a Kinesis client to analyze the stored data. After processing, send the results to an Amazon RDS instance </strong>is incorrect. Amazon RDS is not a suitable data warehousing solution. AWS recommends Amazon Reshift as a data warehouse solution.</p><p>The option that says: <strong>Store all inbound data files directly to an Amazon S3 bucket. Use Amazon Kinesis with Amazon SQS to analyze the data stored in the S3 bucket. After processing, send the results to an Amazon Redshift cluster</strong> is incorrect. Storing files to S3 first and then sending a message to an SQS queue takes some time. This solution may not meet the near-real-time requirement. You will also have to write your own Lambda function which increases operational overhead.</p><p>The option that says: <strong>Leverage Amazon API Gateway to accept the inbound data and send it to an Amazon SQS queue. Write an AWS Lambda function that will process the messages on the SQS queue. After processing, use Amazon EMR to save the results to an Amazon Redshift cluster</strong> is incorrect. It is possible to integrate API Gateway with Amazon SQS. However, the messages will be processed not in an orderly manner required for near-real-time analysis.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-streams/\">https://aws.amazon.com/kinesis/data-streams/</a></p><p><a href=\"https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html\">https://docs.aws.amazon.com/streams/latest/dev/shared-throughput-kcl-consumers.html</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/welcome.html\">https://docs.aws.amazon.com/redshift/latest/dg/welcome.html</a></p><p><br></p><p><strong>Check out these Amazon Kinesis and Amazon Redshift Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
	},
	{
		"question": "<p>A company is running its main web service in a fleet of Amazon EC2 instances in the us-east-1 AWS Region. The EC2 instances are launched by an Auto Scaling group behind an Application Load Balancer (ALB). The EC2 instances are spread across multiple Availability Zones. The MySQL database is hosted on an Amazon EC2 instance in a private subnet. To improve the resiliency of the web service in case of a disaster, the Solutions Architect must design a data recovery strategy in another region using the available AWS services to lessen the operational overhead. The target RPO is less than a minute and the target RTO is less than 5 minutes. The Solutions Architect has started to provision the ALB and the Auto Scaling group on the us-west-2 region.</p><p>Which of the following steps should be implemented next to achieve the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Migrate the database from the Amazon EC2 instance to an Amazon Aurora global database. Set the us-east-1 region as the primary database and the us-west-2 region as the secondary database. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Set the us-east-1 region database as the master and configure a cross-Region read replica to the us-west-2 region. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Enable Multi-AZ deployment for this database. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a snapshot of the current Amazon EC2 database instance and restore the snapshot to the us-west-2 region. Configure the new EC2 instance as MySQL standby database of the us-east-1 instance. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Aurora Global Database</strong> is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages.</p><p>Critical workloads with a global footprint, such as financial, travel, or gaming applications, have strict availability requirements and may need to tolerate a region-wide outage. Traditionally this required difficult tradeoffs between performance, availability, cost, and data integrity. Global Database uses storage-based replication with typical latency of less than 1 second, using dedicated infrastructure that leaves your database fully available to serve application workloads. In the unlikely event of a regional degradation or outage, one of the secondary regions can be promoted to read and write capabilities in less than 1 minute.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_aurora_global_database.png\"></p><p>Aurora Global Database lets you easily scale database reads across the world and place your applications close to your users. Your applications enjoy quick data access regardless of the number and location of secondary regions, with typical cross-region replication latencies below 1 second. If your primary region suffers a performance degradation or outage, you can promote one of the secondary regions to take read/write responsibilities. An Aurora cluster can recover in less than 1 minute even in the event of a complete regional outage. This provides your application with an effective Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of less than 1 minute, providing a strong foundation for a global business continuity plan.</p><p><strong>Amazon Route 53 health checks</strong> monitor the health and performance of your web applications, web servers, and other resources. If you have multiple resources that perform the same function, you can configure DNS failover so that Route 53 will route your traffic from an unhealthy resource to a healthy resource. Each health check that you create can monitor one of the following:</p><p>- The health of a specified resource, such as a web server</p><p>- The status of other health checks</p><p>- The status of an Amazon CloudWatch alarm</p><p>Therefore, the correct answer is: <strong>Migrate the database from the Amazon EC2 instance to an Amazon Aurora global database. Set the us-east-1 region as the primary database and the us-west-2 region as the secondary database. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region.</strong></p><p>The option that says: <strong>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Set the us-east-1 region database as the master and configure a cross-Region read replica to the us-west-2 region. Configure Amazon Route 53 DNS entry with health checks and failover routing policy to the us-west-2 region</strong> is incorrect. Although this is possible, there is no automatic way to promote the read replica on the backup region as the master database. You need to manually configure this, and when you do, the RDS instance will reboot. In this case, you might exceed the RPO of 1 minute and RTO of 5 minutes.</p><p>The option that says: <strong>Migrate the database from the Amazon EC2 instance to an Amazon RDS for MySQL instance. Enable Multi-AZ deployment for this database. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region</strong> is incorrect. Multi-AZ deployment will protect you from outages on single AZ’s only. It will not protect your database from regional outages.</p><p>The option that says: <strong>Create a snapshot of the current Amazon EC2 database instance and restore the snapshot to the us-west-2 region. Configure the new EC2 instance as MySQL standby database of the us-east-1 instance. Configure Amazon Route 53 DNS entry with failover routing policy to the us-west-2 region</strong> is incorrect. Although this is a possible solution, the requirement is to use the available AWS services for lower operational overhead. This requires extra management effort to set up, configure and manage the database on the EC2 instance, instead of using a managed Amazon RDS database. Moreover, it won't be able to satisfy the requirement of providing a 1-minute RPO and 5-minute RTO.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/aurora/global-database/\">https://aws.amazon.com/rds/aurora/global-database/</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><br></p><p><strong>Check out these Amazon Aurora Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-aurora-vs-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A multinational corporation has recently acquired a smaller company. The solutions architect was instructed to consolidate the multiple AWS accounts of both entities using AWS Organizations. The solutions architect has set up the required service control policies (SCPs) to simplify the process of controlling access permissions for each individual account and Organizational Units (OUs). However, one account is having trouble creating a new S3 bucket, and it is required to investigate the cause of this issue. The account has the following SCP attached:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"pln\">    </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"cloudtrail:*\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"pln\">    </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L7\"><span class=\"pln\">  </span><span class=\"pun\">},</span></li><li class=\"L8\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L9\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L0\"><span class=\"pln\">    </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"iam:*\"</span><span class=\"pun\">,</span></li><li class=\"L1\"><span class=\"pln\">    </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"*\"</span></li><li class=\"L2\"><span class=\"pln\">  </span><span class=\"pun\">}</span></li><li class=\"L3\"><span class=\"pln\"> </span><span class=\"pun\">]</span></li><li class=\"L4\"><span class=\"pun\">}</span></li></ol></pre></div></div><p><br></p><p>Each IAM user of the account has the following IAM policy attached:</p><p><br></p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L3\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L4\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span></li><li class=\"L5\"><span class=\"pln\">    </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"s3:*\"</span><span class=\"pun\">,</span></li><li class=\"L6\"><span class=\"pln\">    </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L7\"><span class=\"pln\">     </span><span class=\"str\">\"arn:aws:s3:::*\"</span></li><li class=\"L8\"><span class=\"pun\">]</span></li><li class=\"L9\"><span class=\"pun\">},</span></li><li class=\"L0\"><span class=\"pln\">  </span><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"pln\">    </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"Deny\"</span><span class=\"pun\">,</span></li><li class=\"L2\"><span class=\"pln\">    </span><span class=\"str\">\"NotAction\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"str\">\"s3:*\"</span><span class=\"pun\">,</span></li><li class=\"L3\"><span class=\"pln\">    </span><span class=\"str\">\"NotResource\"</span><span class=\"pun\">:</span><span class=\"pln\"> </span><span class=\"pun\">[</span></li><li class=\"L4\"><span class=\"pln\">     </span><span class=\"str\">\"arn:aws:s3:::*\"</span></li><li class=\"L5\"><span class=\"pln\">   </span><span class=\"pun\">]</span></li><li class=\"L6\"><span class=\"pln\">  </span><span class=\"pun\">}</span></li><li class=\"L7\"><span class=\"pln\"> </span><span class=\"pun\">]</span></li><li class=\"L8\"><span class=\"pun\">}</span></li></ol></pre></div></div><p><br></p><p>Based on the provided SCP and IAM policy, which of the following options could be the possible root cause of this problem?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Both the IAM policy and the SCP are the problem. The SCP should explicitly allow S3 bucket creation in its policy and the IAM policy should exactly match the permissions of the SCP.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The IAM policy is the root cause because you have denied user permissions to execute any S3-related actions.</p>"
			},
			{
				"correct": true,
				"answer": "<p>The SCP is the root cause since it does not explicitly allow the required action that would enable the account to create an S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>The SCP is the root cause because it does not support whitelisting actions of the AWS resources.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>service control policy (SCP)</strong> is a policy that specifies the services and actions that users and roles can use in the specified AWS accounts. SCPs are similar to IAM permission policies except that they don't grant any permissions. Instead, SCPs specify the maximum permissions for an organization, organizational unit (OU), or account. When you attach an SCP to your organization root or an OU, the SCP limits permissions for entities in member accounts. Even if a user is granted full administrator permissions with an IAM permission policy, any access that is not explicitly allowed or that is explicitly denied by the SCPs affecting that account is blocked.</p><p>For example, if you assign an SCP that allows only database service access to your \"database\" account, then any user, group, or role in that account is denied access to any other service's operations. SCPs are available only when you enable all features in your organization.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_organization_s3.png\"></p><p>By default, an SCP named <strong>FullAWSAccess</strong> is attached to every root, OU, and account. This default SCP allows all actions and all services. So in a new organization, until you start creating or manipulating the SCPs, all of your existing IAM permissions continue to operate as they did. As soon as you apply a new or modified SCP to a root or OU that contains an account, the permissions that your users have in that account become filtered by the SCP. Permissions that used to work might now be denied if they're not allowed by the SCP at every level of the hierarchy down to the specified account.</p><p>Therefore, the correct answer is: <strong>The SCP is the root cause since it does not explicitly allow the required action that would enable the account to create an S3 bucket.</strong> The default service policy was changed which means that you would need to explicitly allow your account access to S3 to be able to create buckets. By removing the default FullAWSAccess SCP, all actions for all services are now implicitly denied. To use SCPs as a whitelist, you must replace the AWS-managed <strong>FullAWSAccess</strong> SCP with an SCP that explicitly permits only those services and actions that you want to allow. Your custom SCP then overrides the implicit Deny with an explicit Allow for only those actions that you want to permit.</p><p>The option that says: <strong>The SCP is the root cause because it does not support whitelisting actions of the AWS resources</strong> is incorrect. The SCP format is correct, and it definitely does support the whitelisting feature.</p><p>The option that says: <strong>The IAM policy is the root cause because you have denied user permissions to execute any S3-related actions</strong> is incorrect. The IAM policy allows the user to perform all actions under Amazon S3. The included Deny policy only affects actions on AWS resources that are not of S3, and therefore, should not hinder you from creating S3 buckets. Take note that the <code>NotAction/NotResource</code> is an advanced policy element that explicitly matches everything except the list of action/resources that you specified.</p><p>The option that says:<strong> Both the IAM policy and the SCP are the problem. The SCP should explicitly allow S3 bucket creation in its policy and the IAM policy should exactly match the permissions of the SCP</strong> is incorrect. The IAM policy does not necessarily need to match the service control policy. Although it is true that you would have to explicitly allow S3 bucket creation on the SCP, an SCP does not grant any permissions. Users and roles must still be granted permissions with appropriate IAM permission policies. A user without any IAM permission policies has no access at all, even if the applicable SCPs allow all services and all actions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_deny-except-bucket.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_deny-except-bucket.html</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p></div>"
	},
	{
		"question": "<p>A company runs its critical application in an Auto Scaling group of Amazon EC2 instances that uses ElastiCache with Append Only Files (AOF) enabled in multiple AWS regions. Recently, one of the regions experienced a power outage due to a storm which has affected the business revenue.</p><p>Assuming that only a short recovery downtime period is allowed, how should the solutions architect maintain site availability in case an event like this occurs again in the future?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set up a DNS active-active failover using latency based routing policy that resolves to an ELB. Configure the 'Evaluate Target Health' attribute to Yes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Consolidate all of your VPCs across multiple regions into a single Private Hosted Zone using Route 53.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a dedicated Transit VPC to directly route multi-VPC traffic over a VPN connection across multiple regions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Enable Domain Name System Security Extensions (DNSSEC) in your domain. Configure Route 53 to automatically failover the traffic to a secondary group of healthy resources on standby. Configure the 'Evaluate Target Health' attribute to No.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>DNS active-active failover allows access to your unhealthy instances to be redirected to active instances. Together with latency-based routing, customers accessing your web servers will be balanced throughout available healthy instances based on latency.</p><p><img src=\"https://media.tutorialsdojo.com/sap_route53_active_avtive.png\"></p><p>Hence, the correct answer is: <strong>Set up a DNS active-active failover using latency based routing policy that resolves to an ELB. Configure the 'Evaluate Target Health' attribute to Yes.</strong></p><p>The option that says: <strong>Console all of your VPCs across multiple regions into a single Private Hosted Zone using Route 53</strong> is incorrect because private hosted zones are primarily used to specify how you want to route traffic in a single VPC or a group of VPCs, instead of going through the public Internet. This does not provide an active-active failover.</p><p>The option that says: <strong>Enable Domain Name System Security Extensions (DNSSEC) in your domain. Configure Route 53 to automatically failover the traffic to a secondary group of healthy resources on standby. Configure the 'Evaluate Target Health' attribute to No</strong> is incorrect because DNSSEC is primarily used to protect your domain from DNS spoofing or man-in-the-middle attacks. If you set Evaluate Target Health to No, Route 53 continues to route traffic to the records that an alias record refers to even if health checks for those records are failing. Thus, this configuration will lead to unavailability issues on the application.</p><p>The option that says: <strong>Create a dedicated Transit VPC to directly route multi-VPC traffic over a VPN connection across multiple regions</strong> is incorrect because a Transit VPC is not suitable in providing a failover routing for your resources. This set up is more suitable for scenarios where you are designing a Multiple-VPC VPN connection sharing.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-types.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html</a></p><p><a href=\"https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html\">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-configuring.html</a></p><p><br></p><p><strong>Check out this Amazon Route 53 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-route-53/?src=udemy\">https://tutorialsdojo.com/amazon-route-53/</a></p></div>"
	},
	{
		"question": "<p>A payment startup developed an application that enables users to pay for their rent using either their debit or credit cards. Since the system is not fully compliant with the Payment Card Industry Data Security Standard (PCI DSS), they are using a third-party payment service to handle and process credit card payments on their platform. Their prototype payments portal uses auto-scaled EC2 instances<strong> </strong>that are hosted in the default VPC, which are launched in private subnets behind an internal-facing ELB. Upon user payment, the system should connect to the payment service over the Internet to complete the transaction. The solution should be highly available and scalable to avoid any degradation of the service.</p><p>Which of the following is the best option to satisfy the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Route credit card payment requests from the Amazon EC2 instances through NAT Instance with an associated Elastic IP address.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Route credit card payment requests from the EC2 instances through NAT Gateway with an associated Elastic IP address.</p>"
			},
			{
				"correct": false,
				"answer": "Whitelist the Internet Gateway Public IP in the Security Group and route payment requests through the Internet Gateway."
			},
			{
				"correct": false,
				"answer": "Route payment requests from the application servers through the ELB directly, which will then be routed to a Customer Gateway."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use a <strong>network address translation (NAT) gateway</strong> to enable instances in a private subnet to connect to the Internet or other AWS services, but prevent the Internet from initiating a connection with those instances.</p><p>To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside. You must also specify an Elastic IP address to associate with the NAT gateway when you create it. The Elastic IP address cannot be changed once you associate it with the NAT Gateway. After you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway. This enables instances in your private subnets to communicate with the internet. Each NAT gateway is created in a specific Availability Zone and implemented with redundancy in that zone. You have a limit on the number of NAT gateways you can create in an Availability Zone.</p><p><img src=\"https://media.tutorialsdojo.com/sap_nat_gateway.png\"></p><p>Remember the difference between NAT Instance and NAT Gateways. A NAT Instance needs to use a script to manage failover between instances while this is done automatically in NAT gateways.</p><p>Therefore the correct answer is: <strong>Route credit card payment requests from the EC2 instances through NAT Gateway with an associated Elastic IP address</strong> is correct as a NAT gateway is the right type of NAT setup due to its high availability.</p><p>The option that says: <strong>Whitelisting the Internet Gateway Public IP in the Security Group and routing payment requests through the Internet Gateway</strong> is incorrect because this won't solve the problem of connecting to the external payment on the internet. Additionally, if you want the EC2 instances to connect to the Internet, allow them to connect to the NAT gateway, not the Internet Gateway.</p><p>The option that says: <strong>Route payment requests from the application servers through the ELB directly, which will then be routed to a Customer Gateway</strong> is incorrect because what you need here is a NAT Gateway and not a Customer Gateway. The use of an ELB is also not suitable for this scenario.</p><p>The option that says: <strong>Route credit card payment requests from the Amazon EC2 instances through NAT Instance with an associated Elastic IP address</strong> is incorrect because a NAT instance is not as highly available and scalable as a NAT gateway.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A cryptocurrency startup owns multiple AWS accounts which are all linked under AWS Organizations. Due to the financial nature of the business, the DevOps lead has been instructed by the CTO to prepare for IT auditing activities to meet industry compliance requirements.</p><p>Which of the following provides the most durable and secure logging solution that can be used to track changes made to all of the company’s AWS resources globally?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "1. Launch a new CloudTrail trail using the AWS console with an existing S3 bucket to store the logs and with the \"Apply trail to all regions\" checkbox enabled.\n2. Enable MFA Delete on the S3 bucket."
			},
			{
				"correct": false,
				"answer": "1. Launch a new CloudTrail with one new S3 bucket to store the logs. \n2. Configure SNS to send log file delivery notifications to your management system. \n3. Enable MFA Delete and Log Encryption on the S3 bucket."
			},
			{
				"correct": true,
				"answer": "<p>1. Launch a new CloudTrail trail using the AWS console with one new S3 bucket to store the logs and with the \"Enable for all accounts in my organization\" checkbox enabled.</p><p>2. Enable MFA Delete and Log Encryption on the S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Launch three new CloudTrail trails using three new S3 buckets to store the logs for the AWS Management console, for AWS SDKs, and for the AWS CLI.</p><p>2. Enable MFA Delete and Log Encryption on the S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CloudTrail</strong> is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_enable_organization.png\"></p><p>CloudTrail is enabled on your AWS account when you create it. When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. You can easily view recent events in the CloudTrail console by going to Event history. You can also enable the tracking of multi-region and global events. By default, the log files delivered by CloudTrail to your bucket are encrypted by Amazon server-side encryption with Amazon S3-managed encryption keys (SSE-S3). To provide a security layer that is directly manageable, you can instead use server-side encryption with AWS KMS–managed keys (SSE-KMS) for your CloudTrail log files.</p><p>If you have created an organization in <strong>AWS Organizations</strong>, you can create a trail that will log all events for all AWS accounts in that organization. This is sometimes referred to as an <strong>organization trail</strong>. You can also choose to edit an existing trail in the management account and apply it to an organization, making it an organization trail. Organization trails log events for the management account and all member accounts in the organization.</p><p><strong>Organization trails</strong> are similar to regular trails in many ways. You can create multiple trails for your organization, and choose whether to create an organization trail in all regions or a single region, and what kinds of events you want logged in your organization trail, just as in any other trail.</p><p>To create an organization trail, ensure that the \"Enable for all accounts in my organization\" option is checked when you create a new CloudTrail trail.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_organization.png\"></p><p>To protect your logs, you can encrypt the S3 bucket and add MFA Delete to protect your trail logs from accidental deletions. In this scenario, the following option is the best answer as it provides all of the things mentioned above:</p><p><strong>1. Launch a new CloudTrail trail using the AWS console with one new S3 bucket to store the logs and with the \"Enable for all accounts in my organization\" checkbox enabled.</strong></p><p><strong>2. Enable MFA Delete and Log Encryption on the S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because although CloudTrail encrypts the data by default using SSE-S3, it is still more secure if you enabled log encryption and use SSE-KMS. Take note that the scenario asked for the most durable and secure logging solution:</p><p><strong>1. Launch a new CloudTrail trail using the AWS console with an existing S3 bucket to store the logs and with the \"Apply trail to all regions\" checkbox</strong></p><p><strong>2. Enable MFA Delete on the S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because the multi-region option is not enabled which is needed to fetch all CloudTrail trail from all AWS regions:</p><p><strong>1. Launch a new CloudTrail with one new S3 bucket to store the logs.</strong></p><p><strong>2. Configure SNS to send log file delivery notifications to your management system.</strong></p><p><strong>3. Enable MFA Delete and Log Encryption on the S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because this option creates too many S3 buckets that are unnecessary whereas all of the events can be easily logged in just a single S3 bucket:</p><p><strong>1. Launch three new CloudTrail trails using three new S3 buckets to store the logs for the AWS Management console, for AWS SDKs, and for the AWS CLI.</strong></p><p><strong>2. Enable MFA Delete and Log Encryption on the S3 bucket.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-encryption-cli.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-encryption-cli.html</a></p><p><a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html\">https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html</a></p><p><br></p><p><strong>Check out this AWS CloudTrail Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudtrail/?src=udemy\">https://tutorialsdojo.com/aws-cloudtrail/</a></p></div>"
	},
	{
		"question": "<p>A company wants to migrate its on-premises application to the AWS cloud. Due to limited manpower, the company wants to utilize fully managed AWS services as much as possible. This way, there will be less maintenance work after the migration. The application processes large files containing sensitive information so the company has the following requirements:</p><p> - Data encryption at rest and in transit are both required on all files that will be processed by the application.</p><p> - The storage solution must be highly durable and available.</p><p> - The company must be able to use its own encryption key and then periodically rotated for improved security.</p><p> - Amazon Redshift Spectrum will be used to analyze the migrated data.</p><p>Which of the following should the Solutions Architect implement to achieve these requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Provision an AWS Storage Gateway – File Gateway device in the on-premises data center. Enable encryption on the file share with AWS KMS. The data will be transferred securely to an Amazon S3 bucket with SSE-S3 encryption.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store the data files in an Amazon EC2 instance with an encrypted EBS volume. Use AWS KMS to encrypt the EBS volume and enable automatic key rotation.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store the data files on an Amazon DynamoDB table. Leverage on the default SSL connection settings of the DynamoDB table. Use AWS KMS to encrypt the table and enable automatic key rotation.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon S3 bucket to store all data. Enable server-side encryption with AWS KMS (SSE-KMS). Apply a bucket policy that enforces HTTPS only connections to the S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Storage Service (Amazon S3)</strong> is an object storage service that offers scalability, data availability, security, and performance. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.</p><p>Amazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket. You can set default encryption on a bucket so that all new objects are encrypted when they are stored in the bucket. The objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS).</p><p><strong>SSE-S3</strong> requires that Amazon S3 manage the data and the encryption keys.</p><p><strong>SSE-C</strong> requires that you manage the encryption key.</p><p><strong>SSE-KMS</strong> requires that AWS manage the data key but you manage the customer master key (CMK) in AWS KMS.</p><p>When you configure your bucket to use default encryption with SSE-KMS, you can also enable an S3 Bucket Key to decrease request traffic from Amazon S3 to AWS Key Management Service (AWS KMS) and reduce the cost of encryption. When you configure your bucket to use an S3 Bucket Key for SSE-KMS on new objects, AWS KMS generates a bucket-level key that is used to create a unique data key for objects in the bucket. This bucket key is used for a time-limited period within Amazon S3, reducing the need for Amazon S3 to make requests to AWS KMS to complete encryption operations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_on_premises.PNG\"></p><p>By default, Amazon S3 allows both HTTP and HTTPS requests. To comply with the<code> s3-bucket-ssl-requests-only</code> rule (only accepting HTTPS connections), your bucket policy should explicitly deny access to HTTP requests. To determine HTTP or HTTPS requests in a bucket policy, use a condition that checks for the key <code>\"aws:SecureTransport\"</code>. When this key is set to <code>true</code>, this means that the request is sent through HTTPS. To be sure to comply with the <code>s3-bucket-ssl-requests-only</code> rule, create a bucket policy that explicitly denies access when the request meets the condition <code>\"aws:SecureTransport\": \"false\"</code>. This policy explicitly denies access to HTTP requests.</p><p>Therefore, the correct answer is: <strong>Create an Amazon S3 bucket to store all data. Enable server-side encryption with AWS KMS (SSE-KMS). Apply a bucket policy that enforces HTTPS only connections to the S3 bucket.</strong></p><p>The option that says: <strong>Provision an AWS Storage Gateway – File Gateway device in the on-premises data center. Enable encryption on the file share with AWS KMS. The data will be transferred securely to an Amazon S3 bucket with SSE-S3</strong> <strong>encryption</strong> is incorrect. Although this is possible, the encryption key on the S3 with SSE-S3 is managed by AWS and not the customer. This also entails more management overhead because you need to manage your file gateway. Less maintenance is needed if data is just sent directly to an encrypted S3 bucket.</p><p>The option that says:<strong> Store the data files on an Amazon DynamoDB table. Leverage on the default SSL connection settings of the DynamoDB table. Use AWS KMS to encrypt the table and enable automatic key rotation</strong> is incorrect. The application processes large documents that may not fit on an Amazon DynamoDB table in which each entry is limited to 400 KB only.</p><p>The option that says: <strong>Store the data files in an Amazon EC2 instance with an encrypted EBS volume. Use AWS KMS to encrypt the EBS volume and enable automatic key rotation </strong>is incorrect as this entails more management overhead as you need to provision your own EC2 instance. Amazon S3 is also more durable compared to a single Amazon EC2 instance with EBS volume.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\">https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/\">https://aws.amazon.com/premiumsupport/knowledge-center/s3-bucket-policy-for-config-rule/</a></p><p><br></p><p><strong>Check out the Amazon S3 and AWS KMS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/customer-master-keys-cmks-policy-management-in-aws-kms/?src=udemy\">https://tutorialsdojo.com/customer-master-keys-cmks-policy-management-in-aws-kms/</a></p></div>"
	},
	{
		"question": "<p>A leading financial company owns multiple AWS accounts that are consolidated under one AWS Organization. To properly manage all of the resources in your organization, the solutions architect has been tasked to ensure that the tags are always added when users create any resources across all the accounts.</p><p>Which of the following options are the recommended actions to achieve the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set up AWS Service Catalog to tag the provisioned resources with corresponding unique identifiers for portfolio, product, and users.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up the CloudFormation Resource Tags property to apply tags to certain resource types upon creation.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS generated tags by activating it in the Billing and Cost Management console of the member account.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS Systems Manager Automation to automatically add tags to your provisioned resources.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up AWS Config to add the corresponding tags to your resources right from the very moment that they are created.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>AWS offers a variety of tools to help you implement proactive tag governance practices by ensuring that tags are consistently applied when resources are created.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_tags.png\"></p><p><strong>AWS CloudFormation</strong> provides a common language for provisioning all the infrastructure resources in your cloud environment. CloudFormation templates are simple text files that create AWS resources in an automated and secure manner. When you create AWS resources using AWS CloudFormation templates, you can use the CloudFormation Resource Tags property to apply tags to certain resource types upon creation.</p><p><strong>AWS Service Catalog</strong> allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can include everything from virtual machine images, servers, software, and databases to complete multi-tier application environments. AWS Service Catalog enables a self-service capability for users, allowing them to provision the services they need while also helping you to maintain consistent governance – including the application of required tags and tag values.</p><p><strong>AWS Identity and Access Management (IAM)</strong> enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow or deny their access to AWS resources. When you create IAM policies, you can specify resource-level permissions, which include specific permissions for creating and deleting tags. In addition, you can include condition keys, such as <code>aws:RequestTag</code> and <code>aws:TagKeys</code>, which will prevent resources from being created if specific tags or tag values are not present.</p><p>Therefore, the correct answers are:</p><p><strong>- Set up AWS Service Catalog to tag the provisioned resources with corresponding unique identifiers for portfolio, product, and users.</strong></p><p><strong>- Set up the CloudFormation Resource Tags property to apply tags to certain resource types upon creation.</strong></p><p>The option that says: <strong>Set up AWS config to add the corresponding tags to your resources right from the very moment that they are created<em> </em></strong>is incorrect. Although you can use AWS Config to determine if your resources have tags or not, it does not have the capability to immediately add the corresponding tags to your resources across multiple AWS accounts by default. You usually issue the <em>TagResource </em>AWS Config API action to tag a resource in your current AWS account and not for multiple accounts. AWS Config supports Multi-Account Multi-Region Data Aggregation but you have to manually create an Aggregator, which is not mentioned in this option. You can use AWS Service Catalog in conjunction with AWS Config to satisfy the requirement.</p><p>The option that says: <strong>Set up AWS generated tags by activating it in the Billing and Cost Management console of the member account</strong> is incorrect. Although you can use the AWS generated tags feature in this scenario, you have to activate it using the master account and not on the member account.</p><p>The option that says: <strong>Set up AWS Systems Manager Automation to automatically add tags to your provisioned resources</strong> is incorrect because you cannot automatically add tags to your provisioned resources using AWS Systems Manager Automation.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf\">https://d1.awsstatic.com/whitepapers/aws-tagging-best-practices.pdf</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/03/aws-service-catalog-announces-autotags-for-automatic-tagging-of-provisioned-resources/\">https://aws.amazon.com/about-aws/whats-new/2018/03/aws-service-catalog-announces-autotags-for-automatic-tagging-of-provisioned-resources/</a></p><p><br></p><p><strong>Check out this AWS Service Catalog Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-service-catalog/?src=udemy\">https://tutorialsdojo.com/aws-service-catalog/</a></p></div>"
	},
	{
		"question": "<p>A company is hosting its three-tier web application on the us-east-1 region of AWS. The web and application tiers are stateless and both are running on their own fleet of On-Demand Amazon EC2 instances, each with its respective Auto Scaling group. The database tier is running on an Amazon Aurora database with about 40 TB of data. As part of the business continuity strategy of the company, the Solutions Architect must design a disaster recovery plan in case the primary region fails. The application requires an RTO of 30 minutes and the data tier requires an RPO of 5 minutes.</p><p>Which of the following options should the Solution Architect implement to achieve the company requirements in a cost-effective manner? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use AWS Backup to create a backup job that will copy the EC2 EBS volumes and RDS data to an Amazon S3 bucket in another region. Restore the backups in case of a disaster in the primary region.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Schedule a daily snapshot of the Amazon EC2 instances for the web and application tier. Copy the snapshot to the backup region. Restore the backups in case of a disaster in the primary region.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a cross-Region read replica of the Amazon Aurora database to the backup region. Promote this read replica as the master database in case of a disaster in the primary region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>For a quick recovery time, set up a hot-standby of web and application tier on the backup region. Redirect the traffic to the backup region in case of a disaster in the primary region.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure an automated snapshot of the Amazon Aurora database every 5 minutes. Quickly restore the database on the backup region in case of a disaster in the primary region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon EC2 EBS volumes</strong> are the primary persistent storage option for Amazon EC2. You can use this block storage for structured data, such as databases, or unstructured data, such as files in a file system on a volume. With Amazon EBS, you can create point-in-time snapshots of volumes, which we store for you in Amazon S3. After you create a snapshot and it has finished copying to Amazon S3, you can copy it from one AWS Region to another, or within the same Region.</p><p>Snapshots are useful if you want to back up your data and logs across different geographical locations at regular intervals. In case of disaster, you can restore your applications using point-in-time backups stored in the secondary Region. This minimizes data loss and recovery time.</p><p>You can create <strong>cross-region read replicas for Amazon Aurora</strong>. This allows you to serve read traffic from your users in different geographic regions and increases your application’s responsiveness. This feature also provides you with improved disaster recovery capabilities in case of regional disruptions. You can seamlessly migrate your database from one region to another by creating a cross-region read replica and promoting it to be the new primary database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_cross_region_replica.png\"></p><p>You can create an Amazon Aurora MySQL DB cluster as a read replica in a different AWS Region than the source DB cluster. You can promote an Aurora MySQL read replica to a standalone DB cluster. When you promote an Aurora MySQL read replica, its DB instances are rebooted before they become available. Typically, you promote an Aurora MySQL read replica to a standalone DB cluster as a data recovery scheme if the source DB cluster fails.</p><p>When restoring Amazon Aurora snapshots, or point-in-time restore, the restoration may take several minutes to hours. Long restore times are caused by long-running transactions in the source database at the time the backup was taken.</p><p>The option that says: <strong>Schedule a daily snapshot of the Amazon EC2 instances for the web and application tier. Copy the snapshot to the backup region. Restore the backups in case of a disaster in the primary region</strong> is correct. The web and application tiers are stateless, meaning they don’t have any important data stored on them. Therefore, copying the daily snapshot of the EC2 instance to the backup region will suffice. The RTO of 30 minutes is ample time to spawn new EC2 instances on the backup region.</p><p>The option that says: <strong>Set up a cross-Region read replica of the Amazon Aurora database to the backup region. Promote this read-replica as the master database in case of a disaster in the primary region</strong> is correct. Given that the RPO for the data tier is 5 minutes, it is better to create a cross-Region read-replica on the backup region. The primary DB instance will asynchronously replicate the data to the Read Replica. So in the event that the primary DB failed, the Read Replica contains the updated data. You can also quickly promote this as the master DB instance in case of a disaster in the primary region. You don’t have to wait for a long database snapshot restore time too, which might exceed the 30-minute RTO requirement.</p><p>The option that says: <strong>For a quick recovery time, set up a hot-standby of web and application tier on the backup region. Redirect the traffic to the backup region in case of a disaster in the primary region</strong> is incorrect. Although this is possible, this is not the most cost-effective solution as it entails a significant number of resources that are continuously running. With an RTO of 30 minutes, you can quickly restore backups of the EC2 snapshots of the web and application tier instead of running a hot-standby environment. Take note that in Disaster Recovery, a \"hot-standby\" means that the application runs in the DR region. Because it's always running, you will incur a significant amount of cost.</p><p>The option that says: <strong>Configure an automated snapshot of the Amazon Aurora database every 5 minutes. Quickly restore the database on the backup region in case of a disaster in the primary region</strong> is incorrect. Restoring 40 TB of data may not be possible if you have an RTO requirement of 30 minutes. Depending on how busy the database was during the time the snapshot was taken, the restoration process may take longer than 30 minutes to complete. Moreover, automated backups only occur once every day during the defined backup window. You can't configure it to run every 5 minutes.</p><p>The option that says: <strong>Use AWS Backup to create a backup job that will copy the EC2 EBS volumes and RDS data to an Amazon S3 bucket in another region. Restore the backups in case of a disaster in the primary region </strong>is incorrect. This may be possible, but the restoration time from the RDS backup may take more time than the required 30 minutes of RTO. The highest backup frequency in AWS Backup is every 12 hours only and not every 5-minutes. Thus, it can only provide a maximum RPO of 12 hours. A better solution is to use a Read Replica with a replication latency of only about a few minutes, providing a higher RPO.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-slow-snapshot-restore/\">https://aws.amazon.com/premiumsupport/knowledge-center/aurora-mysql-slow-snapshot-restore/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Replication.CrossRegion.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/06/amazon-aurora-now-supports-cross-region-replication/\">https://aws.amazon.com/about-aws/whats-new/2016/06/amazon-aurora-now-supports-cross-region-replication/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html</a></p><p><br></p><p><strong>Check out these Amazon EBS and Amazon RDS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-ebs/?src=udemy\">https://tutorialsdojo.com/amazon-ebs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A leading aerospace engineering company is experiencing high growth and demand on their highly available and fault-tolerant cloud services platform that is hosted in AWS. The technical lead of your team has asked you to virtually extend two existing on-premises data centers into AWS cloud to support an online flight-tracking service that is used by a lot of airline companies. The online service heavily depends on existing, on-premises resources located in multiple data centers and static content that is served from an S3 bucket. To meet the requirement, you launched a dual-tunnel VPN connection between your CGW and VGW.</p><p>In this scenario, which component of your cloud architecture represents a potential single point of failure, which you should consider changing to make the solution more highly available?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Set up a NAT Gateway in a different data center and set up another dual-tunnel VPN connection."
			},
			{
				"correct": true,
				"answer": "Create another Customer Gateway in a different data center and set up another dual-tunnel VPN connection."
			},
			{
				"correct": false,
				"answer": "Create a second Virtual Gateway in a different AZ and a Customer Gateway in a different data center. Create another dual-tunnel connection to ensure high-availability and fault-tolerance."
			},
			{
				"correct": false,
				"answer": "Create another Virtual Gateway in a different AZ and create another dual-tunnel VPN connection."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this question, you will easily get confused if you do not know the basics of VPC and other AWS fundamentals. You can eliminate the obviously wrong answers and then just choose between the remaining options.</p><p>Remember that only one virtual private gateway (VGW) can be attached to a VPC at a time.</p><p>A Site-to-Site VPN connection offers two VPN tunnels between a virtual private gateway or a transit gateway on the AWS side, and a customer gateway (which represents a VPN device) on the remote (on-premises) side. A <strong>virtual private gateway</strong> is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. You create a virtual private gateway and attach it to the VPC from which you want to create the Site-to-Site VPN connection.</p><p><img src=\"https://media.tutorialsdojo.com/sap_virtual_private_gateway.png\"></p><p>The correct answer is: <strong>Create another Customer Gateway in a different data center and setting up another dual-tunnel VPN connection.</strong> This will ensure high availability for your online flight-tracking service.</p><p>The option that says:<strong> Create another Virtual Gateway in a different AZ and create another dual-tunnel VPN connection</strong> is incorrect. There can only be one VGW attached to a VPC at a given time.</p><p>The option that says: <strong>Create a second Virtual Gateway in a different AZ and a Customer Gateway in a different data center. Create another dual-tunnel connection to ensure high-availability and fault-tolerance</strong> is incorrect. There can only be one VGW attached to a VPC at a given time.</p><p>The option that says: <strong>Setting up a NAT Gateway in a different data center and setting up another dual-tunnel VPN connection</strong> is incorrect. You don't need to use a NAT gateway in this situation. NAT is basically used to enable EC2 instances launched in the private subnet to access the Internet while blocking incoming public requests to the VPC.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Appendix_Limits.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonVPC/latest/NetworkAdminGuide/Introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\">https://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p></div>"
	},
	{
		"question": "<p>A world-renowned logistics company runs its global enterprise e-commerce platform on the AWS cloud. The company has built a multi-tier web application running in a VPC that uses an Elastic Load Balancer in front of both the web tier and the app tier, with static assets served directly from an Amazon S3 bucket. It uses a combination of Amazon RDS and DynamoDB for the dynamic data and then archiving nightly into an Amazon S3 bucket for further processing with Amazon Elastic MapReduce. After a routine audit, the company found questionable log entries and suspected that someone is attempting to gain unauthorized access to the system. The solutions architect has been tasked to improve the security of the architecture from DDoS, SQL injection, and HTTP flood attacks as well as from bad bots (content scrapers).</p><p>Which of the following approach provides the MOST suitable and scalable solution to protect the infrastructure from these kinds of security attacks?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an identical application stack that acts as a standby environment in another AWS region by using an AWS CloudFormation template. Use AWS CloudFormation StackSets to deploy the new stack and configure the security groups as well as network ACLs of the EC2 instances. Use Amazon Macie to protect the data stored in the Amazon S3 bucket. Create a Route 53 failover routing policy and configure an active-passive failover.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up AWS WAF and AWS Shield Advanced on all web endpoints. Launch AWS WAF rules against SQL injection and other common web exploits.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Establish an AWS Direct Connect (DX) connection to the VPC through a Direct Connect partner. Configure Internet connectivity to filter the traffic in hardware Web Application Firewall (WAF) and then reroute the traffic through the DX connection into the application. Use the company's wide area network (WAN) to send traffic over the DX connection.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Insert the identified suspect's source IP as an explicit inbound deny to the network ACL rules of the web tier's subnet. Set up AWS Config to periodically audit the network ACLs and ensure that the blacklisted IP addresses are always in place.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Shield</strong> is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency, so there is no need to engage AWS Support to benefit from DDoS protection.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. AWS WAF gives you control over which traffic to allow or block to your web applications by defining customizable web security rules. You can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that are designed for your specific application. New rules can be deployed within minutes, letting you respond quickly to changing traffic patterns. Also, AWS WAF includes a full-featured API that you can use to automate the creation, deployment, and maintenance of web security rules.</p><p><img src=\"https://media.tutorialsdojo.com/sap_waf_shield_cloudfront.png\"></p><p>Hence, the correct answer is: <strong>Set up AWS WAF and AWS Shield Advanced on all web endpoints. Launch AWS WAF rules against SQL injection and other common web exploits.</strong></p><p>The option that says:<strong> Create an identical application stack that acts as a standby environment in another AWS region by using an AWS CloudFormation template. Use AWS CloudFormation StackSets to deploy the new stack and configure the security groups as well as network ACLs of the EC2 instances. Use Amazon Macie to protect the data stored in the Amazon S3 bucket. Create a Route 53 failover routing policy and configure an active-passive failover</strong> is incorrect because this solution doesn't provide the necessary security protection against DDoS and other web vulnerability attacks. It only provides a disaster recovery plan in the event that your primary environment goes down. You have to set up AWS WAF and AWS Shield Advanced instead.</p><p>The option that says:<strong> Insert the identified suspect's source IP as an explicit inbound deny to the network ACL rules of the web tier's subnet. Set up AWS Config to periodically audit the network ACLs and ensure that the blacklisted IP addresses are always in place</strong> is incorrect. Even though blocking certain IPs will mitigate the risk, the attacker could maneuver the IP address and circumvent the IP check by NACL, and it does not prevent attacks from new sources of threat.</p><p>The option that says: <strong>Establish an AWS Direct Connect (DX) connection to the VPC through a Direct Connect partner. Configure Internet connectivity to filter the traffic in hardware Web Application Firewall (WAF) and then reroute the traffic through the DX connection into the application. Use the company's wide area network (WAN) to send traffic over the DX connection </strong>is incorrect. Although this option could work, the setup is very complex and it is not a cost-effective solution. Using the AWS Shield Advanced and AWS WAF combination is still the better solution.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><a href=\"https://www.slideshare.net/AmazonWebServices/aws-august-webinar-series-ddos-resiliency\">https://www.slideshare.net/AmazonWebServices/aws-august-webinar-series-ddos-resiliency</a></p><p><br></p><p><strong>Check out these AWS WAF and Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/shield/</a></p></div>"
	},
	{
		"question": "<p>A supermarket chain has a team that handles branded credit card transactions from major card schemes such as Mastercard, Visa, Discover, and AMEX. The company requested an external auditor to audit its AWS environment as part of the Payment Card Industry Data Security Standard (PCI DSS) security compliance. The external auditor has specified that they just need read-only access to the AWS resources on all accounts to perform the checks.</p><p>Which of the following options is the recommended action to give the auditor the required access?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a new IAM User which has an access key ID and a secret access key for API calls that can be used by the auditor.</p>"
			},
			{
				"correct": false,
				"answer": "Create an Active Directory account for the auditor and use identity federation for SSO to let the auditor log in to your AWS environment and conduct the audit."
			},
			{
				"correct": true,
				"answer": "Provide the auditor an AWS account with an IAM role that has read-only permissions to your AWS services. Add a permissions policy that will allow the auditor to assume the ARN role for each AWS account that has an assigned role."
			},
			{
				"correct": false,
				"answer": "Give the auditor each of your AWS users' username and password in your VPC and let the auditor use those credentials to login to a specific account and conduct the audit."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, it is recommended that you create an IAM Role which contains the required permissions needed by the auditor. This specific role can be revoked from the user once the compliance activities end.</p><p>An <strong>IAM role</strong> is similar to a user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials (password or access keys) associated with it. Instead, if a user assumes a role, temporary security credentials are created dynamically and provided to the user.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iamRole.jpg\"></p><p>Therefore, the correct answer is:<strong> Providing the auditor an AWS account with an IAM role that has read-only permissions to your AWS services and adding a permissions policy that will allow the auditor to assume the ARN role for each AWS account that has an assigned role.</strong> It creates an IAM Role that has all the necessary permission policies attached to it, which allows the auditor to assume the appropriate role while accessing the resources.</p><p>The options that says: <strong>Giving the auditor each of your AWS users' username and password in your VPC and letting the auditor use those credentials to login to a specific account and conduct the audit</strong> is incorrect. Providing the username and password is in itself a major security breach which will result in compliance failure.</p><p>The options that says: <strong>Creating an Active Directory account for the auditor and using identity federation for SSO to let the auditor log in to your AWS environment and conduct the audit</strong> is incorrect. Setting up a single sign-on feature is not enough. It is recommended to create an IAM Role specifically for the auditor's use.</p><p>The options that says: <strong>Creating a new IAM User which has an access key ID and a secret access key for API calls that can be used by the auditor</strong> is incorrect. You should use an IAM Role instead of creating an IAM User for the auditor. Having both access key ID and a secret access key is unnecessary because this is primarily used for programmatic access and for development.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company has a gaming store platform hosted in its on-premises data center for a whole variety of digital games. The application just experienced downtime last week due to a large burst in web traffic caused by a year-end sale on almost all of the games. Due to the success of the previous promotion, the CEO has planned to do the same in a few weeks, which will drive similar unpredictable bursts in web traffic. The solutions architects are looking to find ways to quickly improve the infrastructure's ability to handle unexpected increases in traffic. The web application is currently made up of a 2-tier web tier which consists of a load balancer and several web app servers, as well as a database tier that hosts an Oracle database.</p><p>Which of the following infrastructure changes should the team implement to avoid any further incidences of downtime considering that the new announcement will be done in a few weeks?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set up a CloudFront distribution to cache objects from a custom origin to offload traffic from your on-premises environment. Customize your object cache behavior, and choose a time-to-live that will determine how long objects will reside in the cache.</p>"
			},
			{
				"correct": false,
				"answer": "Migrate your environment to AWS by using AWS VM Import to quickly convert your web server into an AMI. Then set up an Auto Scaling group that uses the imported AMI. Also, create an RDS read replica and migrate the Oracle database to an RDS instance through replication."
			},
			{
				"correct": false,
				"answer": "<p>Create an AMI that can be used to launch new EC2 web servers. Then create an Auto Scaling group which will use the AMI to scale the web tier. Finally, place an Application Load Balancer to distribute traffic between your on-premises servers and servers running in AWS.</p>"
			},
			{
				"correct": false,
				"answer": "Set up an Amazon S3 bucket for website hosting. Migrate your DNS to Route 53 using zone import, and use DNS failover to failover to the hosted website in S3."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudFront</strong> is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds. CloudFront is integrated with AWS – including physical locations that are directly connected to the AWS global infrastructure, as well as software that works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code close to your viewers. In this scenario, the major points of consideration are: your application may get unpredictable bursts of traffic, you need to improve the current infrastructure in the shortest period possible, and your web servers that are on-premises.</p><p>CloudFront caches content at edge locations for a period of time that you specify. If a visitor requests content that has been cached for longer than the expiration date, CloudFront checks the origin server to see if a newer version of the content is available. If a newer version is available, CloudFront copies the new version to the edge location. Changes that you make to the original content are replicated to edge locations as visitors request the content.</p><p>Since the time period at hand is short, instead of migrating the app to AWS, you need to consider different ways where the performance would improve without doing much modification to the existing infrastructure.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_cache.png\"></p><p>Therefore, the correct answer is: <strong>Set up a CloudFront distribution to cache objects from a custom origin to offload traffic from your on-premises environment. Customize your object cache behavior, and choose a time-to-live that will determine how long objects will reside in the cache.</strong> CloudFront is a highly scalable, highly available content delivery service, which can perform excellently even in case of sudden unpredictable burst of traffic. Plus, the only change you need to make is the on-premises load balancer as the custom origin of the CloudFront distribution.</p><p>The option that says: <strong>Create an AMI that can be used to launch new EC2 web servers. Then create an Auto Scaling group which will use the AMI to scale the web tier. Finally, place an Application Load Balancer to distribute traffic between your on-premises servers and servers running in AWS</strong> is incorrect. ELB cannot do load balancing to your on-premises instances if it is not connected to your VPC either through a DirectConnect connection or a VPN.</p><p>The option that says: <strong>Migrate your environment to AWS by using AWS VM Import to quickly convert your web server into an AMI. Then set up an Auto Scaling group that uses the imported AMI. Also, create an RDS read replica and migrate the Oracle database to an RDS instance through replication</strong> is incorrect. You are supposed to improve the current situation at the shortest time possible. Migrating to AWS would be more time consuming than simply setting up the CloudFront distribution.</p><p>The option that says: <strong>Set up an Amazon S3 bucket for website hosting. Migrate your DNS to Route 53 using zone import, and use DNS failover to failover to the hosted website in S3</strong> is incorrect. You cannot host dynamic websites on S3 bucket. Also, this option provides insufficient infrastructure set up options.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/cloudfront\">https://aws.amazon.com/cloudfront</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/\">https://aws.amazon.com/blogs/networking-and-content-delivery/improve-your-website-performance-with-amazon-cloudfront/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-cloudfront-walkthrough.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A company is running hundreds of Linux-based Amazon EC2 instances launched with custom AMIs that are dedicated to specific products and services. As part of the security compliance requirements, vulnerability scanning must be done on all EC2 instances wherein each instance must be scanned and pass a Common Vulnerabilities and Exposures (CVE) assessment. Since the development team relies heavily on the custom AMIs for their deployments, the company wants to have an automated process to run the security assessment on any new AMIs and properly tag them before they can be used by the developers. To ensure continuous compliance, the security-approved AMIs must also be scanned every 30 days to check for new vulnerabilities and apply the necessary patches.</p><p>Which of the following steps should the Solutions Architect implement to achieve the security requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Install the AWS Systems Manager (SSM) agent on all EC2 instances. With the agent running, run a detailed CVE assessment scan on the EC2 instances launched from the AMIs that need scanning.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Check AWS CloudTrail logs to determine the Amazon EC2 instance IDs that were launched from the AMIs that need scanning. Use AWS Config managed rule to run CVE assessment and remediation on the instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Assessment template on Amazon Inspector to target the EC2 instances. Run a detailed CVE assessment scan on all running Amazon EC2 instances launched from the AMIs that need scanning.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Develop a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a 30-day interval cron rule on Amazon EventBridge to trigger an AWS SSM Automation document run on all EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a managed rule on AWS Config to continuously scan all running EC2 instances. For any detected vulnerability, run the designated SSM Automation document.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build automations to configure and manage instances and AWS resources.</p><p>- Create custom runbooks or use pre-defined runbooks maintained by AWS.</p><p>- Receive notifications about Automation tasks and runbooks by using Amazon EventBridge.</p><p>- Monitor Automation progress and details by using the AWS Systems Manager console.</p><p>SSM Automation offers one-click automation for simplifying complex tasks such as creating golden Amazon Machines Images (AMIs) and recovering unreachable EC2 instances. For example, you can use Use the <code>AWS-UpdateLinuxAmi</code> and <code>AWS-UpdateWindowsAmi</code> runbooks to create golden AMIs from a source AMI. You can run custom scripts before and after updates are applied. You can also include or exclude specific packages from being installed.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ssm_overview.png\"></p><p>With <strong>AWS EventBridge</strong>, you can create rules that self-trigger on an automated schedule in EventBridge using cron or rate expressions. Rate expressions are simpler to define but don't offer the fine-grained schedule control that cron expressions support. For example, with a cron expression, you can define a rule that triggers at a specified time on a certain day of each week or month. With this, you can schedule running AWS SSM Automation documents to remediate the vulnerable AMIs.</p><p>You can use <strong>Amazon Inspector</strong> to conduct a detailed scan for CVE in your fleet of EC2 instances. Amazon Inspector offers predefined software called an agent that you can optionally install in the operating system of the EC2 instances that you want to assess. Amazon Inspector also has rules packages that help verify whether the EC2 instances in your assessment targets are exposed to common vulnerabilities and exposures (CVEs). Attacks can exploit unpatched vulnerabilities to compromise the confidentiality, integrity, or availability of your service or data. The CVE system provides a reference method for publicly known information security vulnerabilities and exposures.</p><p>The option that says:<strong> Develop a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a 30-day interval cron rule on Amazon EventBridge to trigger an AWS SSM Automation document run on all EC2 instances</strong> is correct because it satisfies the requirement for updating the security-approved AMI, along with scheduled patches every 30-days using SSM Automation document. AWS SSM Automation can automatically pack AMIs after patches are applied.</p><p>The option that says: <strong>Create an Assessment template on Amazon Inspector to target the EC2 instances. Run a detailed CVE assessment scan on all running Amazon EC2 instances launched from the AMIs that need scanning</strong> is correct because Amazon Inspector can run assessments on target EC2 instances to check if they are exposed to common vulnerabilities and exposures (CVEs).</p><p>The option that says: <strong>Install the AWS Systems Manager (SSM) agent on all EC2 instances. With the agent running, run a detailed CVE assessment scan on the EC2 instances launched from the AMIs that need scanning</strong> is incorrect because the SSM agent cannot run a detailed CVE assessment scan on EC2 instances. You have to use Amazon Inspector to satisfy the given requirement.</p><p>The option that says: <strong>Write a Lambda function that will create automatic approval rules. Create a parameter on AWS SSM Parameter Store to save the list of all security-approved AMI. Set up a managed rule on AWS Config to continuously scan all running EC2 instances. For any detected vulnerability, run the designated SSM Automation document</strong> is incorrect because AWS Config cannot automatically run checks on the operating system of your Amazon EC2 instances. The requirement is to run the assessment every 30-days only and not continuously.</p><p>The option that says: <strong>Check AWS CloudTrail logs to determine the Amazon EC2 instance IDs that were launched from the AMIs that need scanning. Use AWS Config managed rule to run CVE assessment and remediation on the instances</strong> is incorrect. Although it is possible to parse the EC2 instance IDs from CloudTrail and determine the vulnerable instances, you still cannot run the CVE assessment in AWS Config for your Amazon EC2 instances. Using Amazon Inspector is the most suitable service to use in running the CVE assessment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><a href=\"https://docs.aws.amazon.com/eventbridge/latest/userguide/scheduled-events.html\">https://docs.aws.amazon.com/eventbridge/latest/userguide/scheduled-events.html</a></p><p><br></p><p><strong>Check out the AWS Systems Manager and AWS Inspector Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-inspector/?src=udemy\">https://tutorialsdojo.com/amazon-inspector/</a></p></div>"
	},
	{
		"question": "<p>A small telecommunications company has recently adopted a hybrid cloud architecture with AWS. They are storing static files of their on-premises web application on a 5 TB gateway-stored volume in AWS Storage Gateway, which is attached to the application server via an iSCSI interface. As part of their disaster recovery plan, they should be able to run the web application on AWS in case their on-premises network encountered any technical issues.</p><p>Which of the following options is the MOST suitable solution that you should implement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Generate an EBS snapshot of the static content from the AWS Storage Gateway service. Afterward, restore it to an EBS volume that you can then attach to the EC2 instance where the application server is hosted.</p>"
			},
			{
				"correct": false,
				"answer": "For the static content, create an EFS file system from the AWS Storage Gateway service and mount it to the EC2 instance where the application server is hosted."
			},
			{
				"correct": false,
				"answer": "Restore the static content by&nbsp;attaching the AWS Storage Gateway to the EC2 instance that hosts the application server. "
			},
			{
				"correct": false,
				"answer": "<p>Restore the static content from an AWS Storage Gateway to an S3 bucket and link it to the EC2 instance where the app server is running.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>By using stored volumes, you can store your primary data locally, while asynchronously backing up that data to AWS. <strong>Stored volumes</strong> provide your on-premises applications with low-latency access to their entire datasets. At the same time, they provide durable, offsite backups. You can create storage volumes and mount them as iSCSI devices from your on-premises application servers. Data written to your stored volumes are stored on your on-premises storage hardware. This data is asynchronously backed up to Amazon S3 as Amazon Elastic Block Store (Amazon EBS) snapshots.</p><p><img src=\"https://media.tutorialsdojo.com/sap_storage_gateway_stored.png\"></p><p>You can restore an Amazon EBS snapshot to an on-premises gateway storage volume if you need to recover a backup of your data. You can also use the snapshot as a starting point for a new Amazon EBS volume, which you can then attach to an Amazon EC2 instance.</p><p>Since this is using a <strong>Volume Storage Gateway</strong>, you have to generate an EBS snapshot and generate an EBS Volume to restore the data.</p><p>Therefore, the correct answer is: <strong>Generate an EBS snapshot of the static content from the AWS Storage Gateway service. Afterwards, restore it to an EBS volume that you can then attach to the EC2 instance where the application server is hosted.</strong></p><p>The option that says: <strong>Restore the static content from an AWS Storage Gateway to an S3 bucket and linking it on the EC2 instance where the app server is running</strong> is incorrect because linking the S3 bucket to the EC2 instance is not a suitable option to restore data from AWS Storage Gateway. You should generate a snapshot first and generate an EBS Volume that you can attach to the instance.</p><p>The option that says: <strong>Restore the static content by attaching the AWS Storage Gateway to the EC2 instance that hosts the application server</strong> is incorrect because you cannot directly attach the AWS Storage Gateway to a running EC2 instance which runs your application server. Although you can deploy and activate a volume or tape gateway on EC2, this instance has a different AMI than your application server which runs on a different EC2 instance. You have to generate an EBS Volume first, based on the generated snapshot from AWS Storage Gateway.</p><p>The option that says: <strong>Create an EFS file system from the AWS Storage Gateway service and mounting it to the EC2 instance where the application server is hosted for the static content</strong> is incorrect because using EFS in this scenario is not appropriate. You should use EBS Volumes to restore your data from Storage Gateway.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-stored-volume-concepts\">https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html#storage-gateway-stored-volume-concepts</a></p><p><a href=\"https://aws.amazon.com/storagegateway/faqs/\">https://aws.amazon.com/storagegateway/faqs/</a></p><p><br></p><p><strong>Check out this AWS Storage Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-storage-gateway/?src=udemy\">https://tutorialsdojo.com/aws-storage-gateway/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A fashion company in France sells bags, clothes, and other luxury items in its online web store. The online store is currently hosted on the company’s on-premises data center. The company has recently decided to move all of its on-premises infrastructure to the AWS cloud. The main application is running on an NGINX web server and a database with an Oracle Real Application Clusters (RAC) One Node configuration.</p><p>Which of the following is the best way to migrate the application to AWS and set up an automated backup?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS volumes to the EC2 instance of the database and then use the Data Lifecycle Manager to automatically create scheduled snapshots against the EBS volumes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an EC2 instance and run an NGINX server to host the application. Deploy an RDS instance and enable automated backups on the RDS RAC cluster.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS Volumes on the EC2 instance of the database and then write a shell script that runs the manual snapshot of the volumes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an On-Demand EC2 instance and run an NGINX server to host the application. Deploy an RDS instance with a Multi-AZ deployment configuration and enable automated backups on the RDS RAC cluster.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Oracle RAC is not supported by RDS. That is why you need to deploy the database in an EC2 instance and then either create a shell script to automate the backup or use the Data Lifecycle Manager to automate the process.</p><p>An Oracle Real Application Clusters (RAC) One Node option provides virtualized servers on a single machine. This provides an 'always on' availability for single-instance databases for a fraction of a cost.</p><p><img src=\"https://media.tutorialsdojo.com/sap_data_lifecycle_manager.JPG\"></p><p><strong>Amazon Data Lifecycle Manager (DLM)</strong> for EBS Snapshots provides a simple, automated way to back up data stored on Amazon EBS volumes. You can define backup and retention schedules for EBS snapshots by creating lifecycle policies based on tags. With this feature, you no longer have to rely on custom scripts to create and manage your backups.</p><p>Hence, the correct answer is: <strong>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS volumes to the EC2 instance of the database and then use the Data Lifecycle Manager to automatically create scheduled snapshots against the EBS volumes.</strong></p><p>The option that says: <strong>Launch an EC2 instance for both the NGINX server as well as for the database. Attach EBS Volumes on the EC2 instance of the database and then write a shell script that runs the manual snapshot of the volumes</strong> is incorrect. Although this approach is valid, a more suitable option is to use the Data Lifecycle Manager (DLM) to automatically take the snapshot of the EC2 instance. The DLM can also reduce storage costs by deleting outdated backups.</p><p>The following options are incorrect as these two use Amazon RDS, which doesn't natively support Oracle RAC:</p><p><strong>- Launch an EC2 instance and run an NGINX server to host the application. Deploy an RDS instance and enable automated backups on the RDS RAC cluster.</strong></p><p><strong>- Launch an On-Demand EC2 instance and run an NGINX server to host the application. Deploy an RDS instance with a Multi-AZ deployment configuration and enable automated backups on the RDS RAC cluster.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/oracle/faqs/\">https://aws.amazon.com/rds/oracle/faqs/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/\">https://aws.amazon.com/about-aws/whats-new/2015/11/self-managed-oracle-rac-on-ec2/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company recently developed a web application that processes customer behavioral data and stores the results in a DynamoDB table. The application is expected to receive a high usage load. To ensure that data is not lost when DynamoDB write requests are throttled, the solutions architect must reduce the load taken by the table.</p><p>Which of the following is the MOST cost-effective strategy for reducing the load on the DynamoDB table?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Provision more DynamoDB tables to absorb the load.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Replicate the DynamoDB table to another AWS region using global tables.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Provision higher write-capacity units (WCUs) to your DynamoDB table.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use an SQS queue to decouple messages from the application and the database.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Queuing is a commonly used solution for separating computation components in a distributed processing system. It is a form of the asynchronous communication system used in serverless and microservices architectures. Messages wait in a queue for processing, and leave the queue when received by a single consumer.</p><p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> is a scalable message queuing system that stores messages as they travel between various components of your application architecture. Amazon SQS enables web service applications to quickly and reliably queue messages that are generated by one component and consumed by another component. A queue is a temporary repository for messages that are awaiting processing.</p><p>In the below diagram, the goal is to smooth out the traffic from the application, so that the load process into DynamoDB is much more consistent. The key service used to achieve this is Amazon SQS, which holds all the items until a loader process stores the data in DynamoDB. The architecture looks like this:</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb.png\"></p><p>Therefore, the correct answer is: <strong>Use an SQS queue to decouple messages from the application and the database.</strong> Data can be lost if the application fails to store it in DynamoDB due to throttling. Amazon SQS can reduce the load by temporarily holding the data until the DynamoDB throttling subsides. It is scalable as well as cost-efficient.</p><p>The option that says: <strong>Replicate the DynamoDB table to another AWS region using global tables</strong> is incorrect. The global table is a DynamoDB feature that is primarily used for applications requiring multi-region fault tolerance. It won't reduce the load on the existing DynamoDB table.</p><p>The option that says: <strong>Provision higher write-capacity units (WCUs) to your DynamoDB table</strong> is incorrect because increasing the write capacity is an expensive option.</p><p>The option that says: <strong>Provision more DynamoDB tables to absorb the load</strong> is incorrect. While it is possible to create another table for the application to use, it is an anti-pattern and will require a lot of development overhead to keep the data from the tables in sync, not to mention when joining queries. Remember that JOIN operations are not possible in DynamoDB so you need to implement this at the application level. Furthermore, this is not a cost-efficient solution.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/creating-a-scalable-serverless-import-process-for-amazon-dynamodb/\">https://aws.amazon.com/blogs/compute/creating-a-scalable-serverless-import-process-for-amazon-dynamodb/</a></p><p><a href=\"https://aws.amazon.com/blogs/database/implementing-priority-queueing-with-amazon-dynamodb/\">https://aws.amazon.com/blogs/database/implementing-priority-queueing-with-amazon-dynamodb/</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
	},
	{
		"question": "<p>A global data analytics firm has various data centers from different countries all over the world. The staff are regularly uploading analytics, financial, and regulatory files of each of their respective data centers to a web portal deployed in AWS, which uses an S3 bucket named <code>global-analytics-reports-bucket</code> to durably store the data. The staff download various reports from a CloudFront distribution which uses the <code>global-analytics-reports-bucket</code> S3 bucket as the origin. The security team noticed that the staff are using both the CloudFront link and the direct Amazon S3 URLs to download the reports. The security team sees this as a security risk and they recommended implementing a way to prevent anyone from bypassing CloudFront and using the direct Amazon S3 URLs.</p><p>Which of the following options should the solutions architect implement to meet the above requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "1. Set up a field-level encryption configuration in the CloudFront distribution.\n2. Remove anyone else's permission to use Amazon S3 URLs to read the objects."
			},
			{
				"correct": false,
				"answer": "1. Configure the distribution to use Signed URLs.\n2. Create a special CloudFront user called an origin access identity (OAI).\n3. Give the origin access identity permission to read the objects in your bucket."
			},
			{
				"correct": true,
				"answer": "<p>1. Create a special CloudFront user called an origin access identity (OAI) and associate it with your CloudFront distribution.</p><p>2. Give the origin access identity permission to read the objects in your bucket.</p><p>3. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</p>"
			},
			{
				"correct": false,
				"answer": "1. In your CloudFront distribution, use a custom SSL instead of the default SSL.\n2. Remove anyone else's permission to use Amazon S3 URLs to read the objects."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can optionally secure the content in your Amazon S3 bucket so that users can access it through CloudFront but cannot access it directly by using Amazon S3 URLs. This prevents someone from bypassing CloudFront and using the Amazon S3 URL to get content that you want to restrict access to. This step isn't required to use signed URLs, but we recommend it. Be aware that this option is only available if you have not set up your Amazon S3 bucket as a website endpoint.</p><p>To require that users access your content through CloudFront URLs, you do the following tasks:</p><p>- Create a special CloudFront user called an origin access identity and associate it with your CloudFront distribution.</p><p>- Give the origin access identity permission to read the files in your bucket.</p><p>- Remove permission for anyone else to use Amazon S3 URLs to read the files.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\"></p><p>In this scenario, the main objective is to prevent the staff from using the direct Amazon S3 URLs to download the reports. The best solution that you can choose here is to use an Origin Access Identity (OAI) and remove anyone else's permission to use the S3 URLs to read the objects. Hence, the correct answer is the following option:</p><p><strong>1. Create a special CloudFront user called an origin access identity (OAI).</strong></p><p><strong>2. Give the origin access identity permission to read the objects in your bucket.</strong></p><p><strong>3. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</strong></p><p><br></p><p>The following option is incorrect because SSL is not needed in this particular scenario:</p><p><strong>1. In your CloudFront distribution, use a custom SSL instead of the default SSL.</strong></p><p><strong>2. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</strong></p><p>What you need to implement is an OAI.</p><p><br></p><p>The following option is incorrect because the field-level encryption configuration is mainly used for safeguarding sensitive fields in your CloudFront, and not suitable for this scenario:</p><p><strong>1. Set up a field-level encryption configuration in the CloudFront distribution.</strong></p><p><strong>2. Remove anyone else's permission to use Amazon S3 URLs to read the objects.</strong></p><p><br></p><p>The following option is incorrect because although it is recommended to use Signed URLs and OAI in CloudFront, this option is still missing the crucial step of removing anyone else's permission to use the S3 URLs to read the objects:</p><p><strong>1. Configure the distribution to use Signed URLs.</strong></p><p><strong>2. Create a special CloudFront user called an origin access identity (OAI).</strong></p><p><strong>3. Give the origin access identity permission to read the objects in your bucket.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/PrivateContent.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#private-content-overview-s3\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html#private-content-overview-s3</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A multinational investment bank has multiple cloud architectures across the globe. The company has a VPC in the US East region for their East Coast office and another VPC in the US West for their West Coast office. There is a requirement to establish a low latency, high-bandwidth connection between their on-premises data center in Texas and both of their VPCs in AWS.</p><p>Which of the following options should the solutions architect implement to achieve the requirement in a cost-effective manner?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Set up two separate VPC peering connections for the two VPCs and for the on-premises data center.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Establish a Direct Connect connection between the VPC in US East region and the on-premises data center in Texas, and then establish another Direct Connect connection between the VPC in US West region and the on-premises data center.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up an AWS VPN managed connection between the VPC in US East region and the on-premises data center in Texas.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up an AWS Direct Connect Gateway with two virtual private gateways. Launch and connect the required Private Virtual Interfaces to the Direct Connect Gateway.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use an <strong>AWS Direct Connect gateway</strong> to connect your AWS Direct Connect connection over a private virtual interface to one or more VPCs in your account that are located in the same or different regions. You associate a Direct Connect gateway with the virtual private gateway for the VPC, and then create a private virtual interface for your AWS Direct Connect connection to the Direct Connect gateway. You can attach multiple private virtual interfaces to your Direct Connect gateway. A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any public region and access it from all other public regions.</p><p><img src=\"https://media.tutorialsdojo.com/sap_directconnect_gateway.png\"></p><p>Hence, the correct answer is:<strong><em> </em>Set up an AWS Direct Connect Gateway with two virtual private gateways. Create the required Private Virtual Interfaces to the Direct Connect Gateway.</strong></p><p>The option that says: <strong>Establish a Direct Connect connection between the VPC in US East region and the on-premises data center in Texas, and then establish another Direct Connect connection between the VPC in US West region and the on-premises data center</strong> is incorrect because establishing two separate Direct Connect connections is expensive and hence, not a cost-effective option. It is better to establish a Direct Connect gateway instead which uses one Direct Connect connection to integrate the 2 VPCs and the on-premises data center.</p><p><strong>Setting up an AWS VPN managed connection between the VPC in US East region and the on-premises data center in Texas</strong> is incorrect because a VPN Connection is a more suitable solution for low to modest bandwidth requirements which can tolerate the inherent variability in Internet-based connectivity.</p><p><strong>Setting up two separate VPC peering connections for the two VPCs and for the on-premises data center</strong> is incorrect because VPC Peering is used to connect 2 VPC’s together and not to connect your on-premises data center.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways.html</a></p><p><a href=\"https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html\">https://docs.aws.amazon.com/directconnect/latest/UserGuide/virtualgateways.html</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p></div>"
	},
	{
		"question": "<p>A company runs a mission-critical application on a fixed set of Amazon EC2 instances behind an Application Load Balancer. The application responds to user requests by querying a 120GB dataset. The application requires high throughput and low latency storage so the dataset is stored on Provisioned IOPS (PIOPS) Amazon EBS volumes with 3000 IOPS provisioned. The Amazon EC2 launch template has been configured to allocate and attach this 120GB size PIOPS EBS volume for the fleet of EC2 instances. After a few months of operation, the company noticed the high cost of EBS volumes in the billing section. The Solutions Architect has been tasked to design a solution that will reduce the costs without a negative impact on the application performance and data durability.</p><p>Which of the following solutions will meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon EFS volume and mount it across all the Amazon EC2 instances. Use Max I/O performance mode on the EFS volume to ensure the application can reach the required IOPS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Remove the PIOPS EBS volume allocation on the EC2 launch template. Attach a 120GB instance store volume on the EC2 instance to ensure that the application will have enough IOPS for its operation.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon EFS volume and mount it across all the Amazon EC2 instances. Use the Provisioned Throughput mode on the EFS volume to ensure that the application can reach the required IOPS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the cheaper General Purpose SSD (gp2) EBS volumes instead of PIOPS EBS volumes. Allocating 1TB EBS volumes (gp2) will have a throughput of 3000 IOPS. Update the EC2 launch template to allocate this type of volume.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Elastic File System (Amazon EFS)</strong> provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth. Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily. The service manages all the file storage infrastructure for you, meaning that you can avoid the complexity of deploying, patching, and maintaining complex file system configurations.</p><p><img src=\"https://media.tutorialsdojo.com/sap_efs_how_it_works.png\"></p><p>Amazon EFS is designed to provide the throughput, IOPS, and low latency needed for a broad range of workloads. With Amazon EFS, you can choose from two performance modes and two throughput modes:</p><p>- The default<strong> General Purpose performance mode</strong> is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving. File systems in the <strong>Max I/O mode</strong> can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file metadata operations.</p><p>- Using the default <strong>Bursting Throughput mode</strong>, throughput scales as your file system grows. Using <strong>Provisioned Throughput mode</strong>, you can specify the throughput of your file system independent of the amount of data stored.</p><p>With Bursting Throughput mode, throughput on Amazon EFS scales as the size of your file system in the standard storage class grows. With Provisioned Throughput mode, you can instantly provision the throughput of your file system (in MiB/s) independent of the amount of data stored.</p><p>If your file system is in the Provisioned Throughput mode, you can increase the Provisioned Throughput of your file system as often as you want. You can decrease your file system throughput in Provisioned Throughput mode as long as it's been more than 24 hours since the last decrease.</p><p>Therefore, the correct answer is: <strong>Create an Amazon EFS volume and mount it across all the Amazon EC2 instances. Use the Provisioned Throughput mode on the EFS volume to ensure that the application can reach the required IOPS.</strong></p><p>The option that says: <strong>Create an Amazon EFS volume and mount it across all the Amazon EC2 instances. Use Max I/O performance mode on the EFS volume to ensure the application can reach the required IOPS</strong> is incorrect. File systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second. However, this scaling is done with a tradeoff of slightly higher latencies for file metadata operations.</p><p>The option that says: <strong>Use the cheaper General Purpose SSD (gp2) EBS volumes instead of PIOPS EBS volumes. Allocating 1TB EBS volumes (gp2) will have a throughput of 3000 IOPS. Update the EC2 launch template to allocate this type of volume</strong> is incorrect. Although this may look cheaper at first, creating several 1TB volumes for each EC2 instance entails higher costs. The Amazon EFS volume solution will be cheaper for sharing storage across all EC2 instances. Although you can use EBS Multi-Attach to attach EBS volumes to multiple EC2 instances, this is limited only to Provisioned IOPS SSD (io1 or io2) volumes that are attached to Nitro-based EC2 instances in the same Availability Zone.</p><p>The option that says: <strong>Remove the PIOPS EBS volume allocation on the EC2 launch template. Attach a 120GB instance store volume on the EC2 instance to ensure that the application will have enough IOPS for its operation</strong> is incorrect. Instance store volumes are ephemeral which means that you will lose all data in the volume when you stop/start the instance. This is not recommended for this mission-critical application.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html\">https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html</a></p><p><br></p><p><strong>Check out these Amazon EFS and Comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p></div>"
	},
	{
		"question": "<p>A leading blockchain company is getting ready to do a major announcement of their latest product next month on their public website which is hosted in AWS. It is running on an Auto Scaling group of Spot EC2 instances deployed across multiple Availability Zones with a MySQL RDS database instance. The website performs a high number of read operations to load the articles for their clients around the globe and a relatively low number of write operations to store the comments and inquiries of customers on their products. Before the major announcement, the solutions architect did some performance testing and found out that the database could not handle a surge of incoming requests.</p><p>Which of the following options are cost-effective and suitable options to solve the database performance issue? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Implement sharding to the RDS instances and set up database caching with Amazon Elasticsearch Service (Amazon ES).</p>"
			},
			{
				"correct": false,
				"answer": "Implement database caching with CloudFront. "
			},
			{
				"correct": true,
				"answer": "Launch a Read Replica in each Availability Zone."
			},
			{
				"correct": false,
				"answer": "Upgrade the RDS MySQL Instance size and increase the Provisioned IOPS for both read and write capacity."
			},
			{
				"correct": true,
				"answer": "Use Provisioned IOPS storage to improve the read performance of your database."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.</p><p>For production application that requires fast and consistent I/O performance, AWS recommends Provisioned IOPS (input/output operations per second) storage. Provisioned IOPS storage is a storage type that delivers predictable performance and consistently low latency. Provisioned IOPS storage is optimized for online transaction processing (OLTP) workloads that have consistent performance requirements. Provisioned IOPS helps performance tuning of these workloads.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_read_replica.png\"></p><p>The option that says: <strong>Use Provisioned IOPS storage to improve the read performance of your database</strong> is correct. This is a recommended option for increasing the read performance of your RDS databases.</p><p>The option that says: <strong>Launch a Read Replica in each Availability Zone </strong>is correct. This is a recommended practice to increase the availability of your read-replicas.</p><p>The option that says: <strong>Implement sharding to the RDS instances and setting up database caching with Amazon ElastiCache Service</strong> is incorrect. Sharding a database is not entirely supported in RDS. You should use Read Replicas instead.</p><p>The option that says: <strong>Upgrade the RDS MySQL Instance size and increasing the Provisioned IOPS for both read and write capacity</strong> is incorrect. Although upgrading an RDS instance will improve both the read and write capability of your database, this will entail significant costs compared with using Read Replicas. Hence, this is not cost-effective.</p><p>The option that says: <strong>Implement database caching with CloudFront</strong> is incorrect. You cannot use CloudFront for database caching. CloudFront is primarily used to securely deliver data, videos, applications, and APIs to customers globally with low latency and high transfer speeds.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/caching/database-caching/\">https://aws.amazon.com/caching/database-caching/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A company has released a new mobile game and its backend servers are hosted on the company’s on-premises data center. The game logic is exposed using REST APIs that have multiple functions depending on the user state. Access to the backend services is controlled with an API key, while any test traffic is distinguished by a different key. A central file server stores player session data. User traffic is variable throughout the day but the on-premises servers cannot handle traffic during peak hours. The game also has latency issues caused by the slow fetching of player session data. The management tasked the solutions architect to migrate this infrastructure to AWS in order to improve scalability and reduce the latency for data access while keeping the backend API model unchanged.</p><p>Which of the following is the recommended solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using Amazon API Gateway. Use Amazon DynamoDB with auto-scaling to store the player session data.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using AWS AppSync. Use Amazon Aurora Serverless to store the player session data.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a fleet of Amazon EC2 instances to host the backend services. Expose the REST APIs by placing the instances behind a Network Load Balancer (NLB). Use Amazon Aurora Serverless to store the player session data.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by placing the Lambda functions behind an Application Load Balancer (ALB). Use Amazon DynamoDB with auto-scaling to store the player session data.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Lambda</strong> is a compute service that lets you run code without provisioning or managing servers. Lambda runs your code only when needed and scales automatically, from a few requests per day to thousands per second. You pay only for the compute time that you consume—there is no charge when your code is not running. With Lambda, you can run code for virtually any type of application or backend service, all with zero administration.</p><p>You can create a web API with an HTTP endpoint for your Lambda function by using <strong>Amazon API Gateway</strong>. API Gateway provides tools for creating and documenting web APIs that route HTTP requests to Lambda functions. You can secure access to your API with authentication and authorization controls. Your APIs can serve traffic over the Internet or can be accessible only within your VPC. Amazon API Gateway invokes your function synchronously with an event that contains a JSON representation of the HTTP request. For custom integration, the event is the body of the request. For a proxy integration, the event has a defined structure.</p><p>API Gateway allows you to throttle traffic and authorize API calls to ensure that backend operations withstand traffic spikes and backend systems are not unnecessarily called.</p><p><img src=\"https://media.tutorialsdojo.com/sap_lambda_api_function.png\"></p><p>You can use <strong>Amazon DynamoDB</strong> for storing player session data. Gaming companies use Amazon DynamoDB in all parts of game platforms, including game state, player data, session history, and leaderboards. The main benefits that these companies get from DynamoDB are its ability to scale reliably to millions of concurrent users and requests while ensuring consistently low latency—measured in single-digit milliseconds. In addition, as a fully managed service, DynamoDB has no operational overhead. Game developers can focus on developing their games instead of managing databases. Also, as game makers are looking to expand from a single AWS Region to multiple Regions, they can rely on DynamoDB global tables for multi-region, active-active data replication.</p><p>Therefore, the correct answer is: <strong>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using Amazon API Gateway. Use Amazon DynamoDB with auto-scaling to store the player session data. </strong>API Gateway tightly integrates with AWS Lambda which enables you to quickly build REST compliant serverless applications. DynamoDB is well-suited for scalability to handle fast access to player session data.</p><p>The option that says: <strong>Create a fleet of Amazon EC2 instances to host the backend services. Expose the REST APIs by placing the instances behind a Network Load Balancer (NLB). Use Amazon Aurora Serverless to store the player session data</strong> is incorrect. Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. It is recommended to use Aurora Serverless for lightly-used applications, with peaks of 30 minutes to several hours a few times each day or several times per year, such as human resources, budgeting, or operational reporting application.</p><p>The option that says: <strong>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by placing the Lambda functions behind an Application Load Balancer (ALB). Use Amazon DynamoDB with auto-scaling to store the player session data</strong> is incorrect. Although you can use an ALB with Lambda as the backend service, it is still recommended to use API Gateway to expose your APIs. You have a lot more options and flexibility on API Gateway including the ability to use API keys and add custom headers to each request.</p><p>The option that says: <strong>Use AWS Lambda functions to run the backend game logic. Expose the REST APIs by using AWS AppSync. Use Amazon Aurora Serverless to store the player session data </strong>is incorrect. AWS AppSync is recommended for applications that are written for GraphQL APIs, not REST APIs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/\">https://aws.amazon.com/blogs/database/amazon-dynamodb-gaming-use-cases-and-design-patterns/</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html\">https://docs.aws.amazon.com/lambda/latest/dg/services-apigateway.html</a></p><p><a href=\"https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\">https://docs.aws.amazon.com/lambda/latest/dg/welcome.html</a></p><p><br></p><p><strong>Check out these Amazon API Gateway and Amazon DynamoDB Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-api-gateway/?src=udemy\">https://tutorialsdojo.com/amazon-api-gateway/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A call center company has recently adopted a hybrid architecture in which they need a predictable network performance and reduced bandwidth costs to connect their data center and their AWS Cloud. You have implemented two AWS Direct Connect connections between your data center and AWS to have a stable and highly available network performance. After a recent IT financial audit, it was decided to review the current implementation and replace it with a more cost-effective option.</p><p>Which of the following connectivity setup would you recommend for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Setup a Hardware VPN on your datacenter and set it to use the Direct Connect for its connection</p>"
			},
			{
				"correct": false,
				"answer": "<p>A single AWS Direct Connect connection and enable the built-in failover feature</p>"
			},
			{
				"correct": true,
				"answer": "<p>A single AWS Direct Connect and an AWS managed VPN connection to connect your data center with Amazon VPC</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS VPN CloudHub to connect your data center network to Amazon VPC</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The scenario requires you to revise the current setup of using two Direct Connect connections being used by the company. Since Direct Connect does not provide any redundancy for its connection, it is recommended to set up at least two connections for high availability. However, this setup is expensive as you will be charged for the two Direct Connect connections. Usually, the second connection is used only when the main connection fails which rarely happens.</p><p>To maintain high availability, but reduce the costs, you can use a single <strong><em>AWS Direct Connect</em></strong> to create a dedicated private connection from your data center network to your Amazon VPC, and then combine this connection with an <strong><em>AWS managed VPN connection</em></strong> to create an IPsec-encrypted connection as a backup connection. If the Direct Connect connection fails, you still have a managed VPN to connect to your Amazon VPC, albeit with a slower connection. This will suffice until your Direct Connect connection is restored.</p><p><img src=\"https://media.tutorialsdojo.com/sap_directconnect_vpn.jpg\"></p><p>Therefore, the correct answer is: <strong>A single AWS Direct Connect and an AWS managed VPN connection to connect your data center with Amazon VPC.</strong> The AWS Direct Connect connection will provide the high speed bandwidth network required while having the VPN as the slower, backup link in case the main Direct Connect link fails.</p><p>The option that says: <strong>A single AWS Direct Connect connection and enable the built-in failover feature</strong> is incorrect. AWS Direct Connect does not have a built-in failover feature.</p><p>The option that says: <strong>Setup a Hardware VPN on your datacenter and set it to use the Direct Connect for its connection</strong> is incorrect. If you implement this, your VPN will also fail if Direct Connect fails.</p><p>The option that says: <strong>Use AWS VPN CloudHub to connect your data center network to Amazon VPC</strong> is incorrect. You still need a dedicated high bandwidth network provided by Direct Connect.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpn-connections.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpn-connections.html</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/\">https://aws.amazon.com/premiumsupport/knowledge-center/configure-vpn-backup-dx/</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p><p><a href=\"https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/?src=udemy\">https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A company manually runs its custom scripts when deploying a new version of its application that is hosted on a fleet of Amazon EC2 instances. This method is prone to human errors such as accidentally running the wrong script or deploying the wrong artifact. The company wants to automate its deployment procedure and leverage its team’s knowledge in Chef deployment tools. The new version of the application must first be deployed on a staging environment for verification and testing. After passing the tests, it can then be deployed to the production environment. If errors are encountered after the deployment, the company wants to roll back to the older application version within five minutes.</p><p>Which of the following options should the Solutions Architect implement to meet the requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Utilize AWS CodeBuild and add a job with the Chef recipes for the new application version. Use a “canary” deployment strategy to the new version on a new instance. Delete the canary instance if errors are found on the new version.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an environment on AWS Elastic Beanstalk and deploy the application. For succeeding deployments, choose a “rolling update” strategy for fast deployment and easy rollback procedure in case of errors.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a stack on AWS OpsWorks and deploy the application. Clone this stack and deploy the new application version on it. Use a “blue/green” deployment strategy to shift traffic to the newer stack.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a new pipeline on AWS CodePipeline and add a stage that will deploy the application on the AWS EC2 instances. Choose a “rolling update with an additional batch” deployment strategy, to allow a quick rollback to the older version in case of errors.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS OpsWorks for Chef Automate</strong> provides a fully managed Chef Automate server and suite of automation tools that give you workflow automation for continuous deployment, automated testing for compliance and security, and a user interface that gives you visibility into your nodes and their status. The Chef Automate platform gives you full stack automation by handling operational tasks such as software and operating system configurations, continuous compliance, package installations, database setups, and more. The Chef server centrally stores your configuration tasks and provides them to each node in your compute environment at any scale, from a few nodes to thousands of nodes. OpsWorks for Chef Automate is completely compatible with tooling and cookbooks from the Chef community and automatically registers new nodes with your Chef server.</p><p><img src=\"https://media.tutorialsdojo.com/opsworks-green.png\"></p><p>You can implement a <strong>blue/green deployment strategy</strong> with OpsWorks stacks which allows you to run another version of your application – blue and green environment.</p><p>A <strong>blue-green deployment </strong>strategy is one common way to efficiently use separate stacks to deploy an application update to production. In a nutshell, you will clone your current OpsWorks stack and then deploy a new version on the cloned stack. Then you will use Amazon Route 53 to point the users to the new stack URL. Here’s the set up for a blue/green deployment on OpsWorks Stacks:</p><p>The blue environment is the production stack, which hosts the current application.</p><p>The green environment is the staging stack, which hosts the updated application.</p><p>When you are ready to deploy the updated app to production, you switch user traffic from the blue stack to the green stack, which becomes the new production stack. You then retire the old blue stack.</p><p>Therefore, the correct answer is: <strong>Create a stack on AWS OpsWorks and deploy the application. Clone this stack and deploy the new application version on it. Use a “blue/green” deployment strategy to shift traffic to the newer stack.</strong></p><p>The option that says: <strong>Create an environment on AWS Elastic Beanstalk and deploy the application. For succeeding deployments, choose a “rolling update” strategy for fast deployment and easy rollback procedure in case of errors</strong> is incorrect. Although this results in a fast deployment, the rollback procedure will approximately take the same time amount of time as the deployment because you will have to trigger a re-deployment of the old version. This approach doesn't leverage the team’s knowledge in Chef deployment tools.</p><p>The option that says: <strong>Create a new pipeline on AWS CodePipeline and add a stage that will deploy the application on the AWS EC2 instances. Choose a “rolling update with an additional batch” deployment strategy, to allow a quick rollback to the older version in case of errors</strong> is incorrect. Although the pipeline can deploy the new version on the EC2 instances, rollback for this strategy takes time. You will have to re-deploy the older version if you want to do a rollback.</p><p>The option that says: <strong>Utilize AWS CodeBuild and add a job with the Chef recipes for the new application version. Use a “canary” deployment strategy to the new version on a new instance. Delete the canary instance if errors are found on the new version</strong> is incorrect. Although you can detect errors on a canary deployment, AWS CodeBuild cannot deploy the new application version on the EC2 instances. You have to use AWS CodeDeploy if you want to go this route. It's also easier to set up Chef deployments using AWS OpsWorks rather than in AWS CodeBuild.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/opsworks/chefautomate/\">https://aws.amazon.com/opsworks/chefautomate/</a></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html#best-deploy-environments-blue-green\">https://docs.aws.amazon.com/opsworks/latest/userguide/best-deploy.html#best-deploy-environments-blue-green</a></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingapps-deploying.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingapps-deploying.html</a></p><p><br></p><p><strong>Check out the AWS OpsWorks Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-opsworks/?src=udemy\">https://tutorialsdojo.com/aws-opsworks/</a></p></div>"
	},
	{
		"question": "<p>A leading insurance company in South East Asia recently deployed a new web portal that enables your users to log in and manage their accounts, view their insurance plans, and pay their monthly premiums. After a few weeks, the solutions architect noticed that there is a significant amount of incoming traffic from a country in which the insurance company does not operate. Later on, it was determined that the same set of IP addresses coming from the unsupported country is sending out massive amounts of requests to your portal which has caused some performance issues on the application.</p><p>Which of the following options is the recommended solution to block the series of attacks coming from a set of determined IP ranges?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Create an inbound Network Access control list associated with explicit deny rules to block the attacking IP addresses."
			},
			{
				"correct": false,
				"answer": "Create a custom route table associated with the web tier and block the attacking IP addresses from the Internet Gateway."
			},
			{
				"correct": false,
				"answer": "Launch a Security Group with explicit deny rules to block the attacking IP addresses. "
			},
			{
				"correct": false,
				"answer": "Launch the online portal on the private subnet."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets. You might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.</p><p>The following are the basic things that you need to know about network ACLs:</p><p>- Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</p><p>- You can create a custom network ACL and associate it with a subnet. By default, each custom network ACL denies all inbound and outbound traffic until you add rules.</p><p>- Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p><p>- You can associate a network ACL with multiple subnets. However, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</p><p>- A network ACL contains a numbered list of rules. We evaluate the rules in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. The highest number that you can use for a rule is 32766. We recommend that you start by creating rules in increments (for example, increments of 10 or 100) so that you can insert new rules where you need to later on.</p><p>- A network ACL has separate inbound and outbound rules, and each rule can either allow or deny traffic.</p><p>- Network ACLs are stateless, which means that responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa).</p><p>The required task is to block all of the offending IP addresses from the unsupported country. In order to do this, you have to use Network Access Control List (NACL).</p><p>Therefore, the correct answer is: <strong>Create an inbound Network Access control list associated with explicit deny rules to block the attacking IP addresses.</strong></p><p>The option that says: <strong>Launch the online portal on the private subnet</strong> is incorrect because if you launch the online portal on the private subnet, then it will not be accessible to the users over the public Internet anymore.</p><p>The option that says: <strong>Create a custom route table associated with the web tier and blocking the attacking IP addresses from the Internet Gateway</strong> is incorrect because you do not have to change the route table nor make any changes to the Internet Gateway (IGW).</p><p>The option that says: <strong>Launch a Security Group with explicit deny rules to block the attacking IP addresses</strong> is incorrect because you cannot explicitly deny or block IP addresses using a security group.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\">http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html</a></p><p><a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html</a></p><p><br></p><p><strong>Check out this Amazon VPC Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-vpc/?src=udemy\">https://tutorialsdojo.com/amazon-vpc/</a></p><p><br></p><p><strong>Check out this security group and network access list comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/security-group-vs-nacl/?src=udemy\">https://tutorialsdojo.com/security-group-vs-nacl/</a></p></div>"
	},
	{
		"question": "<p>A company requires regular processing of a massive amount of product catalogs that need to be handled per batch. The data are sent out to be processed by on-demand Workers using the Amazon Mechanical Turk crowdsourcing marketplace on a regular basis. The company instructed its solutions srchitect to design a workflow orchestration system that will enable them to reprocess failures, manage the result assessment process, and handle multiple concurrent Mechanical Turk operations.</p><p>What is the MOST suitable solution that the solutions architect should implement in order to manage the state of every workflow?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use Amazon SWF to set up a batch process workflow that handles the processing for a single batch. Develop worker jobs that extract the data, transform it, and send it through Amazon Mechanical Turk. Configure the batch process workflow to output the acceptable Human Intelligence Tasks (HITs) and the failed ones.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a workflow using AWS Config and AWS Step Functions in order to orchestrate multiple concurrent workflows. Visualize the status of each workflow using the AWS Management Console. Store the historical data to an S3 bucket and visualize the data using Amazon QuickSight.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon MQ to set up a batch process workflow that handles the processing for a single batch. Develop worker jobs using AWS Lambda that extract the data, transform it, and send it through Amazon Mechanical Turk. Configure the batch process workflow to output the acceptable Human Intelligence Tasks (HITs) and the failed ones.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Store workflow data in an Amazon RDS with AWS Lambda functions polling the RDS database instance for status changes. Set up worker Lambda functions to process the next workflow steps then use Amazon QuickSight to visualize workflow states directly out of Amazon RDS.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Mechanical Turk (MTurk)</strong> is a crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. This could include anything from conducting simple data validation and research to more subjective tasks like survey participation, content moderation, and more. MTurk enables companies to harness the collective intelligence, skills, and insights from a global workforce to streamline business processes, augment data collection and analysis, and accelerate machine learning development.</p><p><img src=\"https://media.tutorialsdojo.com/sap_mturk_marketplace.JPG\"></p><p>You can process large product catalogs using Amazon Mechanical Turk. While validating data in large catalogs, the products in the catalog are processed in batches. Different batches can be processed concurrently. For each batch, the product data is extracted from servers in the datacenter and transformed into CSV (Comma Separated Values) files required by Amazon Mechanical Turk’s Requester User Interface (RUI). The CSV is uploaded to populate and run the HITs (Human Intelligence Tasks). When HITs complete, the resulting CSV file is reverse transformed to get the data back into the original format. The results are then assessed and Amazon Mechanical Turk workers are paid for acceptable results. Failures are weeded out and reprocessed, while the acceptable HIT results are used to update the catalog. As batches are processed, the system needs to track the quality of the Amazon Mechanical Turk workers and adjust the payments accordingly. Failed HITs are re-batched and sent through the pipeline again.</p><p>You can integrate Amazon Mechanical Turk and SWF to implement a set of workflows for batch processing. A BatchProcess workflow handles the processing for a single batch. It has workers that extract the data, transform it, and send it through Amazon Mechanical Turk. The BatchProcess workflow outputs the acceptable HITs and the failed ones. This is used as the input for three other workflows: MTurkManager, UpdateCatalogWorkflow, and RerunProducts.</p><p>The MTurkManager workflow makes payments for acceptable HITs, responds to the human workers who produced failed HITs, and updates its own database for tracking results quality. The UpdateCatalogWorkflow updates the master catalog based on acceptable HITs. The RerunProducts workflow waits until there is a large enough batch of products with failed HITs. It then creates a batch and sends it back to the BatchProcess workflow. The entire end-to-end catalog processing is performed by a CleanupCatalog workflow that initiates child executions of the above workflows. Having a system of well-defined workflows enables this use case to be architected, audited, and run systematically for catalogs with several million products.</p><p>You can configure Amazon Mechanical Turk to notify you whenever certain events occur during the life cycle of Human Intelligence Task (HIT). Mechanical Turk can send you a notification message when a Worker accepts, abandons, returns, or submits an assignment, when a HIT becomes \"reviewable\", or when a HIT expires, for any HIT of a given HIT type. Amazon Mechanical Turk can send a notification to an Amazon Simple Queue Service (Amazon SQS) queue or to an Amazon Simple Notification Service (Amazon SNS) topic.</p><p>Hence, the correct answer in this scenario is the option that says: <strong>Use Amazon SWF to set up a batch process workflow that handles the processing for a single batch. Develop worker jobs that extract the data, transform it and send it through Amazon Mechanical Turk. Configure the batch process workflow to output the acceptable Human Intelligence Tasks (HITs) and the failed ones.</strong></p><p>The option that says: <strong>Store workflow data in an Amazon RDS with AWS Lambda functions polling the RDS database instance for status changes. Set up worker Lambda functions to process the next workflow steps then use Amazon QuickSight to visualize workflow states directly out of Amazon RDS </strong>is incorrect because the AWS Mechanical Turk service does not have direct integration with RDS, which means that you have to manually fetch data and send it to RDS which would be inconvenient. Storing workflow data in an Amazon RDS, with AWS Lambda functions polling the RDS database instance for status changes, requires a lot of configuration instead of utilizing the messages being sent by Mechanical Turk to Amazon SQS.</p><p>The option that says: <strong>Set up a workflow using AWS Config and AWS Step Functions in order to orchestrate multiple concurrent workflows. Visualize the status of each workflow using the AWS Management Console. Store the historical data to an S3 bucket and visualize the data using Amazon QuickSight</strong> is incorrect because there is currently no way to directly to integrate AWS Config and AWS Step Functions with Mechanical Turk in order to orchestrate multiple concurrent workflows. Moreover, visualizing the status of each workflow using the AWS Management Console is inappropriate since it is not a visualization tool unlike Kibana or Amazon QuickSight.</p><p>The option that says: <strong>Use Amazon MQ to set up a batch process workflow that handles the processing for a single batch. Develop worker jobs using AWS Lambda that extract the data, transform it and send it through Amazon Mechanical Turk. Configure the batch process workflow to output the acceptable Human Intelligence Tasks (HITs) and the failed ones</strong> is incorrect because Amazon MQ is not a suitable service to handle a batch process workflow. A better solution is to use Amazon SWF for batch processing of the product catalog.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_NotificationReceptorAPIArticle.html\">https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI/ApiReference_NotificationReceptorAPIArticle.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SvcIntro.html\">https://docs.aws.amazon.com/AWSMechTurk/latest/AWSMechanicalTurkGettingStartedGuide/SvcIntro.html</a></p><p><a href=\"https://aws.amazon.com/swf/faqs/\">https://aws.amazon.com/swf/faqs/</a></p><p><br></p><p><strong>Check out this Amazon Mechanical Turk Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-mechanical-turk/?src=udemy\">https://tutorialsdojo.com/amazon-mechanical-turk/</a></p></div>"
	},
	{
		"question": "<p>A technology company is developing an educational mobile app for students, with an exam feature that also allows them to submit their answers. The developers used React Native so the app can be deployed on both iOS and Android devices. They used AWS Lambda and Amazon API Gateway for the backend services and a DynamoDB table as the database service. After a month, the released app has been downloaded over 3 million times. However, there are a lot of users who complain about the slow processing of the app especially when they are submitting their answers in the multiple-choice exams. The diagrams and images on the exam also take a lot of time to load, which is not a good user experience.</p><p>Which of the following options provides the most cost-effective and scalable architecture for the application?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Launch an SQS queue and develop a custom service which integrates with SQS to buffer the incoming requests. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app.</p>"
			},
			{
				"correct": false,
				"answer": "Enable Auto Scaling in DynamoDB with a Target Utilization of 100% and a maximum provisioned capacity of 1000 units. Use an S3 bucket to host the diagrams, images, and other static assets of the mobile app."
			},
			{
				"correct": false,
				"answer": "Increase the write capacity in DynamoDB. Use an RTMP (Real-Time Messaging Protocol) distribution in CloudFront to host the diagrams, images and other static assets of the mobile app in real time."
			},
			{
				"correct": false,
				"answer": "<p>Instead of DynamoDB, use RDS Multi-AZ configuration with Read Replicas. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the mobile app is both write and read-intensive. You can use SQS to buffer and scale the backend service to accommodate the large incoming traffic. CloudFront is the best choice to distribute the static assets of the mobile app to load it faster.</p><p>SQS can help with the slowness experienced when submitting answers. You can modify your application to use SQS. For example, when the users submit their answers, instead of waiting for the complete submission and processing of all the answers you can save the submitted answers on the SQS Queue and then quickly return the user to a new page stating that the answers are successfully submitted. This improves the user experience as the user will not wait on a \"loading submit\" button until all the answers are processed completely. When the answers are buffered in the queue, the backend instances can now process the answers in the queue. You can then have another webpage that will show the results separately. This is a simple cost-effective design that improves user experience without increasing the capacity of your systems.</p><p>With CloudFront, the images on the S3 bucket will be cached on Edge locations close to users. This further improves the load times of the web page. Images are quickly accessed and loaded because they are physically closer (using CloudFront cache) to users around the world.</p><p>Therefor the correct answer is: <strong>Launch an SQS queue and develop a custom service which integrates with SQS to buffer the incoming requests. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app.</strong></p><p>The option that says: <strong>Enable Auto Scaling in DynamoDB with a Target Utilization of 100% and a maximum provisioned capacity of 1000 units. Use an S3 bucket to host the diagrams, images, and other static assets of the mobile app</strong> is incorrect because you can only set the auto scaling target utilization values between 20 and 90 percent for your read and write capacity, not 100%.</p><p>The option that says: <strong>Increase the write capacity in DynamoDB. Use an RTMP (Real-Time Messaging Protocol) distribution in CloudFront to host the diagrams, images and other static assets of the mobile app in real-time</strong> is incorrect because RTMP is primarily used for video streaming, not for static assets such as images.</p><p>The options that says: <strong>Instead of DynamoDB, use RDS Multi-AZ configuration with Read Replicas. Use a web distribution in CloudFront and Amazon S3 to host the diagrams, images, and other static assets of the mobile app</strong> is incorrect because this will require changing the whole application to use an SQL database and moving from DynamoDB to RDS Multi-AZ is not cost-effective.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/sqs/\">https://aws.amazon.com/sqs/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/\">https://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
	},
	{
		"question": "<p>A privately funded aerospace and sub-orbital spaceflight services company hosts its rapidly evolving applications in AWS. For its deployment process, the company is using CloudFormation templates which are regularly updated to map the latest AMI IDs for its Amazon EC2 instances clusters. It takes a lot of time to execute this on a regular basis which is why the solutions architect has been instructed to automate this process.</p><p>Which of the following options is the most suitable solution that can satisfy the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use a combination of AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure your Systems Manager State Manager to store the latest AMI IDs and integrate them with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use the existing <strong>Parameters</strong> section of your <strong>CloudFormation template</strong> to define Systems Manager parameters, along with other parameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.</p><p>If the parameter being referenced in the template does not exist in Systems Manager, a synchronous validation error is thrown. Also, if you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed against SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudformation_parameter_store.png\"></p><p>Parameters stored in Systems Manager are mutable. Any time you use a template containing Systems Manager parameters to create/update your stacks, CloudFormation uses the values for these Systems Manager parameters at the time of the create/update operation. So, as parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation. The <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_Parameter.html\">Parameters</a> section in the output for Describe API will show an additional ‘ResolvedValue’ field that contains the resolved value of the Systems Manager parameter that was used for the last stack operation.</p><p>Hence, the correct answer is the option that says: <strong>Use CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you decide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.</strong></p><p>The option that says: <strong>Configure your Systems Manager State Manager to store the latest AMI IDs and integrate them with your CloudFormation template. Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation template</strong> is incorrect because the Systems Manager State Manager service simply automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define. This can't be used as a parameter store that refers to the latest AMI of your application.</p><p>The following options are incorrect because using AWS Service Catalog is not suitable in this scenario. This service just allows organizations to create and manage catalogs of IT services that are approved for use on AWS:</p><p><strong>- Use a combination of AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.</strong></p><p><strong>- Use CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding deployments.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/\">https://aws.amazon.com/blogs/mt/integrating-aws-cloudformation-with-aws-systems-manager-parameter-store/</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A media company recently launched a web service that allows users to upload and share short videos. Currently, the web servers are hosted on an Auto Scaling group of Amazon EC2 instances in which the videos are stored in the EBS volumes. Each uploaded video sends a message on the Amazon SQS queue, which is also processed by an Auto Scaling group of Amazon EC2 instances. The company relies on third-party software to analyze and categorize the videos. The website also contains static content that has variable user traffic. The company wants to re-architecture the application to reduce costs, reduce dependency on third-party software, and reduce management overhead by leveraging AWS-managed services.</p><p>Which of the following solutions will meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon S3 bucket with website hosting enabled to host the web application. Store the videos and static content on a separate S3 bucket. Configure S3 event notification to send messages to an Amazon SQS queue for each video upload event. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon ECS Fargate cluster and use containers to host the web application. Create an Auto Scaling group of Amazon EC2 Spot instances to process the SQS queue. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon EFS volume to store the videos and static content. Mount the volume on all EC2 instances of the web application. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Reduce operational overhead by using AWS Elastic Beanstalk to provision the Auto Scaling group of EC2 instances for the web servers and the Amazon SQS queue consumers. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Fargate</strong> is a technology that you can use with Amazon ECS to run containers without having to manage servers or clusters of Amazon EC2 instances. With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to choose server types, decide when to scale your clusters, or optimize cluster packing. Fargate makes it easy for you to focus on building your applications.</p><p>When you run your tasks and services with the Fargate launch type, you package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application.</p><p><strong>Amazon Rekognition</strong> makes it easy to add image and video analysis to your applications. You just provide an image or video to the Amazon Rekognition API, and the service can identify objects, people, text, scenes, and activities. It can detect any inappropriate content as well. Amazon Rekognition also provides highly accurate facial analysis, face comparison, and face search capabilities. You can detect, analyze, and compare faces for a wide variety of use cases, including user verification, cataloging, people counting, and public safety.</p><p>Amazon Rekognition is based on the same proven, highly scalable, deep learning technology developed by Amazon’s computer vision scientists to analyze billions of images and videos daily. It requires no machine learning expertise to use. Amazon Rekognition includes a simple, easy-to-use API that can quickly analyze any image or video file that’s stored in Amazon S3.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_rekognition_screen.png\"></p><p>You can use <strong>Amazon S3</strong> to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts. By contrast, a dynamic website relies on server-side processing, including server-side scripts such as PHP, JSP, or ASP.NET. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites. This makes Amazon S3 suitable for hosting static website contents.</p><p>A <strong>Spot Instance</strong> is an unused EC2 instance that is available for less than the On-Demand price. Because Spot Instances enable you to request unused EC2 instances at steep discounts, you can lower your Amazon EC2 costs significantly. The hourly price for a Spot Instance is called a Spot price. The Spot price of each instance type in each Availability Zone is set by Amazon EC2, and is adjusted gradually based on the long-term supply of and demand for Spot Instances. Your Spot Instance runs whenever capacity is available and the maximum price per hour for your request exceeds the Spot price.</p><p>Spot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. For example, Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks.</p><p>Therefore, the correct answer is: <strong>Create an Amazon ECS Fargate cluster and use containers to host the web application. Create an Auto Scaling group of Amazon EC2 Spot instances to process the SQS queue. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets. </strong>Using an ECS Fargate cluster reduces the overhead of managing EC2 instances and using Spot instances to process the SQS queue significantly reduces the operating costs. Amazon Rekognition is designed to analyze videos which you can use to categorize them. Amazon S3 bucket serves as durable and cost-effective storage for the uploaded videos.</p><p>The option that says: <strong>Create an Amazon EFS volume to store the videos and static content. Mount the volume on all EC2 instances of the web application. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos</strong> is incorrect. Amazon S3 is more cost-effective than using EFS. This also increases management overhead as you need to configure the EFS mount point on all your EC2 instances.</p><p>The option that says: <strong>Create an Amazon S3 bucket with website hosting enabled to host the web application. Store the videos and static content on a separate S3 bucket. Configure S3 event notification to send messages to an Amazon SQS queue for each video upload event. Have AWS Lambda poll the Amazon SQS queue for messages and invoke a Lambda function that calls the Amazon Rekognition API to analyze and categorize the videos</strong> is incorrect. There is no mention in the question regarding the nature of the application but we can safely assume that this website is a dynamic website that allows users to log in and upload videos. We can't safely host a dynamic website on an Amazon S3 bucket.</p><p>The option that says: <strong>Reduce operational overhead by using AWS Elastic Beanstalk to provision the Auto Scaling group of EC2 instances for the web servers and the Amazon SQS queue consumers. Use Amazon Rekognition to analyze and categorize the videos instead of the third-party software. Store the videos and static contents on Amazon S3 buckets</strong> is incorrect. Although Elastic Beanstalk can manage the EC2 instances for you, there is no mention to use Spot instances for processing the SQS queue which can further reduce the operating costs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\">https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html</a></p><p><a href=\"https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html\">https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html</a></p><p><br></p><p><strong>Check out these AWS Fargate and Amazon Rekognition Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-fargate/?src=udemy\">https://tutorialsdojo.com/aws-fargate/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-rekognition/?src=udemy\">https://tutorialsdojo.com/amazon-rekognition/</a></p></div>"
	},
	{
		"question": "<p>A company has an on-premises data center that is hosting its gaming service. Its primary function is player-matching and is accessible from players around the world. The gaming service prioritizes network speed for the users so all traffic to the servers uses User Datagram Protocol (UDP). As more players join, the company is having difficulty scaling its infrastructure so it plans to migrate the service to the AWS cloud. The Solutions Architect has been tasked with the migration and AWS Shield Advanced has been enabled already to protect all public-facing resources.</p><p>Which of the following actions should the Solutions Architect implement to achieve the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Place the Auto Scaling of Amazon EC2 instances behind an Internet-facing Application Load Balancer (ALB). For the domain name, create an Amazon Route 53 entry that is Aliased to the FQDN of the ALB.</p><p><br></p>"
			},
			{
				"correct": true,
				"answer": "<p>Place the Auto Scaling of Amazon EC2 instances behind a Network Load Balancer (NLB). For the domain name, create an Amazon Route 53 entry that points to the Elastic IP address of the NLB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon CloudFront distribution and set the Load Balancer as the origin. Use only secure protocols on the distribution origin settings.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up network ACL rules on the VPC to deny all non-UDP traffic. Ensure that the NACL is associated with the load balancer subnets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an AWS WAF rule that will explicitly block all non-UDP traffic. Ensure that the AWS WAF rule is associated with the load balancer of the EC2 instances.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A<strong> Network Load Balancer</strong> operates at the connection level (Layer 4), routing connections to targets (Amazon EC2 instances, microservices, and containers) within Amazon VPC, based on IP protocol data. Ideal for load balancing of both TCP and UDP traffic, Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is optimized to handle sudden and volatile traffic patterns while using a single static IP address per Availability Zone.</p><p>For UDP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have a different source IP addresses and ports, so they can be routed to different targets.</p><p>When you create <strong>Network Access Control Lists (NACLs)</strong>, you can specify both allow and deny rules. This is useful if you want to explicitly deny certain types of traffic to your application. For example, you can define IP addresses (as CIDR ranges), protocols, and destination ports that are denied access to the entire subnet. If your application is used only for TCP traffic, you can create a rule to deny all UDP traffic or vice versa. This option is useful when responding to DDoS attacks because it lets you create your own rules to mitigate the attack when you know the source IPs or other signatures.</p><p>If you are subscribed to AWS Shield Advanced, you can register Elastic IPs (EIPs) as Protected Resources. DDoS attacks against EIPs that have been registered as Protected Resources are detected more quickly, which can result in a faster time to mitigate.</p><p>The option that says: <strong>Place the Auto Scaling of Amazon EC2 instances behind a Network Load Balancer (NLB). For the domain name, create an Amazon Route 53 entry that points to the Elastic IP address of the NLB</strong> is correct. The service uses UDP traffic and prioritizes network speed so a Network Load Balancer is ideal for this scenario. You can create a Route 53 entry pointing to the NLB’s Elastic IP.</p><p>The option that says: <strong>Set up network ACL rules on the VPC to deny all non-UDP traffic. Ensure that the NACL is associated with the load balancer subnets</strong> is correct. Since all traffic to the servers is UDP, you can set NACL to block all non-UDP traffic which can help block attacks such as traffic coming from TCP connections.</p><p>The option that says: <strong>Place the Auto Scaling of Amazon EC2 instances behind an internet-facing Application Load Balancer (ALB). For the domain name, create an Amazon Route 53 entry that is Aliased to the FQDN of the ALB </strong>is incorrect. UDP traffic operates at Layer 4 of the OSI model while an ALB operates at Layer 7. A Network Load Balancer is a better fit for this scenario.</p><p>The option that says: <strong>Create an AWS WAF rule that will explicitly block all non-UDP traffic. Ensure that the AWS WAF rule is associated with the load balancer of the EC2 instances</strong> is incorrect. AWS WAF rules cannot protect a Network Load Balancer yet. It is better to use NACL rules to block the non-UDP traffic.</p><p>The option that says: <strong>Create an Amazon CloudFront distribution and set the Load Balancer as the origin. Use only secure protocols on the distribution origin settings</strong> is incorrect. Although CloudFront does provide caching and security settings for the origin, it does not block DDoS attacks to your instances or NLBs. AWS Shield Advanced offers integration with NACLs that will block traffic at the edge of the AWS Network.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html\">https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/security-groups-and-network-access-control-lists-nacls-bp5.html</a></p><p><a href=\"https://aws.amazon.com/blogs/security/now-you-can-use-aws-shield-advanced-to-protect-your-amazon-ec2-instances-and-network-load-balancers/\">https://aws.amazon.com/blogs/security/now-you-can-use-aws-shield-advanced-to-protect-your-amazon-ec2-instances-and-network-load-balancers/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html</a></p><p><br></p><p><strong>Check out these Application Load Balancer and AWS Shield Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/?src=udemy\">https://tutorialsdojo.com/application-load-balancer-vs-network-load-balancer-vs-classic-load-balancer/</a></p><p><a href=\"https://tutorialsdojo.com/aws-shield/?src=udemy\">https://tutorialsdojo.com/aws-shield/</a></p></div>"
	},
	{
		"question": "<p>A company has data centers in Europe, Asia, and North America regions. Each data center has a 10Gbps Direct Connect connection to AWS, and the company uses a custom VPN to encrypt traffic between its data center network and AWS. In total, the data centers have about five hundred physical servers that host a mix of Windows and Linux-based applications and database services. The company plans to decommission these data centers and migrate its entire infrastructure to the AWS cloud instead. Separate accounts for staging and launching VMs must be implemented, as well as the ability to do AWS Region to Region VPC stack creation.</p><p>Which of the following options is the recommended solution for this migration?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Leverage AWS Application Migration Service (AWS MGN) for the migration. Install the AWS Replication agent on each physical machine and start the replication on the AWS Management Console to create AMIs of your servers. After completion, launch your AMIs as Amazon EC2 instances.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Leverage the CloudEndure Migration service from AWS. Install the CloudEndure Migration agent on each physical machine and configure a machine blueprint on the CloudEndure Console to start replication. After the replication is completed, start the cutover to AWS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Leverage the Application Discovery Service from AWS. Install the Application Discovery Service agents on each physical server and visualize the infrastructure on the AWS Migration Hub console. Trigger the replication to copy your servers to AWS. After the replication is completed, start the cutover to AWS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Leverage AWS DataSync for the migration. Install the AWS DataSync agent on each physical machine and start the replication on the AWS Management Console to copy your server images to an Amazon S3 bucket as AMIs. After completion, select the AMIs and launch them as Amazon EC2 instances.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>CloudEndure Migration</strong> is a block-level replication tool that simplifies the process of migrating applications from physical, virtual, and cloud-based servers to AWS. CloudEndure Migration supports any source infrastructure as long as it runs on x86 operating systems supported by Amazon Elastic Cloud Compute (EC2). This includes physical servers, P2V (virtual servers converted from physical), VMware, Hyper-V, and other cloud providers like Azure, GCP, IBM, or Oracle.</p><p>If your source environment includes bare metal servers, and you can install agents (more on agents in the next section), the recommendation is to use CloudEndure Migration.</p><p>You can use <strong>CloudEndure Migration</strong> to quickly lift-and-shift physical, virtual, or cloud servers without compatibility issues, performance impact, or long cutover windows. CloudEndure Migration continuously replicates your source servers to your AWS account. Then, when you’re ready to migrate, it automatically converts and launches your servers on AWS so you can quickly benefit from the cost savings, productivity, resilience, and agility of the AWS Cloud.</p><p>Once your applications are running on AWS, you can leverage AWS services and capabilities to quickly and easily re-platform or refactor these applications – which makes lift-and-shift a fast route to modernization.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudendure_overview.png\"></p><p>Therefore, the correct answer is: <strong>Leverage the CloudEndure Migration service from AWS. Install the CloudEndure Migration agent on each physical machine and configure a machine blueprint on the CloudEndure Console to start replication. After the replication is completed, start the cutover to AWS.</strong> CloudEndure Migration allows you quickly lift-and-shift physical, virtual, or cloud servers to AWS. When you’re ready to migrate, it automatically converts and launches your servers on AWS.</p><p>The option that says: <strong>Leverage AWS Application Migration Service (AWS MGN) for the migration. Install the AWS Replication agent on each physical machine and start the replication on the AWS Management Console to create AMIs of your servers. After completion, launch your AMIs as Amazon EC2 instances</strong> is incorrect. AWS MGN does not support separate accounts for staging and launching VMs. The ability to do AWS Region to Region VPC stack creation is not available as well.</p><p>The option that says: <strong>Leverage AWS DataSync for the migration. Install the AWS DataSync agent on each physical machine and start the replication on the AWS Management Console to copy your server images to an Amazon S3 bucket as AMIs. After completion, select the AMIs and launch them as Amazon EC2 instances</strong> is incorrect. AWS DataSync is designed for copying data between Network File System (NFS) shares, Server Message Block (SMB) shares, self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, and Amazon FSx for Windows File Server file systems. It is not designed for physical server migrations to AWS.</p><p>The option that says: <strong>Leverage the Application Discovery Service from AWS. Install the Application Discovery Service agents on each physical server and visualize the infrastructure on the AWS Migration Hub console. Trigger the replication to copy your servers to AWS. After the replication is completed, start the cutover to AWS</strong> is incorrect. AWS Application Discovery Service only gathers data for migration. It helps customers plan migration projects by gathering information about their on-premises data centers. It does not perform the actual migration of physical servers to AWS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/application-migration-service/when-to-choose-aws-mgn/\">https://aws.amazon.com/application-migration-service/when-to-choose-aws-mgn/</a></p><p><a href=\"https://aws.amazon.com/cloudendure-migration/\">https://aws.amazon.com/cloudendure-migration/</a></p><p><a href=\"https://aws.amazon.com/datasync/features/\">https://aws.amazon.com/datasync/features/</a></p><p><br></p><p><strong>Check out these AWS DataSync and AWS Server Migration Service Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-datasync/?src=udemy\">https://tutorialsdojo.com/aws-datasync/</a></p><p><a href=\"https://tutorialsdojo.com/aws-server-migration-service-sms/?src=udemy\">https://tutorialsdojo.com/aws-server-migration-service-sms/</a></p></div>"
	},
	{
		"question": "<p>A company runs hundreds of Amazon EC2 instances inside a VPC. Whenever an EC2 error is encountered, the solutions architect performs manual steps in order to regain access to the impaired instance. The management wants to automatically recover impaired EC2 instances in the VPC. The goal is to automatically fix an instance that has become unreachable due to network misconfigurations, RDP issues, firewall settings, and many others to meet the compliance requirements.</p><p>Which of the following options is the most suitable solution that the solutions architect should implement to meet the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager State Manager to self-diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Automate the recovery process by using AWS Systems Manager Maintenance Windows.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the EC2Rescue tool to diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Run the tool automatically by using the Systems Manager Automation and the <code>AWSSupport-ExecuteEC2Rescue</code> document.</p>"
			},
			{
				"correct": false,
				"answer": "<p>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager Session Manager to self-diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Automate the recovery process by setting up a monitoring system using CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the EC2Rescue tool to diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Run the tool automatically by using AWS OpsWorks Chef Automate and the <code>AWSSupport-ExecuteEC2Rescue</code> document.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>EC2Rescue</strong> can help you diagnose and troubleshoot problems on Amazon EC2 Linux and Windows Server instances. You can run the tool manually or you can run the tool automatically by using Systems Manager Automation and the <em>AWSSupport-ExecuteEC2Rescue</em> document. The <em>AWSSupport-ExecuteEC2Rescue </em>document is designed to perform a combination of Systems Manager actions, AWS CloudFormation actions, and Lambda functions that automate the steps normally required to use EC2Rescue.</p><p><strong>Systems Manager Automation</strong> simplifies common maintenance and deployment tasks of Amazon EC2 instances and other AWS resources. Automation enables you to do the following:</p><p>- Build Automation workflows to configure and manage instances and AWS resources.</p><p>- Create custom workflows or use pre-defined workflows maintained by AWS.</p><p>- Receive notifications about Automation tasks and workflows by using Amazon CloudWatch Events.</p><p>- Monitor Automation progress and execution details by using the Amazon EC2 or the AWS Systems Manager console.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_ssm_overview.png\"></p><p>Therefore, the correct answer is: <strong>Use the EC2Rescue tool to diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Run the tool automatically by using the Systems Manager Automation and the </strong><code><strong>AWSSupport-ExecuteEC2Rescue</strong></code><strong> document.</strong></p><p>The option that says: <strong>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager State Manager to self-diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Automate the recovery process by using AWS Systems Manager Maintenance Windows</strong> is incorrect. AWS Config is a service which is primarily used to assess, audit, and evaluate the configurations of your AWS resources but not to diagnose and troubleshoot problems in your EC2 instances. In addition, AWS Systems Manager State Manager is primarily used as a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define but does not help you in troubleshooting your EC2 instances.</p><p>The option that says: <strong>To meet the compliance requirements, use a combination of AWS Config and the AWS Systems Manager Session Manager to self-diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Automate the recovery process by setting up a monitoring system using CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances</strong> is incorrect. Just like the other option, AWS Config does not help you in troubleshooting the problems in your EC2 instances. The AWS Systems Manager Sessions Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys for your EC2 instances but it does not provide the capability of helping you diagnose and troubleshoot problems in your instance like what the EC2Rescue tool can do. In addition, setting up a CloudWatch, AWS Lambda, and the AWS Systems Manager Run Command that will automatically monitor and recover impaired EC2 instances is an operational overhead which can be easily done by using AWS Systems Manager Automation.</p><p>The option that says: <strong>Use the EC2Rescue tool to diagnose and troubleshoot problems on your Amazon EC2 Linux and Windows Server instances. Run the tool automatically by using AWS OpsWorks Chef Automate and the </strong><code><strong>AWSSupport-ExecuteEC2Rescue</strong></code><strong> document</strong> is incorrect. Chef Automate is a suite of automation tools from Chef which is mainly used for configuration management, compliance and security, and continuous deployment.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/automation-ec2rescue.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-automation.html</a></p><p><br></p><p><strong>Check out this AWS Systems Manager Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p></div>"
	},
	{
		"question": "<p>A company deployed a blockchain application in AWS a year ago using AWS Opsworks. There has been a lot of security patches lately for the underlying Linux servers of the blockchain application, which means that the Opsworks stack instances should be updated.</p><p>In this scenario, which of the following are the best practices when updating an AWS stack? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Delete the entire stack and create a new one."
			},
			{
				"correct": false,
				"answer": "Use CloudFormation to deploy the security patches."
			},
			{
				"correct": false,
				"answer": "On Windows-based instances, run the Update Dependencies stack command."
			},
			{
				"correct": true,
				"answer": "Run the Update Dependencies stack command for Linux based instances."
			},
			{
				"correct": true,
				"answer": "Create and start new instances to replace your current online instances. Then delete the current instances. The new instances will have the latest set of security patches installed during setup."
			},
			{
				"correct": false,
				"answer": "Use WAF to deploy the security patches."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Linux operating system providers supply regular updates, most of which are operating system security patches but can also include updates to installed packages. You should ensure that your instances' operating systems are current with the latest security patches.</p><p><img src=\"https://media.tutorialsdojo.com/sap_php_chef_cookbook.png\"></p><p>By default, AWS OpsWorks Stacks automatically installs the latest updates during setup, after an instance finishes booting. AWS OpsWorks Stacks does not automatically install updates after an instance is online, to avoid interruptions such as restarting application servers. Instead, you manage updates to your online instances yourself, so you can minimize any disruptions.</p><p>AWS recommends that you use one of the following to update your online instances:</p><p><strong>- Create and start new instances to replace your current online instances. Then delete the current instances. The new instances will have the latest set of security patches installed during setup.</strong></p><p><strong>- On Linux-based instances in Chef 11.10 or older stacks, run the Update Dependencies stack command, which installs the current set of security patches and other updates on the specified instances.</strong></p><p>The following options are incorrect as these are irrelevant in updating your online instances in OpsWorks:</p><p><strong>- Delete the entire stack and create a new one.</strong></p><p><strong>- Use CloudFormation to deploy the security patches.</strong></p><p><strong>- On Windows-based instances, run the Update Dependencies stack command.</strong></p><p><strong>- Use WAF to deploy the security patches.</strong></p><p>&nbsp;</p><p><strong>References: </strong></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingsecurity-updates.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingsecurity-updates.html</a></p><p><a href=\"https://docs.aws.amazon.com/opsworks/latest/userguide/workingsecurity.html\">https://docs.aws.amazon.com/opsworks/latest/userguide/workingsecurity.html</a></p><p>&nbsp;</p><p><strong>Check out this AWS OpsWorks Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-opsworks/?src=udemy\">https://tutorialsdojo.com/aws-opsworks/</a></p></div>"
	},
	{
		"question": "<p>A company is running its new web application on a test environment in its on-premises data center. The stateful application is running on a single web server and it connects to a MySQL database that is hosted on a separate server. In a few weeks, the web application is scheduled to be released to the general public and the company is worried about its scalability. The user traffic will be unpredictable so it has been decided to migrate the web application and database to AWS. The company wants to use the Amazon EC2 service for hosting the web application, Amazon Aurora for the database, and Elastic Load Balancing for load distribution.</p><p>Which of the following solutions will allow the web and database tier to scale along with user traffic?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Aurora (Aurora)</strong> is a fully managed relational database engine that's compatible with MySQL and PostgreSQL. Aurora includes a high-performance storage subsystem. Its MySQL- and PostgreSQL-compatible database engines are customized to take advantage of that fast distributed storage. To meet your connectivity and workload requirements, Aurora Auto Scaling dynamically adjusts the number of Aurora Replicas provisioned for an Aurora DB cluster using single-master replication. Aurora Auto Scaling is available for both Aurora MySQL and Aurora PostgreSQL. Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances.</p><p>You define and apply a scaling policy to an Aurora DB cluster. The scaling policy defines the minimum and maximum number of Aurora Replicas that Aurora Auto Scaling can manage. Based on the policy, Aurora Auto Scaling adjusts the number of Aurora Replicas up or down in response to actual workloads, determined by using Amazon CloudWatch metrics and target values. Before you can use Aurora Auto Scaling with an Aurora DB cluster, you must first create an Aurora DB cluster with a primary instance and at least one Aurora Replica.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aurora_rds.jpg\"></p><p>Aurora Auto Scaling uses a scaling policy to adjust the number of Aurora Replicas in an Aurora DB cluster. Aurora Auto Scaling has the following components:</p><p><strong>A service-linked role</strong> – a service role to allow scaling of the Aurora Replicas</p><p><strong>A target metric</strong> – a predefined or custom metric and a target value for the metric. Aurora Auto Scaling creates and manages CloudWatch alarms that trigger the scaling policy</p><p><strong>Minimum and maximum capacity</strong> - the minimum and maximum number of Aurora Replicas to be managed by Application Auto Scaling</p><p><strong>A cooldown period</strong> - blocks subsequent scale-in or scale-out requests until the specified period expires.</p><p><strong>Sticky sessions</strong> are a mechanism to route requests to the same target in a target group. This is useful for servers that maintain state information in order to provide a continuous experience to clients. To use sticky sessions, the clients must support cookies. Sticky sessions are helpful if your stateful application is having problems handling user sessions in an Auto Scaling environment.</p><p>There are two common routing algorithms for Application Load Balancers (ALB):</p><p>- Round Robin</p><p>- Least Outstanding Requests (LOR)</p><p>With the Least outstanding requests (LOR) algorithm, customers can route requests within a target group. As the new request comes in, the load balancer will send it to the target with the least number of outstanding requests. Targets processing long-standing requests or having lower processing capabilities are not burdened with more requests and the load is evenly spread across targets. This also helps the new targets to effectively take the load off of overloaded targets.</p><p>Therefore, the correct answer is: <strong>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB.</strong></p><p>The option that says: <strong>Create an Amazon Aurora MySQL database instance. Create an Aurora Replica and enable Aurora Auto Scaling for the replica. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB</strong> is incorrect. A Network Load Balancer is only recommended if your application expects millions of requests per second. Since there is no mention of this strict requirement on the question, the Application Load Balancer is the recommended choice as it is less expensive and supports Layer 7 features that are suitable for web applications.</p><p>The option that says: <strong>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind an Application Load Balancer with the round-robin routing algorithm. Ensure that the sticky sessions feature is enabled for the ALB</strong> is incorrect. You cannot set Auto Scaling for the master database on Amazon Aurora. You can only manually resize the instance size of the master node.</p><p>The option that says: <strong>Create an Amazon Aurora MySQL database instance and enable Aurora Auto Scaling for the master database. Create an Auto Scaling group of Amazon EC2 instances placed behind a Network Load Balancer with the least outstanding request (LOR) routing algorithm. Ensure that the sticky sessions feature is enabled for the NLB </strong>is incorrect because it is not possible to set Auto Scaling for the master database on Amazon Aurora. You can only manually resize the instance size of the master node. Take note that a Network Load Balancer doesn't fully support the least outstanding request (LOR) routing algorithm as this is commonly used in Application Load Balancers instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Integrating.AutoScaling.html</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.Overview.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.Overview.html</a></p><p><br></p><p><strong>Check out the Amazon Aurora and AWS Elastic Load Balancing Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p></div>"
	},
	{
		"question": "<p>A leading electronics company is getting ready to do a major public announcement of its latest smartphone. Their official website uses an Application Load Balancer in front of an Auto Scaling group of On-Demand EC2 instances, which are deployed across multiple Availability Zones with a Multi-AZ RDS MySQL database. In preparation for their new product launch, the solutions architect checked the performance of the company website and found that the database takes a lot of time to retrieve the data when there are over 100,000 simultaneous requests on the server. The static content such as the images and videos are promptly loaded as expected, but not the customer information that is fetched from the database.</p><p>Which of the following options could be done to solve this issue in a cost-effective way? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Launch a CloudFront web distribution to solve the latency issue."
			},
			{
				"correct": false,
				"answer": "Reduce the latency by switching to Reserved EC2 instances using both Placement Group and Enhanced Networking. "
			},
			{
				"correct": true,
				"answer": "Add Read Replicas in RDS for each Availability Zone."
			},
			{
				"correct": false,
				"answer": "Configure the database tier to use sharding, which will distribute the incoming load to multiple RDS MySQL instances."
			},
			{
				"correct": false,
				"answer": "Upgrade the RDS MySQL database instance size and increase the provisioned IOPS for faster processing."
			},
			{
				"correct": true,
				"answer": "Implement a caching system using ElastiCache in-memory cache on each Availability Zone."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In-memory data caching can be one of the most effective strategies to improve your overall application performance and to <strong>reduce</strong> your database costs. Caching can be applied to any type of database including relational databases such as Amazon RDS or NoSQL databases such as Amazon DynamoDB, MongoDB and Apache Cassandra. The best part of caching is that it’s minimally invasive to implement and by doing so, your application performance regarding both scale and speed is dramatically improved.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_caching.PNG\"></p><p>In this scenario, the issue lies in the database tier of the architecture.</p><p>The option that says: <strong>Implement a caching system using ElastiCache in-memory cache on each Availability Zone</strong> is correct. Adding a cache will significantly reduce the retrieval time for frequent database queries. This also reduces the load significantly on the database tier.</p><p>The option that says: <strong>Add Read Replicas in RDS for each Availability Zone</strong> is correct. This improves the database retrieval time as there will be more nodes that can serve the database request. You won't need to increase the master instance size to accommodate all the read loads.</p><p>The following options are possible answers here but they are not cost-effective unlike the options mentioned above:</p><p>-<strong>Configure the database tier to use sharding, which will distribute the incoming load to multiple RDS MySQL instances.</strong></p><p><strong>-Upgrade the RDS MySQL database instance size and increase the provisioned IOPS for faster processing</strong></p><p>It is quite expensive to implement load distribution on the RDS instances because you will need to use multiple sets of standby instance and read replicas. Since it will use more instances, this solution will cost more as opposed to using just Read Replicas.</p><p>The following options are incorrect because they pertain to improving the performance of the front-end tier:</p><p><strong>-Reduce the latency by switching to Reserved EC2 instances using both Placement Group and Enhanced Networking.</strong></p><p><strong>-Launch a CloudFront web distribution to solve the latency issu<em>e.</em></strong></p><p>To solve the slow data retrieval times from RDS, it is best to implement caching and adding Read Replicas which are cheaper and simpler to do than upgrading the RDS database instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/caching/database-caching/\">https://aws.amazon.com/caching/database-caching/</a></p><p><a href=\"https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/\">https://aws.amazon.com/blogs/database/sharding-with-amazon-relational-database-service/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A finance company has an online analytical processing application that is hosted in AWS and uses Redshift. The application has two different groups: the stocks group and the mutual funds group, which both use data stored in a Redshift cluster. Each query issued by the stocks group takes approximately 2 hours to analyze while the queries from the mutual fund group only take about 10 minutes. The company doesn't want the mutual fund group's queries to wait until the stock group's queries are completed. The management has requested to implement a solution to optimize the processing time of the analytics application.</p><p>Which of the following is the most cost-effective and suitable solution for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Configure Redshift to use Read Replicas. Run the queries of the mutual fund group on the Read Replicas. "
			},
			{
				"correct": false,
				"answer": "Upgrade the Redshift cluster to a larger instance."
			},
			{
				"correct": false,
				"answer": "Implement sharding on your Redshift cluster."
			},
			{
				"correct": true,
				"answer": "Set up two separate workload management groups for both the stocks and mutual fund group."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can use workload management (WLM) to define multiple query queues and to route queries to the appropriate queues at runtime. <strong>Amazon Redshift workload management (WLM)</strong> enables users to flexibly manage priorities within workloads so that short, fast-running queries won't get stuck behind long-running queries. Amazon Redshift WLM creates query queues at runtime according to service classes, which define the configuration parameters for various types of queues, including internal system queues and user-accessible queues. From a user perspective, a user-accessible service class and a queue are functionally equivalent.</p><p>With manual WLM, Amazon Redshift configures one queue with a <em>concurrency level</em> of five, which enables up to five queries to run concurrently, plus one predefined Superuser queue, with a concurrency level of one. You can define up to eight queues. Each queue can be configured with a maximum concurrency level of 50. The maximum total concurrency level for all user-defined queues (not including the Superuser queue) is 50.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_wlm.jpg\"></p><p>Therefore, the correct answer is: <strong>Set up two separate workload management groups for both the stocks and mutual fund group.</strong></p><p>The option that says: <strong>Configure Redshift to use Read Replicas. Run the queries of the mutual fund group on the Read Replicas</strong> is incorrect. This is not cost-effective as it will increase the cost of adding nodes to the cluster.</p><p>The option that says: <strong>Implement sharding on your Redshift cluster</strong> is incorrect. You do not need to implement sharding as this will just increase the complexity of your cluster. WLM takes care of managing the workload for you.</p><p>The option that says: <strong>Upgrade the Redshift cluster to a larger instance</strong> is incorrect. This will significantly increase the cost of the cluster as you will be using bigger instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html\">https://docs.aws.amazon.com/redshift/latest/dg/c_workload_mngmt_classification.html</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html\">https://docs.aws.amazon.com/redshift/latest/dg/cm-c-implementing-workload-management.html</a></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/dg/automatic-wlm.html\">https://docs.aws.amazon.com/redshift/latest/dg/automatic-wlm.html</a></p><p><br></p><p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/ ?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p></div>"
	},
	{
		"question": "<p>A company hosts a serverless application on AWS using Amazon API Gateway and AWS Lambda with Amazon DynamoDB as the backend database. The application has a feature that allows users to create posts and reply to comments based on different topics. The API model currently uses the following methods:</p><p>- <code>GET /posts/[postid]</code> – used to get details about the post</p><p>- <code>GET /users/[userid]</code> – used to get details about a user</p><p>- <code>GET /comments/[commentid]</code> – used to get details of a comment</p><p>The application does not use API keys for request authorization. To increase user engagement on the web app, the company wants to reduce comment latency by making the comments appear in real-time.</p><p>Which of the following solution should be implemented to meet the requirements and improve user experience?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Leverage AWS AppSync by building GraphQL APIs and using Websockets to deliver comments in real-time.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a distribution on Amazon CloudFront and use edge-optimized APIs. Cache API responses in CloudFront to improve comment latency.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Update the application code to call the <code>GET /comments/[commentid]</code> API every 3 seconds to show comments in real-time without sacrificing performance.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Lower the API response time of the Lambda functions by increasing the concurrency limit. This allows functions to run in parallel to deliver the comments in real-time.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS AppSync</strong> is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources like Amazon DynamoDB, Lambda, and more. Adding caches to improve performance, subscriptions to support real-time updates, and client-side data stores that keep offline clients in sync are just as easy. Once deployed, AWS AppSync automatically scales your GraphQL API execution engine up and down to meet API request volumes.</p><p>With managed GraphQL subscriptions, AWS AppSync can push real-time data updates over Websockets to millions of clients. For mobile and web applications, AppSync also provides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.</p><p>AppSync supports real-time chat applications. You can build conversational mobile or web applications that support multiple private chat rooms, offer access to conversation history, and queue outbound messages, even when a device is offline.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_appsync_overview.png\"></p><p>AppSync can also be used for real-time collaboration. You can broadcast data from the backend to all connected clients (one-to-many) or between clients (many-to-many), such as in a second screen scenario where you broadcast the same data to all clients, who can then reply.</p><p>Therefore, the correct answer is: <strong>Leverage AWS AppSync by building GraphQL APIs and using Websockets to deliver comments in real-time.</strong> AWS AppSync can push real-time data updates over Websockets. This can automatically scale to millions of client requests.</p><p>The option that says: <strong>Create a distribution on Amazon CloudFront and use edge-optimized APIs. Cache API responses in CloudFront to improve comment latency</strong> is incorrect. Caching API responses can improve the loading of comments, however, this may not show the comments in real-time if the new comments are not yet on the cache.</p><p>The option that says: <strong>Update the application code to call the </strong><code><strong>GET /comments/[commentid]</strong></code><strong> API every 3 seconds to show comments in real-time without sacrificing performance</strong> is incorrect. This is possible but will heavily put a burden on your Lambda function executions. Even if there are no new comments, the API will still be called. This will add unnecessary cost to function executions.</p><p>The option that says: <strong>Lower the API response time of the Lambda functions by increasing the concurrency limit. This allows functions to run in parallel to deliver the comments in real-time</strong> is incorrect. Increasing the concurrency limit will not help in showing the comments in real-time. If you don't reach the Lambda concurrency limit regularly, there is no apparent advantage for this option.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/appsync/latest/devguide/real-time-websocket-client.html\">https://docs.aws.amazon.com/appsync/latest/devguide/real-time-websocket-client.html</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2020/04/aws-appsync-enables-support-for-generic-websockets-clients-graphql-real-time-subscriptions/\">https://aws.amazon.com/about-aws/whats-new/2020/04/aws-appsync-enables-support-for-generic-websockets-clients-graphql-real-time-subscriptions/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company develops cloud-native applications and uses AWS CloudFormation templates for deploying applications in AWS. The application artifacts and templates are stored in an Amazon S3 bucket with versioning enabled. The developers use Amazon EC2 instances that have integrated development (IDE) to download, modify, and re-upload the artifacts on the S3 bucket. The unit testing is done locally on the EC2 instances. The company wants to improve the existing deployment process with a CI/CD pipeline to help the developers be more productive. The following requirements need to be satisfied:</p><p>- Utilize AWS CodeCommit as the code repository for application and CloudFormation templates.</p><p>- Have automated testing and security scanning for the generated artifacts.</p><p>- Receive a notification when unit testing fails.</p><p>- Ability to turn on/off application features and dynamically customize the deployment as part of CI/CD.</p><p>- The Lead Developer must approve changes before deploying applications to production.</p><p>Which of the following solutions meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an AWS CodeDeploy job to run tests and security scans on the generated artifacts. Create an Amazon CloudWatch rule that will send Amazon SNS alerts when unit testing fails. Use different Docker images for choosing different application features. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write an AWS Lambda function to run unit tests and security scans on the generated artifacts. Add another Lambda trigger on the next pipeline stage to notify the developers if the unit testing fails. Create AWS Amplify plugins to allow turning on/off of application features. Add an AWS SES action on the pipeline to send an approval message to the Lead Developer prior to production deployment.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a Jenkins job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SES alerts when unit testing fails. Use AWS CloudFormation with nested stacks to allow turning on/off of application features. Add an AWS Lambda function to the pipeline to allow approval from the Lead Developer prior to production deployment.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an AWS CodeBuild job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SNS alerts when unit testing fails. Create AWS Cloud Developer Kit (AWS CDK) constructs with a manifest file to turn on/off features of the AWS CDK app. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS CodeBuild</strong> is a fully managed build service in the cloud. CodeBuild compiles your source code, runs unit tests, and produces artifacts that are ready to deploy. CodeBuild eliminates the need to provision, manage, and scale your own build servers. It provides prepackaged build environments for popular programming languages and build tools such as Apache Maven, Gradle, and more. You can also customize build environments in CodeBuild to use your own build tools. CodeBuild scales automatically to meet peak build requests. CodeBuild provides these benefits:</p><p><strong>Fully managed</strong> – CodeBuild eliminates the need to set up, patch, update, and manage your own build servers.</p><p><strong>On demand</strong> – CodeBuild scales on demand to meet your build needs. You pay only for the number of build minutes you consume.</p><p><strong>Out of the box</strong> – CodeBuild provides preconfigured build environments for the most popular programming languages. All you need to do is point to your build script to start your first build.</p><p><img src=\"https://media.tutorialsdojo.com/sap_codepipeline_stages.png\"></p><p>The <strong>AWS Cloud Development Kit (AWS CDK)</strong> lets you easily define applications in the AWS Cloud using your programming language of choice. But creating an application is just the start of the journey. You also want to make changes to it and deploy them. You can do this through the Code suite of tools: AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline. Together, they allow you to build what's called a deployment pipeline for your application.</p><p>In <strong>AWS CodePipeline</strong>, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that someone with the required AWS Identity and Access Management permissions can approve or reject the action.</p><p>If the action is approved, the pipeline execution resumes. If the action is rejected—or if no one approves or rejects the action within seven days of the pipeline reaching the action and stopping—the result is the same as an action failing, and the pipeline execution does not continue.</p><p>You might use manual approvals for these reasons:</p><p>- You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.</p><p>- You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build artifact before it is released.</p><p>- You want someone to review new or updated text that is published on a company website.</p><p>Therefore, the correct answer is: <strong>Create an AWS CodeBuild job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SNS alerts when unit testing fails. Create AWS Cloud Developer Kit (AWS CDK) constructs with a manifest file to turn on/off features of the AWS CDK app. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment. </strong>AWS CodeBuild allows automated build and testing of application artifacts. AWS CDK allows you to control application features using a manifest file and adding an approval on AWS CodePipeline will allow the Lead Developer to approve deployments.</p><p>The option that says: <strong>Write an AWS Lambda function to run unit tests and security scans on the generated artifacts. Add another Lambda trigger on the next pipeline stage to notify the developers if the unit testing fails. Create AWS Amplify plugins to allow turning on/off of application features. Add an AWS SES action on the pipeline to send an approval message to the Lead Developer prior to production deployment </strong>is incorrect. Unit testing and security scanning may take longer than 15 minutes, so AWS Lambda is not recommended here. It would be easier and much simpler to just add an approval stage on the pipeline instead of using AWS SES to send an approval message to the Lead Developer.</p><p>The option that says: <strong>Create a Jenkins job to run tests and security scans on the generated artifacts. Create an Amazon EventBridge rule that will send Amazon SES alerts when unit testing fails. Use AWS CloudFormation with nested stacks to allow turning on/off of application features. Add an AWS Lambda function to the pipeline to allow approval from the Lead Developer prior to production deployment</strong> is incorrect. Instead of creating a Jenkins job, you can use AWS CodeBuild for testing your artifacts. CodeBuild is tightly integrated on AWS so it is very easy to add it to your deployment pipeline.</p><p>The option that says: <strong>Create an AWS CodeDeploy job to run tests and security scans on the generated artifacts. Create an Amazon CloudWatch rule that will send Amazon SNS alerts when unit testing fails. Use different Docker images for choosing different application features. Add a manual approval stage on the pipeline for the Lead Developer’s approval prior to production deployment</strong> is incorrect. AWS CodeDeploy is designed for deploying applications to AWS resources, you should use AWS CodeBuild instead for building and testing application artifacts.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html\">https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html</a></p><p><a href=\"https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\">https://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html</a></p><p><a href=\"https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html\">https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html</a></p><p><br></p><p><strong>Check out these AWS CodeBuild and AWS CodePipeline Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codecommit/?src=udemy\">https://tutorialsdojo.com/aws-codecommit/</a></p><p><a href=\"https://tutorialsdojo.com/aws-codebuild/?src=udemy\">https://tutorialsdojo.com/aws-codebuild/</a></p></div>"
	},
	{
		"question": "<p>A company recently switched to using Amazon CloudFront for its content delivery network. The development team already made the preparations necessary to optimize the application performance for global users. The company’s content management system (CMS) serves both dynamic and static content. The dynamic content is served from a fleet of Amazon EC2 instances behind an application load balancer (ALB) while the static assets are served from an Amazon S3 bucket. The ALB is configured as the default origin of the CloudFront distribution. An Origin Access Identity (OAI) was created and applied to the S3 bucket policy to allow access only from the CloudFront distribution. Upon testing the CMS webpage, the static assets return an error 404 message.</p><p>Which of the following solutions must be implemented to solve this error? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Update the CloudFront distribution and create a new behavior that will forward to the origin of the static assets based on path pattern.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Update the application load balancer listener and create a new path-based rule for the static assets so that it will forward requests to the Amazon S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Edit the CloudFront distribution and create another origin for serving the static assets.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Change the CloudFront distribution type to RTMP in order to serve both the dynamic and static contents.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Update the application load balancer listener to check for HEADER condition if the request is from CloudFront and forward it to the Amazon S3 bucket.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudFront</strong> is a web service that speeds up the distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p>You create a CloudFront distribution to tell CloudFront where you want the content to be delivered from, and the details about how to track and manage content delivery. Then CloudFront uses computers—edge servers—that are close to your viewers to deliver that content quickly when someone wants to see it or use it.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_s3_http_steps.png\"></p><p>In general, if you’re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. To restrict access to content that you serve from Amazon S3 buckets, follow these steps:</p><p>Create a special CloudFront user called an <strong>origin access identity (OAI)</strong> and associate it with your distribution.</p><p>Configure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users. Make sure that users can’t use a direct URL to the S3 bucket to access a file there.</p><p>After you take these steps, users can only access your files through CloudFront, not directly from the S3 bucket.</p><p>You can configure a <strong>single CloudFront web distribution</strong> to serve different types of requests from multiple origins. For example, if you are building a website that serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from a load balancer, you can serve both types of content from a CloudFront web distribution.</p><p>Follow these steps to configure a CloudFront web distribution to serve static content from an S3 bucket and dynamic content from a load balancer:</p><p>Open your web distribution from the CloudFront console.</p><p>Choose the <strong>Origins</strong> tab.</p><p>Create <strong>one origin</strong> for your S3 bucket and <strong>another origin</strong> for your load balancer. <br><strong>Note:</strong> If you're using a custom origin server or an S3 website endpoint, you must enter the origin's domain name into the <strong>Origin Domain Name</strong> field.</p><p>From your distribution, choose the <strong>Behaviors</strong> tab.</p><p>Create a behavior that specifies a path pattern to route all static content requests to the S3 bucket. For example, you can set the \"images/*.jpg\" path pattern to route all requests for \".jpg\" files in the images directory to the S3 bucket.</p><p>Edit the <strong>Default (*) </strong>path pattern behavior and set its <strong>Origin</strong> as your load balancer.</p><p>Therefore, the correct answers are:</p><p><strong>- Edit the CloudFront distribution and create another origin for serving the static assets.</strong></p><p><strong>- Update the CloudFront distribution and create a new behavior that will forward to the origin of the static assets based on path pattern.</strong></p><p>The option that says: <strong>Update the application load balancer listener and create a new path-based rule for the static assets so that it will forward requests to the Amazon S3 bucket</strong> is incorrect. The OAI is used to allow only CloudFront to access the static assets on the S3 bucket. Using the load balancer to forward the requests will result in the access denied error.</p><p>The option that says: <strong>Change the CloudFront distribution type to RTMP in order to serve both the dynamic and static contents </strong>is incorrect. RTMP is used for video-on-demand and live streaming video content from CloudFront. The default CloudFront Web distribution already allows serving both dynamic and static web content.</p><p>The option that says: <strong>Update the application load balancer listener to check for HEADER condition if the request is from CloudFront and forward it to the Amazon S3 bucket</strong> is incorrect. Although an ALB can inspect the HTTP Header requests, using the load balancer to forward the requests will result in the access denied error because of the OAI on the S3 bucket policy.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html\">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html</a></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\">https://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A computer hardware manufacturer has a supply chain application that is written in NodeJS. The application is deployed on an Amazon EC2 Reserved instance which has been provisioned with an IAM Role that provides access to data files stored in an S3 bucket.</p><p>In this architecture, which of the following IAM policies control access to the data files in S3? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>An IAM permissions policy that allows the EC2 role to access S3 objects.</p>"
			},
			{
				"correct": false,
				"answer": "<p>An IAM trust policy that allows the NodeJS supply chain application running on the EC2 instance to access the data files stored in the S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>An IAM trust policy that allows the EC2 instance to assume an EC2 instance role.</p>"
			},
			{
				"correct": false,
				"answer": "<p>An IAM trust policy that allows the EC2 instance to assume an S3 role.</p>"
			},
			{
				"correct": false,
				"answer": "An IAM bucket policy that allows the EC2 role to access S3 objects."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>An IAM role is an IAM identity that you can create in your account that has specific permissions. An IAM role is similar to an IAM user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials such as a password or access keys associated with it. Instead, when you assume a role, it provides you with temporary security credentials for your role session.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_iam_role_s3_bucket.png\"></p><p>To delegate permission to access a resource, you create an IAM role in the trusting account that has <strong>two policies attached:</strong></p><p>The <strong>permissions policy</strong> grants the user of the role the needed permissions to carry out the intended tasks on the resource.</p><p>The <strong>trust policy</strong> specifies which trusted account members are allowed to assume the role.</p><p>Hence, the correct answers are:</p><p><strong>- An IAM trust policy that allows the EC2 instance to assume an EC2 instance role.</strong></p><p><strong>- An IAM permissions policy that allows the EC2 role to access S3 objects.</strong></p><p>The option that says: <strong>An IAM trust policy that allows the NodeJS supply chain application running on the EC2 instance to access the data files stored in the S3 bucket</strong> is incorrect because IAM only provides and controls access to the AWS resources such as EC2 instances but not specifically to the underlying applications being hosted inside the instance.</p><p>The option that says: <strong>An IAM bucket policy that allows the EC2 role to access S3 objects</strong> is incorrect. There is no such thing as an IAM bucket policy as only S3 has this kind of policy.</p><p>The option that says: <strong>An IAM trust policy that allows the EC2 instance to assume an S3 role</strong> is incorrect as an S3 Role is inappropriate in this scenario. We are using an EC2 role in order to provide the EC2 instance access to the data files from the S3 bucket.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#term_trust-policy\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#term_trust-policy</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A law firm has decided to use Amazon S3 buckets for storage after an extensive Total Cost of ownership (TCO) analysis comparing S3 versus acquiring more storage for its on-premises hardware. The attorneys, paralegals, clerks, and other employees of the law firm will be using Amazon S3 buckets to store their legal documents and other media files. For a better user experience, the management wants to implement a single-sign-on system in which the user can just use their existing Active Directory login to access the S3 storage to avoid having to remember yet another password.</p><p>Which of the following options should the solutions architect implement for the above requirement and also provide a mechanism that restricts access for each user to a designated user folder in a bucket? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Set up a federation proxy or a custom identity provider and use AWS Security Token Service to generate temporary tokens. Use an IAM Role to enable access to AWS services.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure an IAM Policy that restricts access only to the user-specific folders in the Amazon S3 Bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure an IAM user that provides access for the user and an IAM Policy that restricts access only to the user-specific folders in the S3 Bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a matching IAM user and IAM Policy for every user in your corporate directory that needs access to a folder in the bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon Connect to integrate the on-premises Active Directory with Amazon S3 and AWS IAM.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Federation enables you to manage access to your AWS Cloud resources centrally. With federation, you can use single sign-on (SSO) to access your AWS accounts using credentials from your corporate directory. Federation uses open standards, such as Security Assertion Markup Language 2.0 (SAML), to exchange identity and security information between an identity provider (IdP) and an application.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sso_ldap_broker.png\"></p><p>Your users might already have identities outside of AWS, such as in your corporate directory. If those users need to work with AWS resources (or work with applications that access those resources) then those users also need AWS security credentials. You can use an IAM role to specify permissions for users whose identity is federated from your organization or a third-party identity provider (IdP). Setting up an identity provider for federated access is required for integrating your on-premises Active Directory with AWS.</p><p>Therefore, the following options are the correct answers:</p><p><strong>- Set up a federation proxy or a custom identity provider and use AWS Security Token Service to generate temporary tokens. Use an IAM Role to enable access to AWS services.</strong></p><p><strong>- Configure an IAM Policy that restricts access only to the user-specific folders in the Amazon S3 Bucket.</strong></p><p>The option that says: <strong>Using Amazon Connect to integrate the on-premises Active Directory with Amazon S3 and AWS IAM </strong>is incorrect because Amazon Connect is simply an easy to use omnichannel cloud contact center that helps companies provide superior customer service at a lower cost.</p><p>The option that says: <strong>Configure an IAM user that provides access for the user and an IAM Policy that restricts access only to the user-specific folders in the S3 Bucket</strong> is incorrect because you have to use an IAM Role, instead of an IAM user, to provide the access needed to your AWS Resources.</p><p>The option that says: <strong>Set up a matching IAM user and IAM Policy for every user in your corporate directory that needs access to a folder in the bucket</strong> is incorrect because you should be creating IAM Roles rather than IAM Users.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/\">https://aws.amazon.com/blogs/security/writing-iam-policies-grant-access-to-user-specific-folders-in-an-amazon-s3-bucket/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_federated-users.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html</a></p><p><br></p><p><strong>Check out this AWS Identity &amp; Access Management (IAM) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company recently launched its new e-commerce platform that is hosted on its on-premises data center. The webservers connect to a MySQL database. The e-commerce platform is quickly gaining popularity and the management is worried that the on-premises servers won’t be able to keep up with user traffic in the coming months. They decided to migrate the entire application to AWS to take advantage of the scalability of the cloud. The following are required for this migration:</p><p>- Improve the security of the application.</p><p>- Increase the reliability and availability of the application.</p><p>- Reduce the latency between the users and the application.</p><p>- Reduce the maintenance overhead after the migration to the cloud.</p><p>Which of the following options should the Solutions Architect implement to meet the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon S3 bucket to store the static contents and enable website hosting. To reduce the latency when serving content, enable S3 Transfer Acceleration. Create AWS WAF rules to block common web exploits.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. Migrate the database to an Amazon RDS MySQL instance with several read replicas.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. Use an Amazon Aurora for MySQL with Multi-AZ enabled as the database.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers and the highly available MySQL database cluster in a master and slave configuration.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an Amazon S3 bucket to store the static contents and enable website hosting. To reduce the latency when serving content, set this bucket as the origin for an Amazon CloudFront distribution. Create AWS WAF rules to block common web exploits.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon RDS Multi-AZ</strong> deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.</p><p>Adding <strong>Amazon EC2 Auto Scaling</strong> to your application architecture is one way to maximize the benefits of the AWS Cloud. When you use Amazon EC2 Auto Scaling, your applications gain the following benefits:</p><p>- Better fault tolerance. Amazon EC2 Auto Scaling can detect when an instance is unhealthy, terminate it, and launch an instance to replace it. You can also configure Amazon EC2 Auto Scaling to use multiple Availability Zones. If one Availability Zone becomes unavailable, Amazon EC2 Auto Scaling can launch instances in another one to compensate.</p><p>- Better availability. Amazon EC2 Auto Scaling helps ensure that your application always has the right amount of capacity to handle the current traffic demand.</p><p>- Better cost management. Amazon EC2 Auto Scaling can dynamically increase and decrease capacity as needed. Because you pay for the EC2 instances you use, you save money by launching instances when they are needed and terminating them when they aren't.</p><p><strong>Amazon CloudFront</strong> works seamlessly with <strong>Amazon Simple Storage Service (S3)</strong> to accelerate the delivery of your web content and reduce the load on your origin servers. The CloudFront edge locations will cache and deliver your content closer to your users to reduce latency and offload capacity from your origin. CloudFront will also restrict access to your S3 bucket to only CloudFront endpoints rendering your content and application more secure and performant.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules that filter out specific traffic patterns you define.</p><p>The option that says: <strong>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. Use an Amazon Aurora for MySQL with Multi-AZ enabled as the database </strong>is correct. The Auto Scaling group ensures there are enough web servers to answer user requests. Spreading the instances on at least two AZ and enabling Multi-AZ on the database ensure that your application is protected if one AZ in AWS goes down.</p><p>The option that says: <strong>Create an Amazon S3 bucket to store the static contents and enable website hosting. To reduce the latency when serving content, set this bucket as the origin for an Amazon CloudFront distribution. Create AWS WAF rules to block common web exploits</strong> is correct. Amazon S3 can serve static content and Amazon CloudFront can cache frequently requested content, which greatly improves latency. AWS WAF has default rules that you can enable to block common web exploits to improve your application security in AWS.</p><p>The option that says: <strong>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers and the highly available MySQL database cluster in a master and slave configuration</strong> is incorrect. Spreading EC2 instances in multiple AZ is good for availability however, hosting the database on the EC2 instances requires a lot of management overhead. It is recommended to use Amazon Aurora for the database.</p><p>The option that says: <strong>Create an Amazon S3 bucket to store the static contents and enable website hosting. To reduce the latency when serving content, enable S3 Transfer Acceleration. Create AWS WAF rules to block common web exploits</strong> is incorrect. Amazon S3 can serve static content but S3 Transfer Acceleration is used for accelerating transfers going to and from the S3 bucket. Caching with CloudFront would be even faster since the user doesn't need to go to the S3 bucket for their cached requests.</p><p>The option that says: <strong>Create an Auto Scaling of Amazon EC2 instances spread in two Availability Zones to host the web servers. Migrate the database to an Amazon RDS MySQL instance with several read-replicas</strong> is incorrect. Without enabling Multi-AZ, your RDS is not protected in the event of crash or failure. When you promote read-replicas to become master, a small downtime is required. This reduces the application availability.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html</a></p><p><a href=\"https://aws.amazon.com/cloudfront/getting-started/S3/\">https://aws.amazon.com/cloudfront/getting-started/S3/</a></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><br></p><p><strong>Check out these Amazon Aurora, Amazon S3, and Amazon CloudFront Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>An Internet-of-Things (IoT) company is building a portal that stores data coming from its 20,000 gas sensors. The gas sensors, which have unique IDs, are used to detect a gas leak or other emissions inside the oil facility. Every 15 minutes, the sensors will send a data point throughout the day containing its ID, current gas level data, as well as the timestamp. Each data point contains critical information coming from the gas sensors. The company would like to query the information coming from a particular gas sensor for the past week and would like to delete all data that are older than eight weeks. The application is using a NoSQL database which is why they are using the Amazon DynamoDB service.</p><p>How would you implement this in the most cost-effective way?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use one table with a primary key which is the sensor ID. Use the timestamp as the hash key."
			},
			{
				"correct": false,
				"answer": "Use one table with a primary key which is the concatenated value of the sensor ID and the timestamp."
			},
			{
				"correct": false,
				"answer": "Use one table every week, with a primary key which is the concatenated value of the sensor ID and the timestamp."
			},
			{
				"correct": true,
				"answer": "Use one table every week, with a composite primary key which is the sensor ID as the partition key and the timestamp as the sort key."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The gas sensors are generating a large amount of data every week. As mentioned in the question, there are 20,000 gas sensors that send data every 15 minutes. That translates to 1,920,000 records in a single day and with that large amount of data, there would be complexities in querying the DynamoDB database. In these scenarios, you can create new DynamoDB every week and then add a partition key and a sort key in the table.</p><p>When you create a table, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key.</p><p><strong>Amazon DynamoDB</strong> supports two different kinds of primary keys:</p><p><strong>Partition key</strong> – A simple primary key, composed of one attribute known as the partition key. DynamoDB uses the partition key's value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored.</p><p><strong>Partition key and sort key</strong> – Referred to as a composite primary key, this type of key is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. DynamoDB uses the partition key value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored.</p><p>Take note that the partition key of an item is also known as its hash attribute. The term hash attribute derives from the use of an internal hash function in DynamoDB that evenly distributes data items across partitions, based on their partition key values.</p><p>Therefore, the correct answer is: <strong>Use one table every week, with a composite primary key which is the sensor ID as the partition key and the timestamp as the sort key.</strong></p><p>The option that says: <strong>Use one table with a primary key which is the sensor ID. Use the timestamp as the hash key</strong> is incorrect. If you only use one table, the table would be very enormous and the searching will take a very long time.</p><p>The option that says: <strong>Use one table every week, with a primary key which is the concatenated value of the sensor ID and the timestamp</strong> is incorrect. You need a fixed value for the primary key so you shouldn't concatenate the timestamp to the sensor ID.</p><p>The option that says: <strong>Use one table with a primary key which is the concatenated value of the sensor ID and the timestamp is</strong> incorrect. If you only use one table, the table would be very enormous and the search will take a very long time.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A company runs its travel and tours website on AWS. The application only supports HTTP at the moment. To improve their SEO ranking and provide more security for their customers, they decided to enable SSL on their website. The company would also like to ensure the separation of roles between the Development team and the Security team in handling the sensitive SSL certificate. The Development team can log in to EC2 Instances but they should not have access to the SSL certificate, which only the Security team has exclusive control of. Currently, they are using an Application Load Balancer which provides loads of incoming traffic to an Auto Scaling group of On-Demand EC2 instances.</p><p>Which of the following options should the solutions architect implement to satisfy the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Store the SSL certificate in IAM and authorize access only to the Security team using an IAM policy. Configure the Application Load Balancer to use the SSL certificate instead of the EC2 instances."
			},
			{
				"correct": false,
				"answer": "Retrieve a read-only copy of the SSL certificate upon the boot of the EC2 instance from a CloudHSM, which is exclusively managed by the Security team."
			},
			{
				"correct": false,
				"answer": "<p>In the web server, set the file owner of the SSL certificate to the Security team and set the file permissions to <code>700</code> which will deny all access to the Development team.</p>"
			},
			{
				"correct": false,
				"answer": "Create a new private S3 bucket and then upload the SSL certificate owned by the Security team. Configure the EC2 instance to have exclusive access to the certificate and block any access from the Development team."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In the ALB, you can create a listener that uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions. To use an HTTPS listener, you must deploy an SSL/TLS server certificate on your load balancer. The load balancer uses this certificate to terminate the connection and then decrypt requests from clients before sending them to the targets.</p><p>Although you can terminate the SSL in the EC2 instance, this setup is not recommended in this scenario. It is best to store the SSL certificate in IAM or in AWS Certificate Manager (ACM) where you can control which teams, either the Security or the Development team, can have access.</p><p><img src=\"https://media.tutorialsdojo.com/sap_alb_ssl_cert.png\"></p><p>Therefore the correct answer is: <strong>Store the SSL certificate in IAM and authorize access only to the Security team using an IAM policy. Configure the Application Load Balancer to use the SSL certificate instead of the EC2 instances.</strong></p><p>The option that says: <strong>In the web server, set the file owner of the SSL certificate to the Security team and set the file permissions to </strong><code><strong>700</strong></code><strong> which will deny all access to the Development team</strong> is incorrect. You don't have to store the SSL Cert on the EC2 instances. The SSL certificate should be at the Load Balancer since the EC2 instances are behind it and the application only supports HTTP.</p><p>The option that says: <strong>Retrieve a read-only copy of the SSL certificate upon the boot of the EC2 instance from a CloudHSM, which is exclusively managed by the Security team</strong> is incorrect. The clients access the website through the Load Balancer and not the EC2 instances directly. Using CloudHSM to store your SSL certificate will just increase your costs without satisfying the requirement.</p><p>The option that says: <strong>Create a new private S3 bucket and then upload the SSL certificate owned by the Security team. Configure the EC2 instance to have exclusive access to the certificate and block any access from the Development team</strong> is incorrect. The Developers can log in to the EC2, therefore, they will still be able to access the SSL certificate in the EC2 instances after they were downloaded from Amazon S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/elb-ssl-tls-certificate-https/\">https://aws.amazon.com/premiumsupport/knowledge-center/elb-ssl-tls-certificate-https/</a></p><p><a href=\"https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates\">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html#https-listener-certificates</a></p><p><br></p><p><strong>Check out this AWS Elastic Load Balancing (ELB) Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-elastic-load-balancing-elb/?src=udemy\">https://tutorialsdojo.com/aws-elastic-load-balancing-elb/</a></p></div>"
	},
	{
		"question": "<p>A retail company runs its two-tier e-commerce website on its on-premises data center. The application runs on a LAMP stack behind a load balancing appliance. The operations team uses SSH to login to the application servers to deploy software updates and install patches on the system. The website has been a target of multiple cyber-attacks recently such as:</p><p> - Distributed Denial of Service (DDoS) attacks</p><p> - SQL Injection attacks</p><p> - Dictionary attacks to SSH accounts on the web servers</p><p>The solutions architect plans to migrate the whole system to AWS to improve its security and availability. The following approaches are laid out to address the company concerns:</p><p> - Fix SQL injection attacks by reviewing existing application code and logic.</p><p> - Use the latest Amazon Linux AMIs to ensure that initial security patches are installed.</p><p> - Install the AWS Systems Manager agent on the instances to manage OS patching.</p><p>Which of the following are additional recommended actions to address the identified attacks while maintaining high availability and security for the application?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login on the EC2 instances but with limited access from the company IP address only. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Enable AWS Shield Standard to protect the instances from DDoS attacks.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution. Enable AWS Shield Advanced for added protection.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login only on a bastion host with limited access from the company IP address. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers and enable AWS Shield Standard for DDoS protection.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Session Manager</strong> allows you to manage your Amazon Elastic Compute Cloud (Amazon EC2) instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS Command Line Interface (AWS CLI). Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.</p><p>A distributed denial of service (DDoS) attack is an attack in which multiple compromised systems attempt to flood a target, such as a network or web application, with traffic. A DDoS attack can prevent legitimate users from accessing a service and can cause the system to crash due to the overwhelming traffic volume.</p><p>AWS provides two levels of protection against DDoS attacks: <strong>AWS Shield Standard</strong> and <strong>AWS Shield Advanced</strong>. All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge. AWS Shield Standard defends against the most common, frequently occurring network and transport layer DDoS attacks that target your website or applications. For higher levels of protection against attacks, you can subscribe to AWS Shield Advanced. When you subscribe to AWS Shield Advanced and add specific resources to be protected, AWS Shield Advanced provides expanded DDoS attack protection for web applications running on the resources.</p><p><strong>AWS WAF</strong> is a web application firewall that helps protect your web applications or APIs against common web exploits and bots that may affect availability, compromise security, or consume excessive resources. AWS WAF gives you control over how traffic reaches your applications by enabling you to create security rules that control bot traffic and block common attack patterns, such as SQL injection or cross-site scripting. You can also customize rules that filter out specific traffic patterns.</p><p><img src=\"https://media.tutorialsdojo.com/sap_well_architectured_security.png\"></p><p>Therefore, the correct answer is: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution. Enable AWS Shield Advanced for added protection.</strong> AWS SSM Session manager allows secure remote access to your instances without using SSH login. AWS WAF rules can block common web exploits like SQL injection attacks and AWS Shield Advanced provides added protection for DDoS attacks.</p><p>The option that says: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login only on a bastion host with limited access from the company IP address. Migrate the on-premises MySQL server to an Amazon RDS Multi-AZ instance. Create a CloudFront distribution in front of the application servers and enable AWS Shield Standard for DDoS protection</strong> is incorrect. You don't need to use a bastion host if you have AWS SSM agent installed on the instances. You can use SSM Session Manager to login to the servers. AWS Shield Standard is already enabled by default but you should enable AWS Shield Advanced for a higher level of protection.</p><p>The option that says: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Enable remote SSH login on the EC2 instances but with limited access from the company IP address only. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Enable AWS Shield Standard to protect the instances from DDoS attacks</strong> is incorrect. You don't need to use a bastion host if you have AWS SSM agent installed on the instances. Using a Single-AZ RDS instance is not recommended if you require a highly available database.</p><p>The option that says: <strong>Use an Application Load Balancer to spread the load on a cluster of Amazon EC2 instances. Disable remote SSH login to the EC2 instances and use AWS SSM Session Manager instead. Migrate the on-premises MySQL server to an Amazon RDS Single-AZ instance. Create a CloudFront distribution in front of the application servers, and apply AWS WAF rules for the distribution</strong> is incorrect. AWS WAF may protect you from common web attacks, but you still need to enable AWS Shield Advanced for a higher level of protection against DDoS attacks. Using a Single-AZ RDS instance is not recommended if you require a highly available database.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html</a></p><p><a href=\"https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html\">https://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html</a></p><p><a href=\"https://aws.amazon.com/waf/\">https://aws.amazon.com/waf/</a></p><p><br></p><p><strong>Check out these AWS Systems Manager and AWS WAF Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-systems-manager/?src=udemy\">https://tutorialsdojo.com/aws-systems-manager/</a></p><p><a href=\"https://tutorialsdojo.com/aws-waf/?src=udemy\">https://tutorialsdojo.com/aws-waf/</a></p><p><br></p><p><strong>AWS Security Services Overview - WAF, Shield, CloudHSM, KMS:</strong></p><p><a href=\"https://youtu.be/-1S-RdeAmMo\">https://youtu.be/-1S-RdeAmMo</a></p></div>"
	},
	{
		"question": "<p>A tech company uses AWS CloudFormation to deploy a three-tier web application that consists of a web tier, application tier, and database tier. The application will utilize an Amazon DynamoDB table for database storage. All resources will be created using a CloudFormation template.</p><p>Which of the following options would allow the application instances access to the DynamoDB tables without exposing the API credentials?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "In the CloudFormation template, use the Parameter section to have the user input the AWS Access and Secret Keys from an already created IAM user that has the permissions required to interact with the DynamoDB table."
			},
			{
				"correct": true,
				"answer": "<p>Launch an IAM Role that has the required permissions to read and write from the DynamoDB table. Reference the IAM Role as a property inside the <code>AWS::IAM::InstanceProfile</code> of the application instance.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Launch an IAM Role that has the required permissions to read and write from the required DynamoDB table. Associate the Role to the application instances by referencing it to the <code>AWS::IAM::InstanceRoleName</code> Property.</p>"
			},
			{
				"correct": false,
				"answer": "Launch an IAM user in the CloudFormation template that has permissions to read and write from the DynamoDB table. Use the GetAtt function to retrieve the Access and secret keys and pass them to the web application instance through the use of its instance user-data."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Identity and Access Management</strong> is an AWS service that you can use to manage users and their permissions in AWS. You can use IAM with AWS CloudFormation to specify what AWS CloudFormation actions users can perform, such as viewing stack templates, creating stacks, or deleting stacks. Furthermore, anyone managing AWS CloudFormation stacks will require permissions to resources within those stacks. For example, if users want to use AWS CloudFormation to launch, update, or terminate Amazon EC2 instances, they must have permission to call the relevant Amazon EC2 actions.</p><p>Using roles to grant permissions to applications that run on EC2 instances requires a bit of extra configuration. An application running on an EC2 instance is abstracted from AWS by the virtualized operating system. Because of this extra separation, an additional step is needed to assign an AWS role and its associated permissions to an EC2 instance and make them available to its applications. This extra step is the creation of an <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">instance profile</a> that is attached to the instance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_lambda_iam.png\"></p><p>The instance profile contains the role and can provide the role's temporary credentials to an application that runs on the instance. Those temporary credentials can then be used in the application's API calls to access resources and to limit access to only those resources that the role specifies. Note that only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.</p><p>Using roles in this way has several benefits. Because role credentials are temporary and rotated automatically, you don't have to manage credentials, and you don't have to worry about long-term security risks. In addition, if you use a single role for multiple instances, you can make a change to that one role and the change is propagated automatically to all the instances.</p><p>An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. If you use the AWS Management Console to create a role for Amazon EC2, the console automatically creates an instance profile and gives it the same name as the role.</p><p>Hence, the correct answer is: <strong>Launch an IAM Role that has the required permissions to read and write from the DynamoDB table. Reference the IAM Role as a property inside the </strong><code><strong>AWS::IAM::InstanceProfile</strong></code><strong> of the application instance.</strong></p><p>The option that says: <strong>Launch an IAM Role that has the required permissions to read and write from the required DynamoDB table. Associate the Role to the application instances by referencing it to the </strong><code><strong>AWS::IAM::InstanceRoleName</strong></code><strong> property</strong> is incorrect because you have to use the <code>InstanceProfile</code> property instead. Take note that there is no <code>InstanceRoleName</code> property in IAM.</p><p>The option that says: <strong>In the CloudFormation template, use the Parameter section to have the user input the AWS Access and Secret Keys from an already created IAM user that has the permissions required to interact with the DynamoDB table</strong> is incorrect because it is a security risk to include the IAM access keys in a CloudFormation template. You have to use an IAM Role instead.</p><p>The option that says:<strong> Launch an IAM user in the CloudFormation template that has permissions to read and write from the DynamoDB table. Use the GetAtt function to retrieve the Access and secret keys and pass them to the web application instance through the use of its instance user-data</strong> is incorrect because it is inappropriate to use an IAM User to provide an EC2 instance the required access to the DynamoDB table. Attaching an IAM Role to the EC2 instances is the most suitable solution in this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#use-iam-to-control-access\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html#use-iam-to-control-access</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-iam-instanceprofile.html</a></p><p><br></p><p><strong>Check out this AWS CloudFormation Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p></div>"
	},
	{
		"question": "<p>A multinational food manufacturing company has recently decided to adopt a hybrid cloud architecture. The company has set up its own AWS account and launched its new VPC. The company wants to have a fast and dedicated network connection from the on-premises data center to the VPC. They aim to increase bandwidth throughput and provide a more consistent network experience to their hybrid architecture than Internet-based connections.</p><p>Which of the following options should the solutions architect implement to achieve this requirement in AWS?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use a software-based VPN to connect the on-premises data center and your VPC.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use a hardware VPN to connect the on-premises data center and your VPC.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Provide a DirectConnect connection between the on-premises data center and your VPC.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Provision a VPN connection between the on-premises data center and your VPC.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Direct Connect</strong> makes it easy to establish a dedicated network connection from your premises to AWS. Using AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.</p><p><img src=\"https://media.tutorialsdojo.com/sap_directconnect_vpn.jpg\"></p><p>Therefore, the correct answer is: <strong>Provide a DirectConnect connection between the on-premises data center and your VPC.</strong> AWS Direct Connect provides private connectivity between AWS and your datacenter and offers a more consistent network experience and increased bandwidth throughput.</p><p>The option that says: <strong>Provision a VPN connection between the on-premises data center and your VPC</strong> is incorrect because a VPN connection does not provide a dedicated network connection. You should use AWS Direct Connect instead.</p><p>The option that says: <strong>Use a hardware VPN to connect the on-premises data center and your VPC</strong> is incorrect because software-based VPN does not provide a dedicated network connection. You should use AWS Direct Connect instead.</p><p>The option that says: <strong>Use a software-based VPN to connect the on-premises data center and your VPC </strong>is incorrect because software-based VPN does not provide a dedicated network connection. You should use AWS Direct Connect instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/directconnect/\">https://aws.amazon.com/directconnect/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p><p><br></p><p><strong>Check out this AWS Direct Connect Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-direct-connect/?src=udemy\">https://tutorialsdojo.com/aws-direct-connect/</a></p><p><br></p><p><strong>S3 Transfer Acceleration vs Direct Connect vs VPN vs Snowball vs Snowmobile:</strong></p><p><a href=\"https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/?src=udemy\">https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>An online bank portal is using Amazon ElastiCache for Redis as its distributed session management layer as well as to store database queries that are expensive to perform. There are some queries that involve joins across multiple tables or queries with intensive calculations and the query results are stored in ElastiCache to avoid having to re-execute the same query again. The solutions architect has been instructed to improve the data durability of the ElastiCache Redis cluster in the event of a hardware fault in an underlying physical server or if the node is accidentally rebooted. In addition, the solutions architect must ensure that there is low data loss potential with minimal downtime.</p><p>Which of the following option is the most suitable solution to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Enable the Redis append-only file feature (AOF).</p>"
			},
			{
				"correct": false,
				"answer": "<p>Improve the cache hit ratio of your Redis cluster.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Schedule a daily automatic backup of the cluster, which writes all data from the cache to a Redis RDB file every midnight.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Set up a Multi-AZ with automatic failover.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>An <strong>ElastiCache Redis cluster</strong> provides varying levels of data durability, performance, and cost for implementing disaster recovery or fault tolerance of your cached data. You can choose the following options to improve the data durability of your ElastiCache cluster:</p><p>- Daily automatic backups<br>- Manual backups using Redis append-only file (AOF)<br>- Setting up a Multi-AZ with Automatic Failover</p><p><img src=\"https://media.tutorialsdojo.com/sap_redis_cluster_failover.png\"></p><p>A <strong>Multi-AZ with Automatic Failover</strong> provides fault tolerance if your cluster’s read/write primary cluster node becomes unreachable or fails. Use this option when data retention, minimal downtime, and application performance are a priority.</p><p>- Its data loss potential is low. Multi-AZ provides fault tolerance for every scenario, including hardware-related issues.</p><p>- Its performance impact is low. Of the available options, Multi-AZ provides the fastest time to recovery, because there is no manual procedure to follow after the process is implemented. Automatic failover buys valuable time that is easily lost when responding to a failure by manually implementing a restore process.</p><p>- Its cost ranges from Low to high. Multi-AZ is the lowest-cost option. Use Multi-AZ when you can't risk losing data as a result of hardware failure or you can't afford the downtime required by other options in your response to an outage.</p><p>Therefore, the correct answer is: <strong>Set up a Multi-AZ with automatic failover.</strong></p><p>The option that says: <strong>Improve the cache hit ratio of your Redis cluster</strong> is incorrect because the cache hit ratio is primarily used in improving the performance of your CloudFront distribution by increasing the proportion of your viewer requests that are served from CloudFront edge caches instead of going to your origin servers for content. This cannot be used in improving the data durability of your ElastiCache layer.</p><p>The option that says: <strong>Schedule a daily automatic backup of the cluster, which writes all data from the cache to a Redis RDB file every midnight</strong> is incorrect because the potential data loss for daily scheduled backups is high - up to a day’s worth of data. Since you will only backup your ElastiCache once a day, there will be hours of gap between the scheduled backup. If you scheduled the backup every 1 AM and the outage happened in the middle of the day (1 PM), then the data loss will be up to 12 hours. You can configure multiple intraday automated backups but there is still the potential risk where you lose data between those scheduled backups.</p><p>The option that says: <strong>Enable the Redis append-only file feature (AOF)</strong> is incorrect. Although enabling Redis Append Only Files (AOF) will improve the data durability of your ElastiCache layer, the downtime for this option is not as minimal as compared to a Multi-AZ solution with an Automatic Failover. The potential data loss is also higher when using Redis Append Only Files (AOF).</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/fault-tolerance-elasticache/\">https://aws.amazon.com/premiumsupport/knowledge-center/fault-tolerance-elasticache/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/AutoFailover.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/RedisAOF.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/RedisAOF.html</a></p><p><br></p><p><strong>Check out this Amazon Elasticache Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p></div>"
	},
	{
		"question": "<p>A digital media publishing company hired a solutions architect to manage its online portal that is composed of a Classic Load Balancer and Amazon EC2 instances deployed across multiple Availability Zones. The architecture is using a combination of Reserved EC2 Instances to handle the steady-state load and On-Demand EC2 Instances to handle the peak load. Currently, the web servers operate at 90% utilization during peak load.</p><p>Which of the following is the most cost-effective option to enable the online portal to quickly recover in the event that one of the Availability Zones is unavailable during peak load?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "To handle the peak load more effectively, use a combination of Reserved and On-Demand instances on all Availability Zones."
			},
			{
				"correct": false,
				"answer": "To handle the peak load, launch an Auto Scaling group of Reserved instances on all Availability Zones."
			},
			{
				"correct": true,
				"answer": "<p>To handle the peak load in the most cost-effective manner, launch a Spot Fleet of EC2 instances with a diversified allocation strategy on all Availability Zones instead of On-Demand instances.</p>"
			},
			{
				"correct": false,
				"answer": "To handle the peak load more effectively, use a combination of Spot and On-Demand instances on all Availability Zones."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The focus of the scenario is how to properly handle the peak load of your application in the most cost-effective manner since the steady-state load is already handled by the Reserved EC2 instances. Spot Instances provide the cheapest compute capacity to handle the peak load.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ec2_type_fleet_request.png\"></p><p>To avoid interruption to your Spot instances, you can actually set up a diversified spot fleet allocation strategy in which you are using a range of different EC2 instance types such as c3.2xlarge, m3.xlarge, r3.xlarge et cetera instead of just one type. This will effectively increase the chances of providing a more stable compute capacity to your application. Therefore, in the event that there is a Spot interruption due to the high demand for a specific instance type, say c3.2xlarge, your application could still scale using another instance type such as m3.xlarge or r3.xlarge.</p><p>Hence, the correct answer in this scenario is the option that says: <strong>To handle the peak load in the most cost-effective manner, launch a Spot Fleet of EC2 instances with a diversified allocation strategy on all Availability Zones instead of On-Demand instances.</strong></p><p>The following options are incorrect because the scenario requires a highly available and cost-effective architecture to allow the application to recover quickly, hence, using an Auto Scaling group is a must to handle the peak load and improve both the availability and scalability of the application:</p><p><strong>- To handle the peak load more effectively, use a combination of Spot and On-Demand instances on all Availability Zones.</strong></p><p><strong>- To handle the peak load more effectively, use a combination of Reserved and On-Demand instances on all Availability Zones.</strong></p><p>The option that says: <strong>To handle the peak load, launch an Auto Scaling group of Reserved instances on all Availability Zones</strong> is incorrect because even though it uses Auto Scaling, Reserved Instances cost more than Spot instances so it is more suitable to use the latter to handle the peak load.</p><p>For additional references about using Spot Instances to handle your peak load requirements in AWS, please refer below:</p><p><em>You can launch your master and core instance groups as On-Demand Instances to handle the normal capacity and launch the task instance group as Spot Instances to handle your </em><strong><em>peak load </em></strong><em>requirements.</em></p><p><a href=\"https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\">https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html</a></p><p><em>If you have urgent, unpredictable scaling needs, such as a news website that must scale during a major news event or game launch, we recommend that you specify alternative instance types for your On-Demand Instances, in the event that your preferred option does not have sufficient available capacity. For example, you might prefer </em><code><em>c5.2xlarge</em></code><em> On-Demand Instances, but if there is insufficient available capacity, you'd be willing to use some </em><code><em>c4.2xlarge</em></code><em> instances during </em><strong><em>peak load.</em></strong><em> In this case, EC2 Fleet attempts to fulfill all of your target capacity using</em><code><em>c5.2xlarge</em></code><em> instances, but if there is insufficient capacity, it automatically launches </em><code><em>c4.2xlarge</em></code><em> instances to fulfill the target capacity.</em></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-fleet-configuration-strategies.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-fleet-configuration-strategies.html</a></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11\">https://d1.awsstatic.com/whitepapers/total-cost-of-operation-benefits-using-aws.pdf#page=11</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-fleet.html#spot-fleet-allocation-strategy</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2019/08/new-capacity-optimized-allocation-strategy-for-provisioning-amazon-ec2-spot-instances/\">https://aws.amazon.com/about-aws/whats-new/2019/08/new-capacity-optimized-allocation-strategy-for-provisioning-amazon-ec2-spot-instances/</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A company wants to release a weather forecasting app for mobile users. The application servers generate a weather forecast every 15 minutes, and each forecast update overwrites the older forecast data. Each weather forecast outputs approximately 1 billion unique data points, where each point is about 20 bytes in size. This results in about 20GB of data for each forecast. Approximately 1,500 global users access the forecast data concurrently every second, and this traffic can spike up to 10 times more during weather events. The company wants users to have a good experience when using the weather forecast application so it requires that each user query must be processed in less than two seconds.</p><p>Which of the following solutions will meet the required application request rate and response time?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Amazon S3 bucket to store the weather forecast data points as individual objects. Create a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer to query the objects on the S3 bucket. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon Elasticsearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Write an Amazon Lambda@Edge function to cache the data points on edge locations for a 15-minute duration.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon Elasticsearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Configure a cache-control timeout of 15 minutes in the API caching section of the API Gateway stage.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use an Amazon EFS volume to store the weather forecast data points. Mount this EFS volume on a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Elastic File System (Amazon EFS)</strong> provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. It is built to scale on-demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files, eliminating the need to provision and manage capacity to accommodate growth.</p><p>Amazon EFS can provide very low and consistent operational latency as well as a throughput scale of 10+GB per second.</p><p>Amazon EFS file systems are distributed across an unconstrained number of storage servers. This distributed data storage design enables file systems to grow elastically to petabyte scale and enables massively parallel access from Amazon EC2 instances to your data. The Amazon EFS-distributed design avoids the bottlenecks and constraints inherent to traditional file servers.</p><p>This distributed data storage design means that multithreaded applications and applications that concurrently access data from multiple Amazon EC2 instances can drive substantial levels of aggregate throughput and IOPS. Big data and analytics workloads, media processing workflows, content management, and web serving are examples of these applications. In addition, Amazon EFS data is distributed across multiple Availability Zones, providing a high level of durability and availability.</p><p><img src=\"https://media.tutorialsdojo.com/sap_efs_how_it_works.png\"></p><p><strong>Amazon CloudFront</strong> is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment. CloudFront can provide a reliable, low latency, and high throughput network connectivity for global users.</p><p>To deliver content to end-users with lower latency, Amazon CloudFront peers with thousands of Tier 1/2/3 telecom carriers globally, is well connected with all major access networks for optimal performance, and has hundreds of terabits of deployed capacity.</p><p>Therefore, the correct answer is: <strong>Use an Amazon EFS volume to store the weather forecast data points. Mount this EFS volume on a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution.</strong></p><p>The option that says: <strong>Create an Amazon Elasticsearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Configure a cache-control timeout of 15 minutes in the API caching section of the API Gateway stage</strong> is incorrect. This is a possible implementation but the Lambda functions won’t be able to quickly scale and serve requests during peak traffic. By default, the burst concurrency for Lambda functions is between 500-3000 requests per second (depending on region).</p><p>The option that says: <strong>Create an Amazon Elasticsearch cluster to store the weather forecast data points. Write AWS Lambda functions to query the ES cluster. Create an Amazon CloudFront distribution and point the origin to an Amazon API Gateway endpoint that invokes the Lambda functions. Write an Amazon Lambda@Edge function to cache the data points on edge locations for a 15-minute duration</strong> is incorrect. This is a good solution for caching, however, this solution will not be able to serve the expected peak traffic during weather events. Lambda@Edge can serve only up to 10,000 requests per second.</p><p>The option that says: <strong>Create an Amazon S3 bucket to store the weather forecast data points as individual objects. Create a fleet of Auto Scaling Amazon EC2 instances behind an Elastic Load Balancer to query the objects on the S3 bucket. Create an Amazon CloudFront distribution and point the origin to the ELB. Configure a 15-minute cache-control timeout for the CloudFront distribution</strong> is incorrect. This solution is not recommended. Access from the EC2 instances to the S3 bucket via HTTP calls will increase the total response time of the application. Access to EFS is faster compared to calling objects on S3 buckets. Although AWS S3 supports strong read-after-write consistency, billions of objects will be overwritten on the S3 bucket every 15 minutes which could take longer to write than using Amazon EFS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/efs/\">https://aws.amazon.com/efs/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/features/\">https://aws.amazon.com/cloudfront/features/</a></p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><p><br></p><p><strong>Check out the Amazon EFS and comparison Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-efs/?src=udemy\">https://tutorialsdojo.com/amazon-efs/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p></div>"
	},
	{
		"question": "<p>A company launched a high-performance computing (HPC) application inside the VPC of its AWS account. The application is composed of hundreds of private EC2 instances running in a cluster placement group, which allows the instances to communicate with each other at network speeds of up to 10 Gbps. There is also a custom cluster controller EC2 instance that closely controls and monitors the system performance of each instance. The cluster controller has the same instance type and AMI as the other instances. It is configured with a public IP address and runs outside the placement group. The Solutions Architect has been tasked to improve the network performance between the controller instance and the EC2 instances in the placement group.</p><p>Which option provides the MOST suitable solution that the Architect must implement to satisfy the requirement while maintaining low-latency network performance?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Terminate the custom cluster controller EC2 instance and stop all of the running instances in the existing placement group. Move the cluster controller instance to the existing placement group and restart all of the instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Attach an Elastic IP address to the custom cluster controller instance to increase its network capability to 10 Gbps.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Terminate the custom cluster controller instance and re-launch it to the existing placement group. Attach an Elastic Network Adapter (ENA) to the cluster controller instance to increase its network performance. Change the placement strategy of the placement group to <code>Spread</code>.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Stop the custom cluster controller instance and move it to the existing placement group.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Cluster placement groups are recommended for applications that benefit from low network latency, high network throughput, or both. They are also recommended when the majority of the network traffic is between the instances in the group. To provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.</p><p><img src=\"https://media.tutorialsdojo.com/sap_az1_placement_group.png\"></p><p>A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Launching instances in a spread placement group reduces the risk of simultaneous failures that might occur when instances share the same racks. Spread placement groups provide access to distinct racks, and are therefore suitable for mixing instance types or launching instances over time.</p><p><img src=\"https://media.tutorialsdojo.com/sap_az1_placement_group_instances.png\"></p><p>You can change the placement group for an instance in any of the following ways:</p><p>Move an existing instance to a placement group</p><p>Move an instance from one placement group to another</p><p>Remove an instance from a placement group</p><p>Before you move or remove the instance, the instance must be in the stopped state. You can move or remove an instance using the AWS CLI or an AWS SDK.</p><p>Hence, the correct answer is: <strong>Stop the custom cluster controller instance and move it to the existing placement group.</strong></p><p>The option that says: <strong>Terminate the custom cluster controller EC2 instance and stop all of the running instances in the existing placement group. Move the cluster controller instance to the existing placement group and restart all of the instances</strong> is incorrect because you don't need to restart or terminate any instance to move a new EC2 instance to an existing placement group. You just have to stop the cluster controller instance, move it to the placement group, and restart it.</p><p>The option that says:<strong> Attach an Elastic IP address to the custom cluster controller instance to increase its network capability to 10 Gbps</strong> is incorrect because an Elastic IP is simply a static IPv4 address designed for dynamic cloud computing. It doesn't increase the network bandwidth of the instance to 10 Gbps either.</p><p>The option that says: <strong>Terminate the custom cluster controller instance and re-launch it to the existing placement group. Attach an Elastic Network Adapter (ENA) to the cluster controller instance to increase its network performance. Change the placement strategy of the placement group to </strong><code><strong>Spread</strong></code> is incorrect because using a Spread placement group will degrade the existing network performance of the architecture. The use of a Cluster placement group must be maintained. And while it is true that the Elastic Network Adapter (ENA) can increase the instance network performance, the described process of moving the custom cluster controller instance to the placement group is still incorrect. You can just stop the instance and directly include it to the placement group. There is no need to terminate the EC2 instance.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A company is developing a prototype distributed system that requires a multithreaded event-based key/value cache storage. The system will cache small arbitrary data, such as strings and objects, from the results of database and API calls. The cache layer should also enable the client programs to automatically identify all of the nodes in a cache cluster and to initiate and maintain connections to all of these nodes.</p><p>Which of the following options is the most suitable and cost-effective service to use to achieve the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Amazon ES</p>"
			},
			{
				"correct": false,
				"answer": "<p>AWS Greengrass</p>"
			},
			{
				"correct": true,
				"answer": "<p>Amazon ElastiCache for Memcached</p>"
			},
			{
				"correct": false,
				"answer": "<p>Amazon ElastiCache for Redis</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Redis and Memcached</strong> are popular, open-source, in-memory data stores. Although they are both easy to use and offer high performance, there are important differences to consider when choosing an engine. Memcached is designed for simplicity while Redis offers a rich set of features that make it effective for a wide range of use cases.</p><p>In this scenario, Redis can provide a much more durable and powerful cache layer to the prototype distributed system, however, you should take note of one keyword in the requirement: <strong>multithreaded</strong>. In terms of commands execution, Redis is mostly a single-threaded server. It is not designed to benefit from multiple CPU cores unlike Memcached, however, you can launch several Redis instances to scale out on several cores if needed. Memcached is a more suitable choice since the scenario specifies that the system will run large nodes with multiple cores or threads and in addition, the prototype only needs simple data structures which Memcached can adequately provide.</p><p>Moreover, the ability to enable the client programs to automatically identify all of the nodes in a cache cluster can be met by using the Auto Discovery feature of Amazon ElastiCache for Memcached.</p><p>You can choose Memcached over Redis if you have the following requirements:</p><p>- You need the simplest model possible.</p><p>- You need to run large nodes with multiple cores or threads.</p><p>- You need the ability to scale out and in, adding and removing nodes as demand on your system increases and decreases.</p><p>- You need to cache objects, such as a database.</p><p><img src=\"https://media.tutorialsdojo.com/sap_memcached_vs_redis.png\"></p><p>Therefore, the correct answer is: <strong>Amazon ElastiCache for Memcached.</strong></p><p>The option that says: <strong>Amazon ElastiCache for Redis</strong> is incorrect because Redis does not totally support a multithreaded architecture and in addition, it does not have an Auto Discovery feature, unlike Memcached. Although Redis has more features compared with Memcached, the scenario only requires a cache layer which is multithreaded and can store simple data model. This is why Memcached is a more suitable cache engine to choose from instead of Redis.</p><p>The option that says: <strong>Amazon ES</strong> is incorrect. It is just a fully managed service that makes it easy for you to deploy, secure, and operate Elasticsearch at scale with downtime. It is not a caching service unlike Amazon ElastiCache.</p><p>The option that says: <strong>AWS Greengrass</strong> is incorrect. The AWS IoT Greengrass service is primarily used to enable connected devices to run AWS Lambda functions, execute predictions based on machine learning models, keep device data in sync, and communicate with other devices securely even without an Internet connection. Hence, this is not a suitable option for this scenario.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/redis-vs-memcached\">https://aws.amazon.com/elasticache/redis-vs-memcached</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html\">https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html</a></p><p><a href=\"https://aws.amazon.com/caching/aws-caching/\">https://aws.amazon.com/caching/aws-caching/</a></p><p><br></p><p><strong>Check out this Redis (cluster mode enabled vs disabled) vs Memcached comparison:<br></strong></p><p><a href=\"https://tutorialsdojo.com/redis-cluster-mode-enabled-vs-disabled-vs-memcached/?src=udemy\">https://tutorialsdojo.com/redis-cluster-mode-enabled-vs-disabled-vs-memcached/</a></p></div>"
	},
	{
		"question": "<p>A large company has multiple AWS accounts with multiple IAM Users that launch different types of Amazon EC2 instances and EBS volumes every day. As a result, most accounts quickly hit the service limit and IAM users can no longer create any new instances. When cleaning up the AWS accounts, the solutions architect noticed that the majority of the instances and volumes are untagged. Therefore, it is difficult to pinpoint the owner of these resources and verify if they are safe to terminate. Because of this, the management had issued a new protocol that requires adding a predefined set of tags before anyone can launch their EC2 instances.</p><p>Which of the following options is the simplest way to enforce this new requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule in AWS Config requiring users to tag specific resources and raise an alert whenever the rule is violated. The Config Rule should allow a user to launch EC2 instances only if the user adds all the tags defined in the rule. If the user applies any other tag then the action is denied.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a Service Control Policy that restricts launching any AWS resources without a tag by including the <code>Condition</code> element in the policy which uses the <code>ForAllValues</code> qualifier and the <code>aws:TagKeys</code> condition. This policy will require its principals to tag resources during creation. Apply the SCP to the OU which will automatically cascade the policy to individual member accounts.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule using AWS Systems Manager requiring users to tag specific resources and raise an alert whenever the rule is violated. This will allow a user to launch EC2 instances only if certain tags were defined. If the user applies any other tag then the action is denied.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Apply an IAM policy to the individual member accounts of the OU that includes a <code>Condition</code> element in the policy containing the <code>ForAllValues</code> qualifier and the <code>aws:TagKeys</code> condition. This policy will require its principals to attach specific tags to their resources during creation.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can specify tags for EC2 instances and EBS volumes as part of the API call that creates the resources. Using this principle, you can require users to tag specific resources by applying conditions to their IAM policy. Using AWS Organizations, you can consolidate all of your AWS accounts and group the business units into separate Organizational Units (OUs) with a custom service control policy (SCP).</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_explanations.png\"></p><p>You can configure your IAM policy to allow a user to launch an EC2 instance and create an EBS volume only if the user applies all the tags that are defined in the policy using the <code><strong>ForAllValues</strong></code><strong> </strong>qualifier. If the user applies any tag that's not included in the policy then the action is denied. To enforce case sensitivity, use the condition <code><strong><em>aws:TagKeys</em></strong></code>.</p><p>You can use organizational units (OUs) to group accounts together to administer as a single unit. This greatly simplifies the management of your accounts. For example, you can attach a policy-based control to an OU, and all accounts within the OU automatically inherit the policy. You can create multiple OUs within a single organization, and you can create OUs within other OUs.</p><p>Therefore, the correct answer is: <strong>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a Service Control Policy that restricts launching any AWS resources without a tag by including the </strong><code><strong>Condition</strong></code><strong> element in the policy which uses the </strong><code><strong>ForAllValues</strong></code><strong> qualifier and the </strong><code><strong>aws:TagKeys</strong></code><strong> condition. This policy will require its principals to tag resources during creation. Apply the SCP to the OU which will automatically cascade the policy to individual member accounts.</strong></p><p>The option that says: <strong>Apply an IAM policy to the individual member accounts of the OU that includes a </strong><code><strong>Condition</strong></code><strong> element in the policy containing the </strong><code><strong>ForAllValues</strong></code><strong> qualifier and the </strong><code><strong>aws:TagKeys</strong></code><strong> condition. This policy will require its principals to attach specific tags to their resources during creation</strong> is incorrect as it requires a lot of effort to implement the policy in each and every AWS account of the organization. You should set up AWS Organizations, group different accounts into separate Organizational Units (OU), and use SCP instead.</p><p>The option that says: <strong>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule in AWS Config requiring users to tag specific resources and raise an alert whenever the rule is violated. The Config Rule should allow a user to launch EC2 instances only if the user adds all the tags defined in the rule. If the user applies any other tag then the action is denied</strong> is incorrect. AWS Config only audits and evaluates if your instance and volume configurations match the rules you have created. Unlike an IAM policy, it does not permit nor restrict users from performing certain actions.</p><p>The option that says: <strong>Configure AWS Organizations to group different accounts into separate Organizational Units (OU) depending on the business function. Create a rule using AWS Systems Manager requiring users to tag specific resources and raise an alert whenever the rule is violated. This will allow a user to launch EC2 instances only if certain tags were defined. If the user applies any other tag then the action is denied</strong> is incorrect because you cannot create a rule using AWS Systems Manager requiring users to tag specific resources and raise an alert whenever the rule is violated. You have to use an IAM policy in order to do this.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/\">https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-restrict/</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_multi-value-conditions.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p><p><br></p><p><strong>Service Control Policies (SCP) vs IAM Policies:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p><p><br></p><p><strong>Comparison of AWS Services Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/comparison-of-aws-services/?src=udemy\">https://tutorialsdojo.com/comparison-of-aws-services/</a></p></div>"
	},
	{
		"question": "<p>A company has a large collection of user-submitted stock photos. An AWS Lambda function processes and extracts metadata from these photos to make a searchable catalog. The metadata is extracted depending on several rules and the output is sent to an Amazon ElastiCache for Redis cluster. The metadata extraction is done in several batches and the whole process takes about 45 minutes to complete. Whenever there is a change in the metadata extraction rules, the update process is triggered manually before the extraction process starts. As the stock photo submissions are steadily growing, the company wants to reduce the metadata extraction time for its catalog.</p><p>Which of the following options should the Solutions Architect implement to reduce the time for the metadata extraction process?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Create another workflow that will retrieve the list of photos for processing and execute the metadata extraction workflow for each photo.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Associate the Lambda functions to an AWS Batch compute environment. Write another Lambda function that will retrieve the list of photos for processing and send each item to the job queue in the AWS Batch compute environment.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Set this SQS queue as the input for the Step Functions workflow.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Configure all the Lambda extraction functions to subscribe to this SQS queue with higher batch size.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Step Functions</strong> can be used to run a serverless workflow that coordinates multiple AWS Lambda functions. AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by your business logic.</p><p><strong>AWS Lambda</strong> is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. With Lambda, you can run code for virtually any type of application or backend service - all with zero administration. Just upload your code as a ZIP file or container image, and Lambda automatically and precisely allocates compute execution power and runs your code based on the incoming request or event, for any scale of traffic.</p><p><img src=\"https://media.tutorialsdojo.com/sap_step_function_lambda_function.JPG\"></p><p>In this scenario, you can quickly make changes to the whole process depending on the extraction rules if you have dedicated Lambda functions for each type of metadata. Technically, you can create one Lambda function to call the other Lambda metadata extraction functions. However, it is quite challenging to orchestrate the data flow of the Lambda functions as the number of functions grow. Plus, any change in the flow of the application will require changes in multiple places, and you could end up writing the same code over and over again.</p><p>To solve this challenge, you can use AWS Step Functions. This is a serverless orchestration service that lets you easily coordinate multiple Lambda functions into flexible workflows that are easy to debug and easy to change. Step Functions will keep your Lambda functions free of additional logic by triggering and tracking each step of your application for you.</p><p>Therefore, the correct answer is: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Create another workflow that will retrieve the list of photos for processing and execute the metadata extraction workflow for each photo.</strong></p><p>The option that says: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Associate the Lambda functions to an AWS Batch compute environment. Write another Lambda function that will retrieve the list of photos for processing and send each item to the job queue in the AWS Batch compute environment</strong> is incorrect. AWS Batch only adds complexity to the solution. AWS Batch is designed to easily and efficiently run hundreds of thousands of batch computing jobs on AWS but not with Lambda functions.</p><p>The option that says: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Create a workflow on AWS Step Functions that will run multiple Lambda functions in parallel. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Set this SQS queue as the input for the Step Functions workflow</strong> is incorrect. This might be possible if you have a Lambda Function that consumes the messages on the SQS and feeds them to the Step Function, which is not explicitly stated on this option. An SQS queue cannot be used as a direct input for an AWS Step Function workflow.</p><p>The option that says: <strong>Split the single Lambda function that processes the photos into several functions dedicated for each type of metadata. Write another Lambda function that will retrieve the list of photos for processing and send each item to an Amazon SQS queue. Configure all the Lambda extraction functions to subscribe to this SQS queue with higher batch size</strong> is incorrect. This may be possible but it will not work for this scenario. When a Lambda function processes the photo for a specific type of metadata, it will become invisible on the queue, and other Lambda functions will not be able to process the same photo. This defeats the purpose of multiple Lambda functions that are supposed to process the photo in parallel.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/\">https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/</a></p><p><a href=\"https://docs.aws.amazon.com/step-functions/latest/dg/connect-lambda.html\">https://docs.aws.amazon.com/step-functions/latest/dg/connect-lambda.html</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/new-step-functions-support-for-dynamic-parallelism/\">https://aws.amazon.com/blogs/aws/new-step-functions-support-for-dynamic-parallelism/</a></p><p><a href=\"https://aws.amazon.com/step-functions/use-cases/\">https://aws.amazon.com/step-functions/use-cases/</a></p><p><br></p><p><strong>Check out the AWS Step Functions Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-step-functions/?src=udemy\">https://tutorialsdojo.com/aws-step-functions/</a></p><p><br></p><p><strong>AWS Lambda Overview - Serverless Computing in AWS:</strong></p><p><a href=\"https://youtu.be/bPVX1zHwAnY\">https://youtu.be/bPVX1zHwAnY</a></p></div>"
	},
	{
		"question": "<p>A company has created several AWS accounts for each of its separate departments such as Accounting, Human Resources, IT, and many others. These accounts are not linked to each other and hence, it is quite difficult to have a consolidated view of all of the bills for each account.</p><p>Which of the following options can the solutions architect implement in order to have a unified view of all accounts and their respective billings that are used by the organization?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "None of the above"
			},
			{
				"correct": true,
				"answer": "Use AWS Organization, which enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. Use one master account and multiple&nbsp;member accounts for each department."
			},
			{
				"correct": false,
				"answer": "Consolidate multiple AWS accounts of your organization by providing cross-account access on each and every account."
			},
			{
				"correct": false,
				"answer": "<p>Set up cross-account access on all AWS accounts of your organization. Use one master account and multiple member accounts for each department.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Organizations</strong> is an account management service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. AWS Organizations includes account management and consolidated billing capabilities that enable you to better meet the budgetary, security, and compliance needs of your business. As an administrator of an organization, you can create accounts in your organization and invite existing accounts to join the organization.</p><p>You can use the consolidated billing feature in AWS Organizations to consolidate payment for multiple AWS accounts or multiple AISPL accounts. Each organization in AWS Organizations has a master account that pays the charges of all the member accounts. If you have access to the master account, you can see a combined view of the AWS charges that are incurred by the member accounts. You also can get a cost report for each member account.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_organization_overview.png\"></p><p>Therefore, the correct answer is: <strong>Use AWS Organization, which enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. Use one master account and multiple member accounts for each department.</strong></p><p>The option that says: <strong>Set up cross-account access on all AWS accounts of your organization. Use one master account and multiple member accounts for each department</strong> is incorrect. Cross-account access is not needed; AWS Organizations with the consolidated billing feature will give you a breakdown of the billing for all accounts under the organization.</p><p>The option that says: <strong>Consolidate multiple AWS accounts of your organization by providing cross-account access on each and every account</strong> is incorrect. Cross-account access is not needed, AWS Organizations with consolidated billing feature will give you a breakdown of the billing for all accounts under the organization.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/organizations/\">https://aws.amazon.com/organizations/</a></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html</a></p><p><br></p><p><strong>Check out this AWS Organizations Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-organizations/?src=udemy\">https://tutorialsdojo.com/aws-organizations/</a></p></div>"
	},
	{
		"question": "A data analytics company is running a Redshift data warehouse for one of its major clients. In compliance with the Business Continuity Program of the client, they need to provide a Recovery Point Objective of 24 hours and a Recovery Time Objective of 1 hour. The data warehouse should be available even in the event that the entire AWS Region is down. \n\nWhich of the following is the most suitable configuration for this scenario?",
		"answers": [
			{
				"correct": false,
				"answer": "Enable Redshift replication from the cluster running in the primary region to the cluster running in the secondary region. Change the DNS endpoint to the secondary cluster's primary node in case of system failures in the primary region."
			},
			{
				"correct": false,
				"answer": "<p>Configure Redshift to<strong> </strong>use Cross-Region Replication (CRR) and in case of system failure, failover to the backup region and manually copy the snapshot from the primary region to the secondary region.</p>"
			},
			{
				"correct": false,
				"answer": "No additional configuration needed. Redshift is configured with automatic snapshot by default."
			},
			{
				"correct": true,
				"answer": "<p>Configure Redshift to have automatic snapshots and do a cross-region snapshot copy to automatically replicate the current production cluster to the disaster recovery region.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When automated snapshots are enabled for a cluster, <strong>Amazon Redshift</strong> periodically takes snapshots of that cluster, usually every eight hours or following every 5 GB per node of data changes, or whichever comes first. Automated snapshots are enabled by default when you create a cluster. These snapshots are deleted at the end of a retention period. The default retention period is one day, but you can modify it by using the Amazon Redshift console or programmatically by using the Amazon Redshift API.</p><p>When you enable Amazon Redshift to automatically copy snapshots to another region, you specify the destination region where you want snapshots to be copied. In the case of automated snapshots, you can also specify the retention period that they should be kept in the destination region. After an automated snapshot is copied to the destination region and it reaches the retention time period there, it is deleted from the destination region, keeping your snapshot usage low. You can change this retention period if you need to keep the automated snapshots for a shorter or longer period of time in the destination region.</p><p><img src=\"https://media.tutorialsdojo.com/sap_redshift_snapshot.png\"></p><p>Therefore the correct answer is: <strong>Configure Redshift to have automatic snapshots and do a cross-region snapshot copy to automatically replicate the current production cluster to the disaster recovery region.</strong> The automatic snapshots feature is automatically enabled by default. You can then configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another AWS Region.</p><p>The option that says: <strong>Configure Redshift to use Cross-Region Replication (CRR) and in case of system failure, failover to the backup region and manually copy the snapshot from the primary region to the secondary region</strong> is incorrect. Amazon Redshift only has cross-region backup feature (using snapshots), not Cross-Region Replication.</p><p>The option that says: <strong>Enable Redshift replication from the cluster running in the primary region to the cluster running in the secondary region. Change the DNS endpoint to the secondary cluster's primary node in case of system failures in the primary region</strong> is incorrect. Amazon Redshift only has cross-region backup feature (using snapshots); it can't replicate directly to another cluster in another region.</p><p>The option that says: <strong>No additional configuration needed. Redshift is configured with automatic snapshot by default</strong> is incorrect. Even though the automatic snapshots feature is enabled by default, cross-region snapshot copy is not.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_CopySnapshot.html</a></p><p><br></p><p><strong>Check out this Amazon Redshift Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-redshift/?src=udemy\">https://tutorialsdojo.com/amazon-redshift/</a></p></div>"
	},
	{
		"question": "<p>A large media company based in Los Angeles, California runs a MySQL RDS instance inside an AWS VPC. The company runs a custom analytics application on its on-premises data center that will require read-only access to the database. The company wants to have a read replica of a running MySQL RDS instance inside of AWS cloud to its on-premises data center and use it as an endpoint for the analytics application.</p><p>Which of the following options is the most secure way of performing this replication?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Create an IPSec VPN connection using either OpenVPN or VPN/VGW through the Virtual Private Cloud service. Prepare an instance of MySQL running external to Amazon RDS. Configure the MySQL DB instance to be the replication source. Use mysqldump to transfer the database from the Amazon RDS instance to the on-premises MySQL instance and start the replication from the Amazon RDS Read Replica.</p>"
			},
			{
				"correct": false,
				"answer": "<p>RDS cannot replicate to an on-premises database server. Instead, configure the RDS instance to replicate to an EC2 instance with core MySQL and then configure replication over a secure VPN/VPG connection.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Configure the RDS instance as the master and enable replication over the open Internet using an SSL endpoint to the on-premises server. Use&nbsp; <code>mysqldump</code> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. Use&nbsp; <code>mysqldump</code> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Amazon supports <strong>Internet Protocol security (IPsec) VPN</strong> connections. IPsec is a protocol suite for securing Internet Protocol (IP) communications by authenticating and encrypting each IP packet of a data stream. Data transferred between your VPC and datacenter routes over an encrypted VPN connection to help maintain the confidentiality and integrity of data in transit. An Internet gateway is not required to establish a hardware VPN connection.</p><p>You can set up replication between an <strong>Amazon RDS MySQL</strong> (or MariaDB DB instance) that is running in AWS and a MySQL (or MariaDB instance) to your on-premises data center. Replication to an instance of MySQL running external to Amazon RDS is only supported during the time it takes to export a database from a MySQL DB instance.</p><p><img src=\"https://media.tutorialsdojo.com/sap_mysql_rds_vpn_replication.png\"></p><p>To allow communication between RDS and to your on-premises network, you must first set up a VPN or an AWS <em>Direct Connect</em> connection. Once that is done, just follow the below the steps to perform the replication:</p><p>Prepare an instance of MySQL running external to Amazon RDS.</p><p>Configure the MySQL DB instance to be the replication source.</p><p>Use <code><strong>mysqldump</strong></code> to transfer the database from the Amazon RDS instance to the instance external to Amazon RDS (e.g. on-premises server)</p><p>Start replication to the instance running external to Amazon RDS.</p><p>The option that says: <strong>Create an IPSec VPN connection using either OpenVPN or VPN/VGW through the Virtual Private Cloud service. Prepare an instance of MySQL running external to Amazon RDS. Configure the MySQL DB instance to be the replication source. Use mysqldump to transfer the database from the Amazon RDS instance to the on-premises MySQL instance and start the replication from the Amazon RDS Read Replica</strong> is correct because it is feasible to set up the secure IPSec VPN connection between the on-premises server and AWS VPC using the VPN/Gateways.</p><p>The option that says: <strong>Configure the RDS instance as the master and enable replication over the open Internet using an SSL endpoint to the on-premises server. Use </strong><code><strong>mysqldump</strong></code><strong> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication</strong> is incorrect because SSL endpoint cannot be utilized here as it is only used to securely access the database.</p><p>The option that says: <strong>RDS cannot replicate to an on-premises database server. Instead, configure the RDS instance to replicate to an EC2 instance with core MySQL and then configure replication over a secure VPN/VPG connection</strong> is incorrect because you do not need to establish a secure VPN/VPG connection in the first place as EC2 and RDS are both in AWS Cloud.</p><p>The option that says: <strong>Create a Data Pipeline that exports the MySQL data each night and securely downloads the data from an S3 HTTPS endpoint. Use </strong><code><strong>mysqldump</strong></code><strong> to transfer the database from the Amazon S3 to the on-premises MySQL instance and start the replication</strong> is incorrect because Data Pipeline is for batch jobs and is not suitable for this scenario.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Exporting.NonRDSRepl.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MySQL.Procedural.Exporting.NonRDSRepl.html</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A corporate investment bank runs a loan origination application in AWS that is hosted on an EBS-backed Reserved EC2 instance. The instance stores financial data which contains highly sensitive information. For regulatory compliance, the data should be durable and highly available with an expected RTO of less than a minute in case of failure.</p><p>Which of the following options is the recommended EBS architecture that will meet the above requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Set up a RAID 1 EBS configuration to store the financial data."
			},
			{
				"correct": false,
				"answer": "Set up a RAID 6 EBS configuration to store the financial data."
			},
			{
				"correct": false,
				"answer": "Set up a RAID 0 EBS configuration to store the financial data."
			},
			{
				"correct": false,
				"answer": "Set up a RAID 5 EBS configuration to store the financial data."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>With <strong>Amazon EBS</strong>, you can select any of the standard RAID configurations that you can use with a traditional bare metal server, as long as that particular RAID configuration is supported by the operating system for your instance. This is because all RAID is accomplished at the software level.</p><p>For greater I/O performance than you can achieve with a single volume, RAID 0 can stripe multiple volumes together; for on-instance redundancy, RAID 1 can mirror two volumes together. Amazon EBS volume data is replicated across multiple servers in an Availability Zone to prevent the loss of data from the failure of any single component. This replication makes Amazon EBS volumes ten times more reliable than typical commodity disk drives.</p><p><img src=\"https://media.tutorialsdojo.com/sap_raid0_vs_raid1.png\"></p><p>RAID 5 and RAID 6 are not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the IOPS available to your volumes. Depending on the configuration of your RAID array, these RAID modes provide 20-30% fewer usable IOPS than a RAID 0 configuration. The increased cost is a factor with these RAID modes as well; when using identical volume sizes and speeds, a 2-volume RAID 0 array can outperform a 4-volume RAID 6 array that costs twice as much.</p><p>Therefore, the correct answer is: <strong>Set up a RAID 1 EBS configuration to store the financial data.</strong> RAID 1 is recommended when you need fault tolerance for your EBS volumes. <br></p><p>The option that says: <strong>Set up a RAID 0 EBS configuration to store the financial data</strong> is incorrect. The main requirement here is to increase the fault tolerance of the EBS volume, not to increase I/O performance so RAID 1 is preferred.</p><p>The option that says: <strong>Set up a RAID 5 EBS configuration to store the financial data</strong> is incorrect. RAID 5 and RAID 6 are not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the IOPS available to your volumes.</p><p>The option that says: <strong>Set up a RAID 6 EBS configuration to store the financial data</strong> is incorrect. RAID 5 and RAID 6 are not recommended for Amazon EBS because the parity write operations of these RAID modes consume some of the IOPS available to your volumes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/raid-config.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/snapshot-ebs-raid-array/\">https://aws.amazon.com/premiumsupport/knowledge-center/snapshot-ebs-raid-array/</a></p><p><br></p><p><strong>Check out this Amazon EC2 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A retail company runs its customer support call system on its in-house data center. The Solutions Architect was tasked to migrate the call system to AWS and leverage the use of managed services to reduce management overhead. The solution must be able to handle the current tasks such as receiving calls and creating contact flows. It must be able to scale to handle more calls as the customer base grows. The company also wants to add deep learning capabilities to the call system to reduce the need to speak to an agent. It must be able to recognize the intent of the caller based on certain keywords and handle basic tasks, as well as provide information to the call center agents.</p><p>Which combination of actions should the Solutions Architect implement to meet the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Build a conversational interface on Amazon Alexa for Business to have AI-based answers to customer queries thereby reducing the need to speak to an agent.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Leverage Amazon Rekognition to identify the caller as well as recognize the intent of the customer based on their voice.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Send the incoming customer calls to an Amazon Kinesis stream and process their voice on Amazon Polly to recognize their intent.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the Amazon Connect service to create an omnichannel cloud-based contact center for the agents.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Buffer the incoming customer calls to an Amazon SQS queue and process their voice on Amazon Lex to recognize their intent.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Connect</strong> is an easy to use omnichannel cloud contact center that helps companies provide superior customer service across voice, chat, and tasks at a lower cost than traditional contact center systems. You can set up a contact center in a few steps, add agents who are located anywhere, and start engaging with your customers.</p><p>You can create personalized experiences for your customers using omnichannel communications. For example, you can dynamically offer chat and voice contact, based on such factors as customer preference and estimated wait times. Agents, meanwhile, conveniently handle all customers from just one interface. For example, they can chat with customers, and create or respond to tasks as they are routed to them.</p><p>The following diagram shows these key characteristics of Amazon Connect.</p><p><img src=\"https://media.tutorialsdojo.com/sap_amazon_connect.png\"></p><p>To help provide a better contact center, you can use Amazon Connect to integrate with several AWS services to provide Machine Learning (ML) and Artificial Intelligence (AI) capabilities.</p><p>Amazon Connect uses the following services for ML/AI:</p><p><strong>Amazon Lex</strong>—Lets you create a chatbot to use as Interactive Voice Response (IVR).</p><p><strong>Amazon Polly</strong>—Provides text-to-speech in all contact flows.</p><p><strong>Amazon Transcribe</strong>—Grabs conversation recordings from Amazon S3, and transcribes them to text so you can review them.</p><p><strong>Amazon Comprehend</strong>—Takes the transcription of recordings, and applies speech analytics machine learning to the call to identify sentiment, keywords, adherence to company policies, and more.</p><p><strong>Amazon Lex</strong> is a service for building conversational interfaces into any application using voice and text. Amazon Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, to enable you to build applications with highly engaging user experiences and lifelike conversational interactions.</p><p>By using an Amazon Lex chatbot in your call center, callers can perform tasks such as changing a password, requesting a balance on an account, or scheduling an appointment, without needing to speak to an agent. These chatbots use automatic speech recognition and natural language understanding to ascertain a caller’s intent, maintain context and fluidly manage the conversation. Amazon Lex uses AWS Lambda functions to query your business applications, provide information back to callers, and make updates as requested.</p><p>The option that says: <strong>Use the Amazon Connect service to create an omnichannel cloud-based contact center for the agents</strong> is correct. Amazon Connect is the AWS service that you will want to use if you want to build a cloud-based call center system.</p><p>The option that says: <strong>Buffer the incoming customer calls to an Amazon SQS queue and process their voice on Amazon Lex to recognize their intent</strong> is correct. Amazon Lex has deep learning functionalities of automatic speech recognition as well as natural language understanding (NLU) to recognize the intent of the text.</p><p>The option that says: <strong>Leverage Amazon Rekognition to identify the caller as well as recognize the intent of the customer based on their voice</strong> is incorrect. Amazon Rekognition is used to identify persons on photos or videos, not on voice calls. You should use Amazon Lex for this.</p><p>The option that says: <strong>Build a conversational interface on Amazon Alexa for Business to have AI-based answers to customer queries thereby reducing the need to speak to an agent</strong> is incorrect. Alexa for Business gives you the tools you need to manage Alexa devices, enroll your users, and assign skills, at scale for your organization. For conversational interfaces such as chatbots, you can use Amazon Lex.</p><p>The option that says: <strong>Send the incoming customer calls to an Amazon Kinesis stream and process their voice on Amazon Polly to recognize their intent</strong> is incorrect. Amazon Polly provides text-to-speech services. It can’t recognize or interpret the intent of the customer based on their voice. The scenario requires understanding the intent of the user based on their speech so Amazon Lex is better suited for this.</p><p><strong><br>References:</strong></p><p><a href=\"https://aws.amazon.com/connect/\">https://aws.amazon.com/connect/</a></p><p><a href=\"https://docs.aws.amazon.com/connect/latest/adminguide/related-services-amazon-connect.html\">https://docs.aws.amazon.com/connect/latest/adminguide/related-services-amazon-connect.html</a></p><p><a href=\"https://aws.amazon.com/lex/\">https://aws.amazon.com/lex/</a></p><p><a href=\"https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html\">https://docs.aws.amazon.com/lexv2/latest/dg/what-is.html</a></p><p><br></p><p><strong>Check out the Amazon Lex Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-lex/?src=udemy\">https://tutorialsdojo.com/amazon-lex/</a></p></div>"
	},
	{
		"question": "<p>A multinational consumer goods company is currently using a VMWare vCenter Server to manage their virtual machines, multiple ESXi hosts, and all dependent components from a single centralized location. To save costs and to avail the benefits of cloud computing, the company decided to move their virtual machines to AWS. The Solutions Architect is required to generate new AMIs of all virtual machines which can be launched as an EC2 instance in the company VPC.</p><p>Which combination of steps should the Solutions Architect do to properly execute the cloud migration? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use Serverless Application Model (SAM) to migrate the virtual machines (VMs) to AWS and automatically launch an Amazon ECS Cluster to host the VMs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Establish a Direct Connect connection between your data center and your VPC. Use AWS Service Catalog to centrally manage all your IT services and to quickly migrate virtual machines to your virtual private cloud.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use CodePipeline to migrate your on-premises workloads to AWS</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use the AWS Server Migration Service (SMS) to migrate your on-premises workloads to AWS </p>"
			},
			{
				"correct": true,
				"answer": "<p>Install the Server Migration Connector to your on-premises virtualization environment.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>It is recommended by AWS to use the Server Migration Service (SMS) to migrate VMs from a vCenter environment to AWS. SMS automates the migration process by replicating on-premises VMs incrementally and converting them to Amazon machine images (AMIs). You can continue using your on-premises VMs while the migration is in progress. The Server Migration Connector is a FreeBSD VM that you install in your on-premises virtualization environment.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_sms.png\"></p><p>Therefore, the correct answers are:</p><p><strong>- Use the AWS Server Migration Service (SMS)</strong></p><p><strong>- Install the Server Migration Connector to your on-premises virtualization environment</strong></p><p>The option that says: <strong>Use Serverless Application Model (SAM) to migrate the virtual machine (VMs) to AWS and</strong> <strong>automatically launch an Amazon ECS Cluster to host the VMs</strong> is incorrect because the Serverless Application Model (SAM) service is primarily used to build serverless applications on AWS, and not to migrate virtual machines from your on-premises data center.</p><p>The option that says: <strong>Use CodePipeline to migrate your on-premises workloads to AWS</strong> is incorrect. CodePipeline is not suitable for migrating your virtual machines to AWS. You have to use the AWS Server Migration Service instead.</p><p>The option that says: <strong>Establish a Direct Connect connection between your data center and your VPC. Use AWS Service Catalog to centrally manage all your IT services and to quickly migrate virtual machines to your virtual private cloud</strong> is incorrect. The AWS Service Catalog is primarily used to allow organizations to create and manage catalogs of IT services that are approved for use on AWS. It enables you to centrally manage commonly deployed IT services and helps you achieve consistent governance and meet your compliance requirements while enabling users to quickly deploy only the approved IT services they need. However, this service is not suitable for migrating your virtual machines.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html\">https://docs.aws.amazon.com/server-migration-service/latest/userguide/server-migration.html</a></p><p><a href=\"https://aws.amazon.com/server-migration-service/\">https://aws.amazon.com/server-migration-service/</a></p><p><br></p><p><strong>Check out this AWS Server Migration Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-server-migration-service-sms/?src=udemy\">https://tutorialsdojo.com/aws-server-migration-service-sms/</a></p></div>"
	},
	{
		"question": "<p>A retail company has an online shopping website that provides cheap bargains and discounts on various products. The company has recently moved its infrastructure from its previous hosting provider to AWS. The architecture uses an Application Load Balancer (ALB) in front of an Auto Scaling group of Spot and On-Demand EC2 instances. The solutions architect must set up a CloudFront web distribution that uses a custom domain name and the origin should point to the new ALB.</p><p>Which of the following options is the correct implementation of an end-to-end HTTPS connection from the origin to the CloudFront viewers?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to HTTPS Only in CloudFront, then use an SSL/TLS certificate from a third-party certificate authority which was imported to S3."
			},
			{
				"correct": false,
				"answer": "Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to Match Viewer to support both HTTP or HTTPS in CloudFront then use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store."
			},
			{
				"correct": false,
				"answer": "<p>Upload a self-signed certificate in the ALB. Set the Viewer Protocol Policy to <code>HTTPS Only</code> in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Import a certificate that is signed by a trusted third-party certificate authority, store it to ACM then attach it in your ALB. Set the Viewer Protocol Policy to HTTPS Only in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Remember that there are rules on which type of SSL Certificate to use if you are using an EC2 or an ELB as your origin. This question is about setting up an end-to-end HTTPS connection between the Viewers, CloudFront, and your custom origin, which is an ALB instance.</p><p>The certificate issuer you must use depends on whether you want to require HTTPS between viewers and CloudFront or between CloudFront and your origin:</p><p><strong>HTTPS between viewers and CloudFront</strong></p><p>- You can use a certificate that was issued by a trusted certificate authority (CA) such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- You can use a certificate provided by AWS Certificate Manager (ACM)</p><p><strong>HTTPS between CloudFront and a custom origin</strong></p><p>- If the origin is not an ELB load balancer, such as Amazon EC2, the certificate must be issued by a trusted CA such as Comodo, DigiCert, Symantec or other third-party providers.</p><p>- If your origin is an ELB load balancer, you can also use a certificate provided by ACM.</p><p><br></p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_ssl.jpg\"></p><p><br></p><p>If you're using your own domain name, such as tutorialsdojo.com, you need to change several CloudFront settings. You also need to use an SSL/TLS certificate provided by AWS Certificate Manager (ACM), or import a certificate from a third-party certificate authority into ACM or the IAM certificate store. Lastly, you should set the Viewer Protocol Policy to HTTPS Only in CloudFront.</p><p>Hence, the option that says: <strong>Import a certificate that is signed by a trusted third-party certificate authority, store it to ACM then attach it in your ALB. Set the Viewer Protocol Policy to HTTPS Only in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store</strong> is the correct answer in this scenario.</p><p>The option that says: <strong>Upload a self-signed certificate in the ALB. Set the Viewer Protocol Policy to </strong><code><strong>HTTPS only</strong></code><strong> in CloudFront and use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store</strong> is incorrect because you cannot directly upload a self-signed certificate in your ALB.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to Match Viewer to support both HTTP or HTTPS in CloudFront then use an SSL/TLS certificate from a third-party certificate authority which was imported to either ACM or the IAM certificate store</strong> is incorrect because you have to set the Viewer Protocol Policy to <code>HTTPS Only</code>.</p><p>The option that says: <strong>Use a certificate that is signed by a trusted third-party certificate authority in the ALB, which is then imported into ACM. Set the Viewer Protocol Policy to HTTPS Only in CloudFront, then use an SSL/TLS certificate from a third-party certificate authority which was imported to S3 </strong>is incorrect because you cannot use an SSL/TLS certificate from a third-party certificate authority which was imported to S3.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cnames-and-https-requirements.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A big fast-food chain in Asia is planning to implement a location-based alert on their existing mobile app. If a user is in proximity to one of its restaurants, an alert will be shown on the user’s mobile phone. The notification needs to happen in less than a minute while the user is still in the vicinity. Currently, the mobile app has 10 million users in the Philippines, China, Korea, and other Asian countries.</p><p>Which one of the following AWS architecture is the most suitable option for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Establish connectivity with mobile carriers using AWS Direct Connect. Set up an API on all EC2 instances to receive the location data from the mobile app via the carrier's GPS connection. Use RDS to store the data and fetch relevant offers from the restaurant. The EC2 instances will communicate with mobile carriers to send alerts to the mobile app.</p>"
			},
			{
				"correct": false,
				"answer": "Set up an API that uses an Application Load Balancer and an Auto Scaling group of EC2 instances. The mobile app will send the user's location data to the API web service. Use DynamoDB to store and retrieve relevant offers on the nearest restaurant. Configure the EC2 instances to push alerts to the mobile app."
			},
			{
				"correct": true,
				"answer": "The mobile app will send device location to an SQS endpoint. Set up an API that utilizes an Application Load Balancer and an Auto Scaling group of EC2 instances, which will retrieve the relevant offers from DynamoDB. Use AWS Mobile Push to send offers to the mobile app."
			},
			{
				"correct": false,
				"answer": "<p>The mobile app will send the real-time location data using Amazon Kinesis. Set up an API which uses an Application Load Balancer and an Auto Scaling group of EC2 instances to retrieve the relevant offers from a DynamoDB table. Use Amazon Lambda and SES to push the notification to the mobile app.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Simple Notification Service (SNS)</strong> is a highly available, durable, secure, fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email.</p><p>With the Mobile Push feature of Amazon SNS, you have the ability to send push notification messages directly to apps on mobile devices. Push notification messages sent to a mobile endpoint can appear in the mobile app as message alerts, badge updates, or even sound alerts.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sns_publisher.png\"></p><p>You send push notification messages to both mobile devices and desktops using one of the following supported push notification services:</p><p>- Amazon Device Messaging (ADM)</p><p>- Apple Push Notification Service (APNS) for both iOS and Mac OS X</p><p>- Baidu Cloud Push (Baidu)</p><p>- Google Cloud Messaging for Android (GCM)</p><p>- Microsoft Push Notification Service for Windows Phone (MPNS)</p><p>- Windows Push Notification Services (WNS)</p><p>Therefore, the correct answer is: <strong>The mobile app will send device location to an SQS endpoint. Set up an API that utilizes an Application Load Balancer and an Auto Scaling group of EC2 instances, which will retrieve the relevant offers from DynamoDB. Use AWS Mobile Push to send offers to the mobile app.</strong></p><p>The option that says: <strong>Set up an API that uses an Application Load Balancer and an Auto Scaling group of EC2 instances. The mobile app will send the user's location data to the API web service. Use DynamoDB to store and retrieve relevant offers on the nearest restaurant. Configure the EC2 instances to push alerts to the mobile app </strong>is incorrect. Using EC2 instances to push alerts to the mobile app is not an appropriate solution. You have to use the AWS Mobile Push feature of SNS.</p><p>The option that says:<strong> Establish connectivity with mobile carriers using AWS Direct Connect. Set up an API on all EC2 instances to receive the location data from the mobile app via the carrier's GPS connection. Use RDS to store the data and fetch relevant offers from the restaurant. The EC2 instances will communicate with mobile carriers to send alerts to the mobile app </strong>is incorrect. AWS Direct Connect is primarily used to establish a dedicated network connection from your premises to AWS.</p><p>The option that says: <strong>The mobile app will send the real-time location data using Amazon Kinesis. Set up an API which uses an Application Load Balancer and an Auto Scaling group of EC2 instances to retrieve the relevant offers from a DynamoDB table. Use Amazon Lambda and SES to push the notification to the mobile app</strong> is incorrect. You can't use SES to send push notifications to mobile phones. You have to use SNS instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html\">https://docs.aws.amazon.com/sns/latest/dg/SNSMobilePush.html</a></p><p><a href=\"https://docs.aws.amazon.com/sns/latest/dg/mobile-push-send.html\">https://docs.aws.amazon.com/sns/latest/dg/mobile-push-send.html</a></p><p><br></p><p><strong>Check out this Amazon SNS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sns/?src=udemy\">https://tutorialsdojo.com/amazon-sns/</a></p></div>"
	},
	{
		"question": "<p>A startup is developing a health-related mobile app for both iOS and Android devices. The co-founder developed a sleep tracking app that collects the user's biometric data then stores them in an Amazon DynamoDB table, which is configured with an on-demand provisioned throughput capacity. Every nine in the morning, a scheduled task scans the DynamoDB table to extract and aggregate last night’s data for each user and stores the results in an Amazon S3 bucket. When the new data is available, the users are then notified via Amazon SNS mobile push notifications. Due to budget constraints, the management wants to optimize the current architecture of the backend system to lower costs and increase the overall revenue.</p><p>Which of the following options can the solutions architect implement to further lower the cost in AWS? (Select TWO.)</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Set up a scheduled job to drop the DynamoDB table for the previous day that contains the biometric data after it is successfully stored in the S3 bucket. Create another DynamoDB table for the day and perform the deletion and creation process everyday."
			},
			{
				"correct": true,
				"answer": "Avail a reserved capacity for provisioned throughput for DynamoDB."
			},
			{
				"correct": false,
				"answer": "Launch a Redshift cluster to replace Amazon DynamoDB. Switch from a Standard S3 bucket to One Zone-Infrequent Access storage class."
			},
			{
				"correct": false,
				"answer": "Use ElastiCache to cache reads and writes from the DynamoDB table."
			},
			{
				"correct": false,
				"answer": "Use a RDS instance configured with Multi-AZ deployments and Read Replicas as a replacement to your DynamoDB."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can purchase reserved capacity in advance to lower the costs of running your DynamoDB instance. With reserved capacity, you pay a one-time upfront fee and commit to a minimum usage level over a period of time. By reserving your read and write capacity units ahead of time, you realize significant cost savings compared to on-demand provisioned throughput settings. In addition, you can also drop the DynamoDB table which contains the biometric data right after it is successfully stored in S3.</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamoDB_auto_scaling.png\"></p><p>The option that says: <strong>Avail a reserved capacity for provisioned throughput for DynamoDB</strong> is correct. If you can predict your need for Amazon DynamoDB read-and-write throughput, reserved capacity offers significant savings over the normal price of DynamoDB provisioned throughput capacity.</p><p>The option that says: <strong>Set up a scheduled job to drop the DynamoDB table for the previous day that contains the biometric data after it is successfully stored in the S3 bucket. Create another DynamoDB table for the day and perform the deletion and creation process everyday</strong> is correct. This saves costs by not storing too much data on DynamoDB tables.</p><p>The option that says: <strong>Use a RDS instance configured with Multi-AZ deployments and Read Replicas as a replacement to your DynamoDB</strong> is incorrect because using an RDS database with Multi-AZ deployments and Read Replicas will actually cost more than the current architecture. The scenario just wants to lower the cost and it didn't specify the need to shift to a NoSQL database.</p><p>The option that says: <strong>Launch a Redshift cluster to replace Amazon DynamoDB. Switch from a Standard S3 bucket to One Zone-Infrequent Access storage class</strong> is incorrect because Redshift is primarily used for online analytical processing (OLAP) applications and not as a NoSQL database. This change also entails a significant amount of time and resources to execute.</p><p>The option that says: <strong>Use ElastiCache to cache reads and writes from the DynamoDB table</strong> is incorrect. Although an ElastiCache cluster can lower the CPU utilization of your EC2 instances and improve application performance, it doesn't provide significant cost reduction as compared to Options 3 and 5. Take note that this will also increase the cost since you have to pay for your ElastiCache cluster.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html#HowItWorks.ProvisionedThroughput.ReservedCapacity\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html#HowItWorks.ProvisionedThroughput.ReservedCapacity</a></p><p><a href=\"https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/amazon-dynamodb-reservations.html\">https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-reservation-models/amazon-dynamodb-reservations.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html</a></p><p><br></p><p><strong>Check out this Amazon DynamoDB Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-dynamodb/?src=udemy\">https://tutorialsdojo.com/amazon-dynamodb/</a></p></div>"
	},
	{
		"question": "<p>A multinational consumer goods company runs its website entirely on its on-premises data center. Due to the unprecedented growth of their popular product, they are expecting an increase in incoming traffic to their website in the coming days ahead. The CTO requested to urgently do the necessary architectural changes to be able to handle the demand. The solutions architect suggested migrating the application to AWS but the CTO decided that they need at least 3 months to implement a hybrid cloud architecture.</p><p>What could the solutions architect do with the current on-premises website to help offload some of the traffic and scale out to meet the demand in a cost-effective way?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use an S3 bucket to host all the static files of the website and create a CloudFront RTMP distribution for serving the static files."
			},
			{
				"correct": false,
				"answer": "<p>Use OpsWorks to integrate AWS with the on-premises website and to manage and configure the servers with Auto Scaling to meet the demand. </p>"
			},
			{
				"correct": false,
				"answer": "<p>Replicate the current web infrastructure of the on-premises website on AWS. Offload the DNS to Route 53 and configure weight-based DNS routing to send 50% of the traffic to AWS.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Launch a CloudFront web distribution with the URL of the on-premises web application as the origin. Offload the DNS to AWS to handle CloudFront traffic.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudFront</strong> is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your content through a worldwide network of data centers called edge locations. When a user requests content that you're serving with CloudFront, the request is routed to the edge location that provides the lowest latency (time delay), so that content is delivered with the best possible performance.</p><p>If the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.</p><p>If the content is not in that edge location, CloudFront retrieves it from an origin that you've defined—such as an Amazon S3 bucket, a MediaPackage channel, or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.</p><p>You can use CloudFront with the on-premises website as the origin. CloudFront is a highly available, scalable service that can cache frequently accessed files on the website and can significantly make the load times faster.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront.png\"></p><p>Therefore, the correct answer is: <strong>Launch a CloudFront web distribution with the URL of the on-premises web application as the origin. Offload the DNS to AWS to handle CloudFront traffic.</strong></p><p>The option that says: <strong>Use OpsWorks to integrate AWS with the on-premises website and to manage and configure the servers with Auto Scaling to meet the demand</strong> is incorrect as this is not a cost-effective solution. OpsWorks can be integrated with their on-premises servers but setting up the EC2 instances with Auto Scaling will have a significant cost.</p><p>The option that says: <strong>Use an S3 bucket to host all the static files of the website and create a CloudFront RTMP distribution for serving the static files</strong> is incorrect as an RTMP distribution is not suitable in this scenario. Instead, you have to use a web distribution in CloudFront.</p><p>The option that says: <strong>Replicate the current web infrastructure of the on-premises website on AWS. Offload the DNS to Route 53 and configure weight-based DNS routing to send 50% of the traffic to AWS</strong> is incorrect as this option is time-consuming and you don't have enough time to replicate the entire architecture of the on-premises website.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/\">https://aws.amazon.com/blogs/aws/amazon-cloudfront-support-for-custom-origins/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p></div>"
	},
	{
		"question": "<p>A local media company has an on-premises Active Directory service for its 100 workstations. The company decided to deploy virtual desktops for their new employees in a virtual private cloud in AWS to save additional expenses from purchasing physical workstations. The new cloud infrastructure should leverage the existing security controls in AWS but can still communicate with their on-premises network.</p><p>Which set of AWS services should the solutions architect use to meet these requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>AWS Directory Services, VPN connection, and Amazon ES.</p>"
			},
			{
				"correct": false,
				"answer": "<p>AWS Directory Services, VPN connection, and QuickSight.</p>"
			},
			{
				"correct": false,
				"answer": "<p>AWS Directory Services, VPN connection, and Rekognition.</p>"
			},
			{
				"correct": true,
				"answer": "<p>AWS Directory Services, VPN connection, and Amazon Workspaces.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>First, you need a VPN connection to connect the VPC and your on-premises network. <strong>AWS VPN</strong> can establish secure connections between your on-premises networks, remote offices, client devices, and the AWS global network.</p><p>Second, you need <strong>AWS Directory Services</strong> to integrate with your on-premises Active Directory. AWS Directory Service for Microsoft Active Directory, also known as AWS Managed Microsoft Active Directory (AD), enables your directory-aware workloads and AWS resources to use managed Active Directory (AD) in AWS. AWS Managed Microsoft AD makes it easy to extend your existing Active Directory to AWS.</p><p>Lastly, you need to use <strong>Amazon WorkSpaces</strong> to create the needed virtual desktops in your VPC. Amazon WorkSpaces is a fully managed, persistent desktop virtualization service that enables your users to access the data, applications, and resources they need, anywhere, anytime, from any supported device. You can use Amazon WorkSpaces to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_directory_service.png\"></p><p>Therefore, the correct answer is: <strong>AWS Directory Services, VPN connection, and Amazon Workspaces.</strong></p><p>The option that says: <strong>AWS Directory Services, VPN connection, and Amazon ES</strong> is incorrect because the Amazon Elasticsearch Service (Amazon ES) is just a fully managed service that makes it easy for you to deploy, secure, and operate Elasticsearch at scale with zero downtime. This service is not appropriate for this scenario.</p><p>The option that says: <strong>AWS Directory Services, VPN connection, and Rekognition</strong> is incorrect because Amazon Rekognition only makes it easy for you to add image and video analysis to your applications. This service will not help you meet the requirement.</p><p>The option that says: <strong>AWS Directory Services, VPN connection, and QuickSight</strong> is incorrect because Amazon QuickSight is primarily used to deliver visual insights to everyone in your organization using dashboards.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/directoryservice/\">https://aws.amazon.com/directoryservice/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html\">https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html</a></p><p><a href=\"https://aws.amazon.com/workspaces/\">https://aws.amazon.com/workspaces/</a></p><p><br></p><p><strong>Check out these AWS Directory Service and Amazon Workspaces Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-directory-service/?src=udemy\">https://tutorialsdojo.com/aws-directory-service/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-workspaces/?src=udemy\">https://tutorialsdojo.com/amazon-workspaces/</a></p><p><br></p><p><strong>Introduction to AWS Directory Service:</strong></p><p><a href=\"https://youtu.be/4XeqotTYBtY\">https://youtu.be/4XeqotTYBtY</a></p></div>"
	},
	{
		"question": "<p>A fintech startup has several resources provisioned on the AWS cloud. The majority of the company’s compute clusters are composed of an Application Load Balancer (ALB) in front of an Auto Scaling group of On-Demand Amazon EC2 instances. To lower down the overall cost, the management wants to have one EC2 instance terminated whenever the overall CPU utilization of the cluster is at 15% or lower.</p><p>Which of the following options should the solutions architect implement for a cost-effective and scalable architecture that satisfies the company requirement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Use CloudWatch for the monitoring and configure the scaling in policy of the Auto Scaling group to terminate one EC2 instance when the CPU Utilization is 15% or below."
			},
			{
				"correct": false,
				"answer": "Use scheduled actions in the Auto Scaling configuration to automatically terminate EC2 instances when the CPU Utilization hits below 15%."
			},
			{
				"correct": false,
				"answer": "Use AWS Lambda triggers to send a notification to the Auto Scaling group when the CPU utilization is less than 15% to kick off the scaling in policy to remove the EC2 instance."
			},
			{
				"correct": false,
				"answer": "Configure a monitoring script that sends out an email using SNS when the CPU utilization is less than 15% so the administrator can manually remove an EC2 instance."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Target tracking scaling policies</strong> simplify how you configure dynamic scaling. You select a predefined metric or configure a customized metric, and set a target value. <strong>Amazon EC2 Auto Scaling</strong> creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value. The scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value. In addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to the fluctuations in the metric due to a fluctuating load pattern and minimizes rapid fluctuations in the capacity of the Auto Scaling group.</p><p><img src=\"https://media.tutorialsdojo.com/sap_autoscaling_group.png\"></p><p>For example, you could use target tracking scaling to:</p><p>- Configure a target tracking scaling policy to keep the average aggregate CPU utilization of your Auto Scaling group at 50 percent.</p><p>- Configure a target tracking scaling policy to keep the request count per target of your Elastic Load Balancing target group at 1000 for your Auto Scaling group.</p><p>Therefore, the correct answer is: <strong>Use CloudWatch for the monitoring and configure the scaling in policy of the Auto Scaling group to terminate one EC2 instance when the CPU Utilization is 15% or below.</strong></p><p>The option that says: <strong>Use scheduled actions in the Auto Scaling configuration to automatically terminate EC2 instances when the CPU Utilization hits below 15%</strong> is incorrect. You can't use scheduled actions because the CPU usage of the instances can be unpredictable depending on the network traffic.</p><p>The option that says: <strong>Configure a monitoring script that sends out an email using SNS when the CPU utilization is less than 15% so the administrator can manually remove an EC2 instance</strong> is incorrect. You don't have to configure your own monitoring script. Amazon CloudWatch has integration with Amazon SNS to send scaling notifications to you.</p><p>The option that says: <strong>Use AWS Lambda triggers to send a notification to the Auto Scaling group when the CPU utilization is less than 15% to kick off the scaling in policy to remove the EC2 instance</strong> is incorrect. This may be possible but you don't have to create your own Lambda triggers. The Auto Scaling group has built-in functionality to automatically remove an EC2 instance depending on your CPU threshold setting.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-monitoring.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-policy-cloudwatch-alarm/\">https://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-policy-cloudwatch-alarm/</a></p><p><a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-monitoring-features.html\">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-monitoring-features.html</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p></div>"
	},
	{
		"question": "<p>A credit company deployed its online load application system in an Auto Scaling group across multiple Availability Zones in the ap-southeast-2 region. As part of the Disaster Recovery Plan of the company, the target RTO must be less than 2 hours and the target RPO must be 10 minutes. At 12:00 PM, there was a production incident in the main database and the operations team found out that they cannot recover the transactions made from 10:30 AM onwards or 1.5 hours ago.</p><p>How can the solutions architect change the current architecture to achieve the required RTO and RPO in case a similar system failure occurred in the future?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Improve data redundancy by implementing a synchronous database master-slave replication between two Availability Zones."
			},
			{
				"correct": true,
				"answer": "<p>Create database backups every hour and store it in an S3 bucket<strong><em> </em></strong>with Cross-Region Replication enabled. Store the transaction logs in the same S3 bucket every 5 minutes.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Perform database backups every hour and store the result to EBS volumes. Backup the transaction logs every 5 minutes to an S3 bucket with Cross-Region Replication enabled.</p>"
			},
			{
				"correct": false,
				"answer": "Create database backups every hour and store it to Glacier for archiving. Store the transaction logs in an S3 bucket every 5 minutes."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Businesses of all sizes are using AWS to enable faster disaster recovery of their critical IT systems without incurring the infrastructure expense of a second physical site. AWS supports many disaster recovery architectures, from those built for smaller workloads to enterprise solutions that enable rapid failover at scale. AWS provides a set of cloud-based disaster recovery services that enable fast recovery of your IT infrastructure and data.</p><p><strong>Recovery time objective (RTO)</strong> - The time it takes after a disruption to restore a business process to its service level, as defined by the operational level agreement (OLA). For example, if a disaster occurs at 12:00 PM (noon) and the RTO is eight hours, the DR process should restore the business process to the acceptable service level by 8:00 PM.</p><p><strong>Recovery point objective (RPO)</strong> - The acceptable amount of data loss measured in time. For example, if a disaster occurs at 12:00 PM (noon) and the RPO is one hour, the system should recover all data that was in the system before 11:00 AM. Data loss will span only one hour, between 11:00 AM and 12:00 PM (noon).</p><p><img src=\"https://media.tutorialsdojo.com/sap_s3_versioning_enable.png\"></p><p>A company typically decides on an acceptable RTO and RPO based on the financial impact to the business when systems are unavailable. The company determines financial impact by considering many factors, such as the loss of business and damage to its reputation due to downtime and the lack of systems availability. IT organizations then plan solutions to provide cost-effective system recovery based on the RPO within the timeline and the service level established by the RTO.</p><p><strong>Cross-region replication (CRR)</strong> enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Buckets configured for cross-region replication can be owned by the same AWS account or by different accounts. This is also helpful to durably store your data and for disaster recovery in the event of a region-wide outage.</p><p>Therefore, the correct answer is: <strong>Create database backups every hour and store it in an S3 bucket with Cross-Region Replication enabled. Store the transaction logs in the same S3 bucket every 5 minutes.</strong></p><p>The option that says: <strong>Improve data redundancy by implementing a synchronous database master-slave replication between two Availability Zones</strong> is incorrect. Although this is a valid answer, it will not be able to provide the needed data redundancy in the event of a region-wide outage. If one Availability Zone goes down, then there is another Availability Zone that contains the data. However, if the whole AWS region is down, then the data will be totally unreachable. A better solution is to use S3 and enable Cross-Region Replication (CRR).</p><p>The option that says: <strong>Perform database backups every hour and store the result to EBS volumes. Backup the transaction logs every 5 minutes to an S3 bucket with Cross-Region Replication enabled</strong> is incorrect. EBS Volumes are not durable enough to store the database backups every hour for this scenario. If there is an AZ or an AWS Region-wide outage, then the data in the EBS Volume could potentially be unavailable.</p><p>The option that says: <strong>Create database backups every hour and store it to Glacier for archiving. Store the transaction logs in an S3 bucket every 5 minutes</strong> is incorrect because Glacier is primarily used for long term data archiving.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/disaster-recovery/\">https://aws.amazon.com/disaster-recovery/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html\">https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html</a></p><p><a href=\"https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html\">https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/plan-for-disaster-recovery-dr.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A digital advertising startup runs an ad-supported photo-sharing website that has users around the globe. The startup is using Amazon S3 to serve photos to website users. Several weeks later, the solutions architect found out that third-party sites have been linking to the photos on the company S3 bucket which is causing losses in overall financial ad revenue. Some users are also reporting that the photos are taking too much time to load.</p><p>Which of the following options is an effective method to mitigate this security flaw and to improve the performance of the photo-sharing website?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Remove public read access from the S3 bucket. Use CloudFront as the global content delivery network (CDN) service for the photos and use Signed URLs with expiry dates.</p>"
			},
			{
				"correct": false,
				"answer": "Use CloudFront to distribute static content and block the IP addresses of the other websites that are illegally linking the photos to their own websites."
			},
			{
				"correct": false,
				"answer": "Block the IPs of the offending websites in Security Groups and use S3 Cross-Region Replication (CRR)."
			},
			{
				"correct": false,
				"answer": "Store photos on an EBS volume of the web server with data encryption enabled. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon CloudFront</strong> is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds. CloudFront is integrated with AWS - including physical locations that are directly connected to the AWS global infrastructure, as well as software that works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code close to your viewers.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudfront_oia.png\"></p><p>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content. These additional information appear in a policy statement, which is based on either a canned policy or a custom policy.</p><p>In this scenario, the main issue is that there are other websites that are illegally using the photos hosted from your S3 bucket. The secondary issue is the slow loading times of the photos. The use of CloudFront as a CDN with Signed URLs is the best solution for this situation.</p><p>Therefore, the correct answer is: <strong>Remove public read access from the S3 bucket. Use CloudFront as the global content delivery network (CDN) service for the photos and use Signed URLs with expiry dates</strong>.</p><p>The option that says: <strong>Use CloudFront to distribute static content and block the IP addresses of the other websites that are illegally linking the photos to their own websites</strong> is incorrect. Although you can block the offending IP addresses of the websites that illegally use your photos, other attackers can still use a different IP address and use your photos without your permission.</p><p>The option that says: <strong>Store photos on an EBS volume of the web server with data encryption enabled</strong> is incorrect. An EBS volume is not a scalable storage solution and is not suitable in hosting static assets for a public website.</p><p>The option that says: <strong>Block the IPs of the offending websites in Security Groups and use S3 Cross-Region Replication (CRR)</strong> is incorrect. In a Security Group, you can specify and allow rules, but not deny rules. In addition, S3 Cross-Region Replication (CRR) will not provide the security needed in this case. Take note: although S3 Cross-Region Replication replicates the bucket to another region, it is still better to use CloudFront to cover all of the global regions.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\">https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html</a></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><br></p><p><strong>Check out this Amazon CloudFront Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudfront/?src=udemy\">https://tutorialsdojo.com/amazon-cloudfront/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professiona</a></p></div>"
	},
	{
		"question": "<p>A software development company based in New Jersey has tasked the solutions architect to design the network architecture for their new enterprise resource planning (ERP) system in AWS. The new system should allow access to business managers and analysts over the Internet, whether they are in their hotel rooms, cafes, or elsewhere. However, the ERP system should not be publicly accessible by anyone over the Internet but only by authorized personnel.</p><p>Which of the following network design meets the above requirements while minimizing deployment and operational costs?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Establish an AWS Direct Connect connection and create a private interface to your VPC. Create a public subnet and place your app servers in it."
			},
			{
				"correct": false,
				"answer": "Establish an IPsec VPN connection and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it."
			},
			{
				"correct": true,
				"answer": "Establish an SSL VPN solution in a public subnet of your VPC. Install and configure SSL VPN client software on all the workstations/laptops of the users who need access to the ERP system. Create a private subnet in your VPC and place your application servers behind it."
			},
			{
				"correct": false,
				"answer": "Deploy the ERP system behind an Elastic Load Balancer with an SSL certificate to allow HTTPS connections. "
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Secure Sockets Layer (SSL) VPN</strong> is an emerging technology that provides remote-access VPN capability, using the SSL function that is already built into a modern web browser. SSL VPN allows users from any Internet-enabled location to launch a web browser to establish remote-access VPN connections, thus promising productivity enhancements and improved availability, as well as further IT cost reduction for VPN client software and support.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_vpn.png\"></p><p>Always keep in mind to install the SSL VPN software in your EC2 instances which are deployed in the public subnet. This will be where your users can connect to the Internet to be able to access the business applications, which is deployed in the private subnet of your VPC.</p><p>Therefore, the correct answer is: <strong>Establish an SSL VPN solution in a public subnet of your VPC. Install and configure SSL VPN client software on all the workstations/laptops of the users who need access to the ERP system. Create a private subnet in your VPC and place your application servers behind it</strong>. Configuring the SSL VPN solution is cost-effective and allows access only for business travelers and remote employees. And since the application servers are in the private subnet, the application is not accessible via the Internet.</p><p>The option that says: <strong>Establish an AWS Direct Connect connection and create a private interface to your VPC. Create a public subnet and place your app servers in it</strong> is incorrect. AWS Direct Connect is not a cost-effective solution compared with a VPN solution.</p><p>The option that says: <strong>Deploy the ERP system behind an Elastic Load Balancer with an SSL certificate to allow HTTPS connections</strong> is incorrect. It does not mention how the application would be accessible only for business travelers and remote employees, and not to the public.</p><p>The option that says: <strong>Establish an IPsec VPN connection and provide the users with the configuration details. Create a public subnet in your VPC, and place your application servers in it</strong> is incorrect. If the application servers are put in the public subnet, they would be publicly accessible via the Internet.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/aviatrix-user-vpn/\">https://aws.amazon.com/quickstart/architecture/aviatrix-user-vpn/</a></p><p><a href=\"https://aws.amazon.com/articles/connecting-multiple-vpcs-with-ec2-instances-ssl\">https://aws.amazon.com/articles/connecting-multiple-vpcs-with-ec2-instances-ssl</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A government technology agency has recently hired a team to build a mobile tax app that allows users to upload their tax deductions and income records using their devices. The app would also allow users to view or download their uploaded files later on. These files are confidential, tax-related documents that need to be stored in a single, secure S3 bucket. The mobile app's design is to allow the users to upload, view, and download their files directly from an Amazon S3 bucket via the mobile app. Since this app will be used by potentially hundreds of thousands of taxpayers in the country, the solutions architect must ensure that proper user authentication and security features are in place.</p><p>Which of the following options should the solutions architect implement in the infrastructure when a new user registers on the app?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "Record the user's information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses his/her mobile app, create temporary credentials using the 'AssumeRole' function in STS. Store these credentials in the mobile app's memory and use them to access the S3 bucket. Generate new credentials the next time the user runs the mobile app."
			},
			{
				"correct": false,
				"answer": "Create a set of long-term credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3."
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon DynamoDB to record the user's information and when the user uses the mobile app, create access credentials using STS with appropriate permissions. Store these credentials in the mobile app's memory and use them to access the S3 bucket every time the user runs the app.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an IAM user then assign appropriate permissions to the IAM user. Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>This scenario requires the mobile application to have access to the S3 bucket. The mobile app might potentially have millions of tax-paying users that will upload their documents to S3. In this scenario where mobile applications need to access AWS Resources, always think about using STS actions such as <strong>\"AssumeRole\"</strong>, <strong>\"AssumeRoleWithSAML\"</strong>, and <strong>\"AssumeRoleWithWebIdentity\"</strong>. DynamoDB is a better database choice for user registration and management, compared to RDS, as it is more scalable in handling millions of records and scales the capacity for the potentially millions of users of the mobile app.</p><p><img src=\"https://media.tutorialsdojo.com/sap_assumeRole.png\"></p><p>You can let users sign in using a well-known third-party identity provider such as Login with Amazon, Facebook, Google, or any OpenID Connect (OIDC) 2.0 compatible provider. You can exchange the credentials from that provider for temporary permissions to use resources in your AWS account. This is known as the web identity federation approach to temporary access. When you use web identity federation for your mobile or web application, you don't need to create custom sign-in code or manage your own user identities. Using web identity federation helps you keep your AWS account secure because you don't have to distribute long-term security credentials, such as IAM user access keys, with your application.</p><p>The option that says: <strong>Record the user's information in Amazon RDS and create a role in IAM with appropriate permissions. When the user uses his/her mobile app, create temporary credentials using the 'AssumeRole' function in STS. Store these credentials in the mobile app's memory and use them to access the S3 bucket. Generate new credentials the next time the user runs the mobile app</strong> is correct because it creates an IAM Role with the required permissions and generates temporary security credentials using STS \"AssumeRole\" function. Furthermore, it generates new credentials when the user runs the app the next time around.</p><p>The option that says: <strong>Create a set of long-term credentials using AWS Security Token Service with appropriate permissions. Store these credentials in the mobile app and use them to access Amazon S3</strong> is incorrect because you should never store credentials inside a mobile app for security purposes, to avoid the risk of exposing any access keys or passwords. You should instead grant temporary credentials for the mobile app. In addition, you cannot create long-term credentials using AWS STS as this service can only generate temporary access tokens.</p><p>The option that says: <strong>Use Amazon DynamoDB to record the user's information and when the user uses the mobile app, create access credentials using STS with appropriate permissions. Store these credentials in the mobile app's memory and use them to access the S3 bucket every time the user runs the app</strong> is incorrect. Even though the setup is similar to the previous option and uses DynamoDB, it is still wrong to store long-term credentials in a mobile app as it is a security risk. In addition, it does not create an IAM Role with proper permissions, which is an essential step.</p><p>The option that says:<strong> Create an IAM user then assign appropriate permissions to the IAM user. Generate an access key and secret key for the IAM user, store them in the mobile app and use these credentials to access Amazon S3</strong> is incorrect because it creates an IAM User and not an IAM Role. You should create an IAM Role so that the app can access the AWS Resource via the \"AssumeRole\" action in STS.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html</a></p><p><a href=\"https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\">https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A top Internet of Things (IoT) company has developed a wrist-worn activity tracker for soldiers deployed in the field. The device acts as a sensor to monitor the health and vital statistics of the wearer. It is expected that there would be thousands of devices that will send data to the server every minute and after 5 years, the number will increase to tens of thousands. One of the requirements is that the application should be able to accept the incoming data, run it through ETL to store in a data warehouse, and archive the old data. The officers in the military headquarters should have a real-time dashboard to view the sensor data.</p><p>Which of the following options is the most suitable architecture to implement in this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Store the data to an S3 bucket with a lifecycle policy to store in Glacier after a month. Launch an EMR cluster that uses the bucket data and runs it through ETL, which then outputs that data to Redshift."
			},
			{
				"correct": true,
				"answer": "<p>Store the data directly to Amazon Kinesis Data Firehose and output the data to an S3 bucket. For archiving, create a lifecycle policy from S3 to Glacier. Use Lambda to process the data through EMR and sends the output to Redshift.</p>"
			},
			{
				"correct": false,
				"answer": "Leverage on Amazon Athena to accept the incoming data and store them using DynamoDB. Setup a cron job that takes data from the DynamoDB table and sends it to an EMR cluster for ETL, then outputs the result to Redshift."
			},
			{
				"correct": false,
				"answer": "Store the data directly to DynamoDB. Launch a data pipeline that starts an EMR cluster using data from DynamoDB and sends the data to S3 and Redshift."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Whenever there is a requirement for a real-time data collection or analysis, always consider Amazon Kinesis.</p><p><strong>Amazon Kinesis Data Firehose</strong> is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Splunk, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, and Sumo Logic. Kinesis Data Firehose is part of the Kinesis streaming data platform, along with Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics.</p><p><img src=\"https://media.tutorialsdojo.com/sap_kinesis_data_firehose_overview.png\"></p><p>Therefore, the correct answer is: <strong>Store the data directly to Amazon Kinesis Data Firehose and output the data to an S3 bucket. For archiving, create a lifecycle policy from S3 to Glacier. Use Lambda to process the data through EMR and sends the output to Redshift.</strong> Amazon Kinesis will ingest the data in real-time and output in Amazon S3.</p><p>The option that says: <strong>Store the data directly to DynamoDB. Launch a data pipeline that starts an EMR cluster using data from DynamoDB and sends the data to S3 and Redshift</strong> is incorrect. For the collection of real-time data, AWS recommends Amazon Kinesis for ingestion. After ingestion, you can output the data into several AWS services for further processing.</p><p>The option that says: <strong>Store the data to an S3 bucket with a lifecycle policy to store in Glacier after a month. Launch an EMR cluster that uses the bucket data and runs it through ETL, which then outputs that data to Redshift</strong> is incorrect. Amazon S3 can only accept data using HTTP requests, and the data sent by IoT may be different. For ingesting the data, Amazon Kinesis should be the first one to accept, process it, and store to Amazon S3.</p><p>The option that says: <strong>Leverage on Amazon Athena to accept the incoming data and store them using DynamoDB. Setup a cron job that takes data from the DynamoDB table and sends it to an EMR cluster for ETL, then outputs the result to Redshift</strong> is incorrect. Amazon Athena is a query service to query data and analyze big data, and not suitable for ingesting real-time IoT data.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/kinesis/data-firehose/\">https://aws.amazon.com/kinesis/data-firehose/</a></p><p><a href=\"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\">https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html</a></p><p><a href=\"https://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html\">https://docs.aws.amazon.com/kinesisanalytics/latest/java/examples-s3.html</a></p><p><br></p><p><strong>Check out this Amazon Kinesis Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-kinesis/?src=udemy\">https://tutorialsdojo.com/amazon-kinesis/</a></p></div>"
	},
	{
		"question": "<p>A leading media company in the country is building a voting system for a popular singing competition show on national TV. The viewers who watch the performances can visit the company’s dynamic website to vote for their favorite singer. After the show has finished, it is expected that the site will receive millions of visitors who would like to cast their votes. Web visitors should log in using their social media accounts and then submit their votes. The webpage will display the winner after the show, as well as the vote total for each singer. The solutions architect is tasked to build the voting site and ensure that it can handle the rapid influx of incoming traffic in the most cost-effective way possible.</p><p>Which of the following architecture should you use to meet the requirement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Use Amazon Cognito for user authentication. The web servers will process the user's vote and pass the result in an SQS queue. Set up an IAM Role to grant the EC2 instances permissions to write to the SQS queue. A group of EC2 instances will then retrieve and process the items from the queue. Finally, store the results in a DynamoDB table.</p>"
			},
			{
				"correct": false,
				"answer": "Use a CloudFront web distribution and an Application Load balancer in front of an Auto Scaling group of EC2 instances. The servers will first authenticate the user using IAM and then process the user's vote which will then be stored to RDS."
			},
			{
				"correct": false,
				"answer": "Use a CloudFront web distribution and deploy the website using S3 hosting feature. Write a custom NodeJS application to authenticate the user using STS and AssumeRole API. Setup an IAM Role to grant permission to store the user's vote to a DynamoDB table."
			},
			{
				"correct": false,
				"answer": "Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Develop a custom authentication service using STS and AssumeRoleWithSAML API. The servers will process the user's vote and store the result in a DynamoDB table."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For User authentication, you can use <strong>Amazon Cognito</strong>. To host the static assets of the website, you can use CloudFront. Considering that there would be millions of voters and data to be stored, it is best to use DynamoDB which can automatically scale.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cognito_identity_pool.png\"></p><p>Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0.</p><p>Therefore, the correct answer is: <strong>Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Use Amazon Cognito for user authentication. The web servers will process the user's vote and pass the result in an SQS queue. Set up an IAM Role to grant the EC2 instances permissions to write to the SQS queue. A group of EC2 instances will then retrieve and process the items from the queue. Finally, store the results in a DynamoDB table.</strong></p><p>The option that says: <strong>Use a CloudFront web distribution and an Application Load balancer in front of an Auto Scaling group of EC2 instances. The servers will first authenticate the user using IAM and then process the user's vote which will then be stored to RDS</strong> is incorrect. By default, you can't use IAM alone to set up social media account registration to your website. You have to use Amazon Cognito. It is also more suitable to use DynamoDB instead of an RDS database since this is only a simple voting application that doesn't warrant a complex table relationship. DynamoDB is a fully managed database that automatically scales, unlike RDS. It can store and accommodate millions of data from the users who cast their votes more effectively than RDS.</p><p>The option that says:<strong> Use a CloudFront web distribution and deploy the website using S3 hosting feature. Write a custom NodeJS application to authenticate the user using STS and AssumeRole API. Setup an IAM Role to grant permission to store the user's vote to a DynamoDB table</strong> is incorrect. The <code>AssumeRole</code> API is not suitable for user authentication that uses a web identity provider such as Amazon Cognito, Login with Amazon, Facebook, Google, or any social media identity provider. You can use the AssumeRoleWithWebIdentity API or better yet, Amazon Cognito instead.</p><p>The option that says: <strong>Use a CloudFront web distribution and an Application Load Balancer in front of an Auto Scaling group of EC2 instances. Develop a custom authentication service using STS and AssumeRoleWithSAML API. The servers will process the user's vote and store the result in a DynamoDB table</strong> is incorrect. The <code>AssumeRoleWithSAML</code> API is primarily used for SAML 2.0-based federation and for social media user registration.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/cognito/\">https://aws.amazon.com/cognito/</a></p><p><a href=\"https://aws.amazon.com/cloudfront/\">https://aws.amazon.com/cloudfront/</a></p><p><a href=\"https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\">https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html</a></p><p><br></p><p><strong>Check out these Amazon Cognito Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito/?src=udemy\">https://tutorialsdojo.com/amazon-cognito/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/?src=udemy\">https://tutorialsdojo.com/amazon-cognito-user-pools-and-identity-pools-explained/</a></p></div>"
	},
	{
		"question": "<p>An enterprise is in the process of integrating the systems of the smaller companies it has acquired in the past few months. The company wants to create an AWS Landing Zone that will allow hundreds of new employees to use their corporate credentials to log in to the AWS Console. The company is using a Microsoft Active Directory (AD) service for user authentication and has an AWS Direct Connect connection to AWS. The newly acquired companies come from a wide range of engineering fields so it is required that the solution will be able to federate third-party services and providers as well as custom applications.</p><p>Which of the following implementations will meet the company requirements with the LEAST amount of management overhead?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create an Active Directory Federation Services (AD FS) portal page with the company branding. Integrate third-party applications on this portal with SAML 2.0 support. Use single sign-on with the AD FS to connect the company Active Directory to AWS. Configure the Identity Provider (IdP) to use form-based authentication with the portal page.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Active Directory Federation Services (AD FA) with SAML 2.0 to connect the company Active Directory to AWS. Configure the AD FS to use Regex with the AD naming convention for the security group. This will allow federation on all AWS accounts. Configure single sign-on integrations for third party applications by adding them to the AD FS server.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Connect the company on-premises Active Directory using the AWS Directory Service AD connector to create a single sign-on experience for users. Configure IAM and service roles to enable federation support. Configure single sign-on integrations for connections with third-party applications.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Configure AWS Single Sign-On with AWS Organizations to manage SSO access and permissions on AWS. Set up a two-way forest trust relationship between the AWS Directory service and the company Active Directory to allow users to use their corporate credentials when logging in to AWS. Leverage on the third-party integration support of AWS Single Sign-On.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Single Sign-On</strong> is a cloud-based single sign-on (SSO) service that makes it easy to centrally manage SSO access to all of your AWS accounts and cloud applications. Specifically, it helps you manage SSO access and user permissions across all your AWS accounts in AWS Organizations. AWS SSO also helps you manage access and permissions to commonly used third-party software as a service (SaaS) applications, AWS SSO-integrated applications as well as custom applications that support Security Assertion Markup Language (SAML) 2.0. AWS SSO includes a user portal where your end-users can find and access all their assigned AWS accounts, cloud applications, and custom applications in one place.</p><p>AWS SSO makes it simple for you to manage SSO across all your AWS accounts, cloud applications, AWS SSO-integrated applications, and custom SAML 2.0–based applications, without custom scripts or third-party SSO solutions. Use the AWS SSO console to quickly assign which users should have one-click access to only the applications that you've authorized for their personalized end-user portal.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_directory_service.png\"></p><p>AWS SSO has integration with Microsoft AD through the <strong>AWS Directory Service</strong>. This means your employees can sign in to your AWS SSO user portal using their corporate Active Directory credentials. To grant Active Directory users access to accounts and applications, you simply add them to the appropriate Active Directory groups. For example, you can grant the DevOps group SSO access to your production AWS accounts. Users added to the DevOps group are then granted SSO access to these AWS accounts automatically. This automation makes it easy to onboard new users and gives existing users access to new accounts and applications quickly.</p><p>You can configure one and two-way external and forest trust relationships between your AWS Directory Service for Microsoft Active Directory and on-premises directories, as well as between multiple AWS Managed Microsoft AD directories in the AWS cloud. AWS Managed Microsoft AD supports all three trust relationship directions: Incoming, Outgoing, and Two-way (Bi-directional). AWS Managed Microsoft AD supports both external and forest trusts.</p><p>Users in your self-managed Active Directory (AD) can also have SSO access to AWS accounts and cloud applications in the AWS SSO user portal. To do that, AWS Directory Service has the following two options available:</p><p><strong>Create a two-way trust relationship</strong> – When two-way trust relationships are created between AWS Managed Microsoft AD and a self-managed AD, users in your self-managed AD can sign in with their corporate credentials to various AWS services and business applications. One-way trusts do not work with AWS SSO.</p><p><strong>Create an AD Connector</strong> – AD Connector is a directory gateway that can redirect directory requests to your self-managed AD without caching any information in the cloud.</p><p>Therefore, the correct answer is: <strong>Configure AWS Single Sign-On with AWS Organizations to manage SSO access and permissions on AWS. Set up a two-way forest trust relationship between the AWS Directory service and the company Active Directory to allow users to use their corporate credentials when logging in to AWS. Leverage on the third-party integration support of AWS Single Sign-On.</strong></p><p>The option that says: <strong>Create an Active Directory Federation Services (AD FS) portal page with the company branding. Integrate third-party applications on this portal with SAML 2.0 support. Use single sign-on with the AD FS to connect the company Active Directory to AWS. Configure the Identity Provider (IdP) to use form-based authentication with the portal page</strong> is incorrect. This may be possible, but creating a form-based authentication defeats the purpose of a single sign-on. Also, this requires more additional management to create the AD FS portal page.</p><p>The option that says: <strong>Connect the company on-premises Active Directory using the AWS Directory Service AD connector to create a single sign-on experience for users. Configure IAM and service roles to enable federation support. Configure single sign-on integrations for connections with third-party applications</strong> is incorrect. This does not address the third-party integrations because when using the AD connector for SSO, you cannot use both on-premises AD and third-party integrations at the same time.</p><p>The option that says: <strong>Create an Active Directory Federation Services (AD FS) with SAML 2.0 to connect the company Active Directory to AWS. Configure the AD FS to use Regex with the AD naming convention for the security group. This will allow federation on all AWS accounts. Configure single sign-on integrations for third party applications by adding them to the AD FS server as a principal (trusted entity)</strong> is incorrect. This is possible but will require more management overhead compared to just using AWS Single Sign-On service. Using Regex is favorable if you have established standard naming conventions, however, you may encounter some problems if other companies are not using your expected naming convention.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html\">https://docs.aws.amazon.com/singlesignon/latest/userguide/connectonpremad.html</a></p><p><a href=\"https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html\">https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_setup_trust.html</a></p><p><a href=\"https://docs.microsoft.com/en-us/azure/active-directory-domain-services/concepts-forest-trust\">https://docs.microsoft.com/en-us/azure/active-directory-domain-services/concepts-forest-trust</a></p><p><br></p><p><strong>Check out this AWS Directory Service Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-directory-service/ ?src=udemy\">https://tutorialsdojo.com/aws-directory-service/</a></p></div>"
	},
	{
		"question": "<p>A leading e-commerce company plans to launch a donation website for all the victims of the recent super typhoon in South East Asia for its Corporate and Social Responsibility program. The company will advertise its program on TV and on social media, which is why they anticipate incoming traffic on their donation website. Donors can send their donations in cash, which can be transferred electronically, or they can simply post their home address where a team of volunteers can pick up their used clothes, canned goods, and other donations. Donors can optionally write a positive and encouraging message to the victims along with their donations. These features of the donation website will eventually result in a high number of write operations on their database tier considering that there are millions of generous donors around the globe who want to help.</p><p>Which of the following options is the best solution for this scenario?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Use an Oracle database hosted on an extra large Dedicated EC2 instance as your database tier and an SQS queue for buffering the write operations."
			},
			{
				"correct": true,
				"answer": "Amazon DynamoDB with a provisioned write throughput. Use an SQS queue to buffer the large incoming traffic to your Auto Scaled EC2 instances, which processes and writes the data to DynamoDB."
			},
			{
				"correct": false,
				"answer": "Use an Amazon RDS instance with Provisioned IOPS."
			},
			{
				"correct": false,
				"answer": "Use DynamoDB as a database storage and a CloudFront web distribution for hosting static resources."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In this scenario, the application is write-intensive which means that we have to implement a solution to buffer and scale out the services according to the incoming traffic. SQS can be used to handle the large incoming requests and you can use DynamoDB to store large amounts of data.</p><p><strong>Amazon Simple Queue Service (Amazon SQS)</strong> offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components. Amazon SQS offers common constructs such as dead-letter queues and cost allocation tags.</p><p>Amazon SQS queues can deliver very high throughput. Standard queues support a nearly unlimited number of transactions per second (TPS) per action. By default, FIFO queues support up to 3,000 messages per second with batching.</p><p><img src=\"https://media.tutorialsdojo.com/sap_sqs_dynamodb.png\"></p><p>Therefore, the correct answer is: <strong>Amazon DynamoDB with a provisioned write throughput. Use an SQS queue to buffer the large incoming traffic to your Auto Scaled EC2 instances, which processes and writes the data to DynamoDB.</strong> The SQS queue can act as a buffer to temporarily store all the requests while waiting for them to be processed and written to the DynamoDB table.</p><p>The option that says: <strong>Use DynamoDB as a database storage and a CloudFront web distribution for hosting static resources</strong> is incorrect. CloudFront is used for caching static content, not for hosting the static website resources</p><p>The option that says: <strong>Use an Oracle database hosted on an extra large Dedicated EC2 instance as your database tier and an SQS queue for buffering the write operations</strong> is incorrect. AWS does not recommend hosting a database on EC2 instances. RDS instances are designed for hosting databases.</p><p>The option that says: <strong>Use an Amazon RDS instance with Provisioned IOPS</strong> is incorrect. This may be possible, however, this is significantly more expensive than using a DynamoDB table and adding an SQS queue.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-throughput-horizontal-scaling-and-batching.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-throughput-horizontal-scaling-and-batching.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-client-side-buffering-request-batching.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-client-side-buffering-request-batching.html</a></p><p><a href=\"https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\">https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html</a></p><p><br></p><p><strong>Check out this Amazon SQS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-sqs/?src=udemy\">https://tutorialsdojo.com/amazon-sqs/</a></p></div>"
	},
	{
		"question": "<p>A leading aerospace engineering company has over 1 TB of aeronautical data stored on the corporate file server of their on-premises network. This data is used by a lot of their in-house analytical and engineering applications. The aeronautical data consists of technical files which can have a file size of a few megabytes to multiple gigabytes. The data scientists typically modify an average of 10 percent of these files every day. Recently, the management decided to adopt a hybrid cloud architecture to better serve their clients around the globe. The management requested to migrate its applications to AWS over the weekend to minimize any business impact and system downtime. The on-premises data center has a 50-Mbps Internet connection which can be used to transfer all of the 1 TB of data in AWS but based on the calculations, it will take at least 48 hours to complete this task.</p><p>Which of the following options will allow the solutions architect to move all of the aeronautical data to AWS to meet the above requirement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. At the end of business hours on Friday, start copying the data to a Snowball Edge device.\n</p><p>2. When the Snowball Edge have completely transferred your data to your AWS Cloud, copy all of the data to multiple EBS Volumes.\n</p><p>3. On Sunday afternoon, mount the generated EBS volume to your EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Synchronize the data from your on-premises data center to an S3 bucket using Multipart upload for large files from Saturday morning to Sunday evening. </p><p><br></p><p>2. Configure your application hosted in AWS to use the S3 bucket to serve the aeronautical data files.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. Synchronize the on-premises data to an S3 bucket one week before the migration schedule using the AWS CLI's S3&nbsp; <code>sync</code> command.\n</p><p>2. Perform a final synchronization task on Friday after the end of business hours.\n</p><p>3. Set up your application hosted in a large EC2 instance in your VPC to use the S3 bucket.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Set up a Gateway-Stored volume gateway using the AWS Storage Gateway service. </p><p><br></p><p>2. Establish an iSCSI connection between your on-premises data center and your AWS Cloud then copy the data to the Storage Gateway volume. </p><p><br></p><p>3. After all of your data has been successfully copied, create an EBS snapshot of the volume. </p><p><br></p><p>4. Restore the snapshots as EBS volumes and attach them to your EC2 instances on Sunday.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>The most effective choice here is to use the S3 <code>sync</code> feature that is available in AWS CLI. In this way, you can comfortably synchronize the data in your on-premises server and in AWS a week before the migration. And on Friday, just do another sync to complete the task. Remember that the sync feature of S3 only uploads the \"delta\" or in other words, the \"difference\" in the subset. Therefore, it will only take just a fraction of the time to complete the data synchronization compared to the other methods.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_s3_cli.png\"></p><p>Therefore, the correct answer is:</p><p><strong>1. Synchronize the on-premise data to an S3 bucket one week before the migration schedule using the AWS CLI's S3 </strong><code><strong>sync</strong></code><strong> command.</strong></p><p><strong>2. Perform a final synchronization task on Friday after the end of business hours.</strong></p><p><strong>3. Set up your application hosted in a large EC2 instance in your VPC to use the S3 bucket.</strong></p><p>The idea is to synchronize the data days before the migration weekend to avoid the risks and possible delays of transferring the data in such a short period of allowable time i.e. 48 hours.</p><p><br></p><p>The following option is incorrect:</p><p><strong>1. At the end of business hours on Friday, start copying the data to a Snowball Edge device.</strong></p><p><strong>2. When the Snowball Edge have completely transferred your data to your AWS Cloud, copy all of the data to multiple EBS Volumes.</strong></p><p><strong>3. On Sunday afternoon, mount the generated EBS volume to your EC2 instances.</strong></p><p>Although using a Snowball appliance is a valid option, there is a risk that your data won't make it on time on Monday. Remember that you have to consider the time it takes to transfer the data to Snowball including the process of shipping it back to AWS, which may take a day or two. Factoring all of these considerations, it is clear that this is not the best option as the risk is just too high that the Snowball delivery might not make it in time.</p><p><br></p><p>The following option is incorrect:</p><p><strong>1. Set up a Gateway-Stored volume gateway using the AWS Storage Gateway service.</strong></p><p><strong>2. Establish an iSCSI connection between your on-premise data center and your AWS Cloud then copy the data to the Storage Gateway volume.</strong></p><p><strong>3. After all of your data has been successfully copied, create an EBS snapshot of the volume.</strong></p><p><strong>4. Restore the snapshots as EBS volumes and attach them to your EC2 instances on Sunday.</strong></p><p>In this scenario, you have to create storage volumes in the AWS Cloud that your on-premises applications can access as Internet Small Computer System Interface (iSCSI) targets, which is not mentioned in the option. This is not a cost-effective solution considering that you only have to migrate 1 TB of data and the fact that you can just use the S3 <code>sync</code> command via AWS CLI.</p><p><br></p><p>The following option is incorrect:</p><p><strong>1. Synchronize the data from your on-premise data center to an S3 bucket using Multipart upload for large files from Saturday morning to Sunday evening.</strong></p><p><strong>2. Configure your application hosted in AWS to use the S3 bucket to serve the aeronautical data files.</strong></p><p>Although using multi-part upload in S3 is faster than regular S3 upload, this method still carries a risk considering that the company only has 50 Mbps internet connection. It is still better to use S3 <code>sync</code> command via AWS CLI days before the actual migration to mitigate the risks.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html\">https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html</a></p><p><a href=\"https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html\">https://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p></div>"
	},
	{
		"question": "<p>A pharmaceutical company has a hybrid cloud architecture in AWS. It has three different accounts for its environments: DEV, UAT, and PROD, which are all part of the consolidated billing account. The PROD account has purchased 10 r4.16xlarge Reserved EC2 Instances in the us-west-2a Availability Zone. Currently, there is no running EC2 instance in the PROD account because the application is not live yet but in the DEV account, there are 5 r4.16xlarge EC2 instances running in the us-west-2a Availability Zone. In the UAT account, there are also 5 r4.16xlarge EC2 instances running in the us-west-1a Availability Zone.</p><p>In this scenario, which account benefits the most out of the Reserved Instance pricing?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "None. Considering that the PROD account is the one that purchased the Reserved Instance and it does not have any running EC2 instance, there is currently no other member account that benefits from the Reserved Instance pricing. "
			},
			{
				"correct": false,
				"answer": "Since both DEV and UAT accounts are running an EC2 instance type which exactly matches the Reserved Instance type (r4.16xlarge), then the Reserved instance pricing will be applied to all EC2 instances in those two member accounts."
			},
			{
				"correct": true,
				"answer": "Currently, only the DEV account benefits from the Reserved Pricing."
			},
			{
				"correct": false,
				"answer": "Currently, only the UAT account benefits from the Reserved Pricing."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>In the question, DEV and UAT are running the same instance type (r4.16xlarge) but the main difference is that the former is using us-west-2a Availability Zone while the latter uses us-west-1a. Remember that the PROD account has bought the Reserved Instance in the us-west-2a Availability Zone which means that only the DEV account exactly matches the criteria.</p><p>As an Amazon EC2 Reserved Instances example, suppose that Bob and Susan each have an account in an organization. Susan has five Reserved Instances of the same type, and Bob has none. During one particular hour, Susan uses three instances and Bob uses six, for a total of nine instances on the organization's consolidated bill. AWS bills five instances as Reserved Instances, and the remaining four instances as regular instances.</p><p>Bob receives the cost benefit from Susan's Reserved Instances only if he launches his instances in the same Availability Zone where Susan purchased her Reserved Instances. For example, if Susan specifies us-west-2a when she purchases her Reserved Instances, Bob must specify us-west-2a when he launches his instances to get the cost benefit on the organization's consolidated bill. However, the actual locations of Availability Zones are independent from one account to another. For example, the us-west-2a Availability Zone for Bob's account might be in a different location than the location for Susan's account.</p><p>Therefore, the correct answer is: <strong>Currently, only the DEV account benefits from the Reserved Pricing.</strong></p><p>The option that says: <strong>Currently, only the UAT account benefits from the Reserved Pricing</strong> is incorrect. The UAT account can only benefit from the Reserved Instances of the PROD account if they were launched in the same Availability Zone where the PROD account purchased the Reserved Instances.</p><p>The option that says: <strong>Since both DEV and UAT accounts are running an EC2 instance type which exactly matches the Reserved Instance type (r4.16xlarge), then the Reserved instance pricing will be applied to all EC2 instances in those two member accounts</strong> is incorrect. The UAT account can only benefit from the Reserved Instances of the PROD account if they were launched in the same Availability Zone where the PROD account purchased the Reserved Instances.</p><p>The option that says: <strong>None. Considering that the PROD account is the one that purchased the Reserved Instance and it does not have any running EC2 instance, there is currently no other member account that benefits from the Reserved Instance pricing</strong> is incorrect. The DEV account benefits from the Reserved Instances pricing because the instances are launched on the same AZ where the PROD account purchased the Reserved Instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html</a></p><p><a href=\"https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html\">https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidatedbilling-other.html</a></p><p><br></p><p><strong>Check out this AWS Billing and Cost Management Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-billing-and-cost-management/?src=udemy\">https://tutorialsdojo.com/aws-billing-and-cost-management/</a></p></div>"
	},
	{
		"question": "<p>A company wants to have a secure content management solution that can be accessed by its external custom applications via API calls. The solutions architect has been instructed to create the infrastructure design. The solution should enable users to upload documents as well as download a specific version or the latest version of a document. There is also a requirement to enable customer administrators to simply submit an API call that can roll back changes to existing files sent to the system.</p><p>Which of the following options is the MOST secure and suitable solution that the solutions architect should implement?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Use S3 with Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Use client-side encryption to encrypt customer files then share the Customer Master Key (CMK) ID and the client-side master key to all customers in order to access the CMS.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Use Amazon WorkDocs for document storage and utilize its user access management, version control, and built-in encryption. Integrate the Amazon WorkDocs Content Manager to the external custom applications. Develop a rollback feature to replace the current document version with the previous version from Amazon WorkDocs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon EFS for object storage and enable data encryption in transit with TLS. Store unique customer managed keys in AWS KMS. Set up IAM roles and IAM access policies for EFS to specify separate encryption keys for each customer application. Utilize file locking and file versioning features in EFS to roll back changes to existing files stored in the CMS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use Amazon S3 with Versioning and Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Encrypt all documents using client-side encryption for enhanced data security. Share the encryption keys to all customers to unlock the documents. Develop a rollback feature to replace the current document version with the previous version from Amazon S3.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon WorkDocs</strong> is a fully managed, secure content creation, storage, and collaboration service. With Amazon WorkDocs, you can easily create, edit, and share content, and because it’s stored centrally on AWS, access it from anywhere on any device. Amazon WorkDocs makes it easy to collaborate with others, and lets you easily share content, provide rich feedback, and collaboratively edit documents. You can use Amazon WorkDocs to retire legacy file share infrastructure by moving file shares to the cloud. Amazon WorkDocs lets you integrate with your existing systems, and offers a rich API so that you can develop your own content-rich applications. Amazon WorkDocs is built on AWS, where your content is secured on the world's largest cloud infrastructure.</p><p><img src=\"https://media.tutorialsdojo.com/sap_amazon_workdocs_overview.png\"></p><p>Amazon WorkDocs Content Manager is a high-level utility tool that uploads content or downloads it from an Amazon WorkDocs site. It can be used for both administrative and user applications. For user applications, a developer must construct the Amazon WorkDocs Content Manager with anonymous AWS credentials and an authentication token. For administrative applications, the Amazon WorkDocs client must be initialized with AWS Identity and Access Management (IAM) credentials. In addition, the authentication token must be omitted in subsequent API calls.</p><p>Hence, the correct answer in this scenario is: <strong>Use Amazon WorkDocs for document storage and utilize its user access management, version control, and built-in encryption. Integrate the Amazon WorkDocs Content Manager to the external custom applications. Develop a rollback feature to replace the current document version with the previous version from Amazon WorkDocs.</strong></p><p>The option that says: <strong>Use Amazon S3 with Versioning and Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Encrypt all documents using client-side encryption for enhanced data security. Share the encryption keys to all customers to unlock the documents. Develop a rollback feature to replace the current document version with the previous version from Amazon S3</strong> is incorrect. Although you can use Amazon S3, sharing the encryption keys to all customers just to unlock the documents is a security risk.</p><p>The option that says: <strong>Use S3 with Server Access Logging enabled. Set up an IAM role and access policy for each customer application. Use client-side encryption to encrypt customer files then share the Customer Master Key (CMK) ID and the client-side master key to all customers in order to access the CMS</strong> is incorrect because the S3 bucket being used has only enabled Server Access Logging and not Versioning. In addition, it is also a security risk to share the Customer Master Key (CMK) ID and the client-side master key to all customers.</p><p>The option that says: <strong>Use Amazon EFS for object storage and enable data encryption in transit with TLS. Store unique customer managed keys in AWS KMS. Set up IAM roles and IAM access policies for EFS to specify separate encryption keys for each customer application. Utilize file locking and file versioning features in EFS to roll back changes to existing files stored in the CMS</strong> is incorrect because EFS is primarily used as a file system and not for object storage. Although EFS has file locking capabilities, it does not have file versioning features.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/workdocs/\">https://aws.amazon.com/workdocs/</a></p><p><a href=\"https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_constructing.html\">https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_constructing.html</a></p><p><a href=\"https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_downloading.html\">https://docs.aws.amazon.com/workdocs/latest/developerguide/content_manager_downloading.html</a></p></div>"
	},
	{
		"question": "<p>A FinTech startup has recently consolidated its multiple AWS accounts using AWS Organizations. It currently has two teams in its organization, a security team and a development team. The former is responsible for protecting their cloud infrastructure and making sure that all of their resources are compliant, while the latter is responsible for developing new applications that are deployed to EC2 instances. The security team is required to set up a system that will check if all of the running EC2 instances are using an approved AMI. However, the solution should not stop the development team from deploying an EC2 instance running on a non-approved AMI. The disruption is only allowed once the deployment has been completed<strong>.</strong> In addition, they have to set up a notification system that sends the compliance state of the resources to determine whether they are compliant.</p><p>Which of the following options is the most suitable solution that the security team should implement?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use an AWS Config Managed Rule and specify a list of approved AMI IDs. This rule will check whether running EC2 instances are using specified AMIs. Configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic which will send a notification for non-compliant instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the Amazon Inspector service to automatically check all of the AMIs that are being used by your EC2 instances. Set up an SNS topic that will send a notification to both the security and development teams if there is a non-compliant EC2 instance running in their VPCs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create and assign an SCP and an IAM policy that restricts the AWS accounts and the development team from launching an EC2 instance using an unapproved AMI. Create a CloudWatch alarm that will automatically notify the security team if there are non-compliant EC2 instances running in their VPCs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a Trusted Advisor check that will verify whether the running EC2 instances in your VPCs are using approved AMIs. Create a CloudWatch alarm and integrate it with the Trusted Advisor metrics that will check all of the AMIs being used by your EC2 instances and that will send a notification if there is a running instance which uses an unapproved AMI.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>When you run your applications on AWS, you usually use AWS resources, which you must create and manage collectively. As the demand for your application keeps growing, so does your need to keep track of your AWS resources. AWS Config is designed to help you oversee your application resources.</p><p><img src=\"https://media.tutorialsdojo.com/sap_config_how_it_works.png\"></p><p><strong>AWS Config</strong> provides a detailed view of the configuration of AWS resources in your AWS account. This includes how the resources are related to one another and how they were configured in the past so that you can see how the configurations and relationships change over time. With AWS Config, you can do the following:</p><p>- Evaluate your AWS resource configurations for desired settings.</p><p>- Get a snapshot of the current configurations of the supported resources that are associated with your AWS account.</p><p>- Retrieve configurations of one or more resources that exist in your account.</p><p>- Retrieve historical configurations of one or more resources.</p><p>- Receive a notification whenever a resource is created, modified, or deleted.</p><p>- View relationships between resources. For example, you might want to find all resources that use a particular security group.</p><p>AWS Config provides <em>AWS managed rules</em>, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. In this scenario, you can use the <code><strong>approved-amis-by-id</strong></code> AWS managed rule which checks whether running instances are using specified AMIs.</p><p>Therefore, the correct answer is: <strong>Use an AWS Config Managed Rule and specify a list of approved AMI IDs. This rule will check whether running EC2 instances are using specified AMIs. Configure AWS Config to stream configuration changes and notifications to an Amazon SNS topic which will send a notification for non-compliant instances.</strong></p><p>The option that says: <strong>Create and assign an SCP and an IAM policy that restricts the AWS accounts and the development team from launching an EC2 instance using an unapproved AMI. Create a CloudWatch alarm that will automatically notify the security team if there are non-compliant EC2 instances running in their VPCs</strong> is incorrect. Setting up an SCP and IAM Policy will totally restrict the development team from launching EC2 instances with unapproved AMIs. The scenario clearly says that the solution should not have this kind of restriction.</p><p>The option that says: <strong>Use the Amazon Inspector service to automatically check all of the AMIs that are being used by your EC2 instances. Set up an SNS topic that will send a notification to both the security and development teams if there is a non-compliant EC2 instance running in their VPCs</strong> is incorrect. The Amazon Inspector service is just an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not have the capability to detect EC2 instances that are using unapproved AMIs, unlike AWS Config.</p><p>The option that says: <strong>Set up a Trusted Advisor check that will verify whether the running EC2 instances in your VPCs are using approved AMIs. Create a CloudWatch alarm and integrate it with the Trusted Advisor metrics that will check all of the AMIs being used by your EC2 instances and that will send a notification if there is a running instance which uses an unapproved AMI</strong> is incorrect. AWS Trusted Advisor is primarily used to check if your cloud infrastructure is in compliance with the best practices and recommendations across five categories: cost optimization, security, fault tolerance, performance, and service limits. Their security checks for EC2 do not cover the checking of individual AMIs that are being used by your EC2 instances.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html\">https://docs.aws.amazon.com/config/latest/developerguide/approved-amis-by-id.html</a></p><p><a href=\"https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html\">https://docs.aws.amazon.com/config/latest/developerguide/evaluate-config_use-managed-rules.html</a></p><p><br></p><p><strong>Check out this AWS Config Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-config/?src=udemy\">https://tutorialsdojo.com/aws-config/</a></p></div>"
	},
	{
		"question": "<p><br>A company is hosting its application and MySQL database in its on-premises data center. The database increases at about 10GB per day and is approximately 25TB in total size. The company wants to migrate the database workload to the AWS cloud. A 50Mbps VPN connection is currently in place to connect the corporate network to AWS. The company plans to complete the migration to AWS within 3 weeks with the LEAST downtime possible.</p><p>Which of the following solutions should be implemented to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Using the VPN connection, configure replication from the on-premises database server to the Amazon Aurora DB instance. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Temporarily stop the on-premises application to stop any database I/O operation. Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Update the database DNS entry to point to the Aurora DB instance. Start the application again to resume normal operations.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an AWS Database Migration Service (DMS) instance and an Amazon Aurora MySQL DB instance. Define the on-premises database server details and the Amazon Aurora MySQL instance details on the AWS DMS instance. Use the VPN connection to start the replication from the on-premises database server to AWS. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a snapshot of the on-premises database server and use VM Import/Export service to import the snapshot to AWS. Provision a new Amazon EC2 instance from the imported snapshot. Configure replication from the on-premises database server to the EC2 instance through the VPN connection. Wait until the replication is complete then update the database DNS entry to point to the EC2 instance. Stop the database replication.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Snowball</strong>, a part of the AWS Snow Family, is an edge computing, data migration, and edge storage device that comes in two options. <strong>Snowball Edge Storage Optimized</strong> devices provide both block storage and Amazon S3-compatible object storage, and 40 vCPUs. They are well suited for local storage and large scale-data transfer up to 80TB. <strong>Snowball Edge Compute Optimized</strong> devices provide 52 vCPUs, block and object storage, and an optional GPU for use cases like advanced machine learning and full-motion video analysis in disconnected environments.</p><p>Snowball moves terabytes of data in about a week. You can use it to move things like databases, backups, archives, healthcare records, analytics datasets, IoT sensor data, and media content, especially when network conditions prevent realistic timelines for transferring large amounts of data both into and out of AWS.</p><p>Each import job uses a single Snowball appliance. After you create a job in the AWS Snow Family Management Console or the job management API, AWS ships a Snowball to you. When it arrives in a few days, you connect the Snowball Edge device to your network and transfer the data that you want to be imported into Amazon S3 onto the device. When you’re done transferring data, ship the Snowball back to AWS, and AWS will import your data into Amazon S3.</p><p>Amazon Aurora MySQL integrates with other AWS services so that you can extend your Aurora MySQL DB cluster to use additional capabilities in the AWS Cloud. Your Aurora MySQL DB cluster can use AWS services to load data <a href=\"https://textfancy.com\">from text</a> or XML files stored in an Amazon Simple Storage Service (Amazon S3) bucket into your DB cluster using the LOAD DATA FROM S3 or LOAD XML FROM S3 command.</p><p>Therefore, the correct answer is: <strong>Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Using the VPN connection, configure replication from the on-premises database server to the Amazon Aurora DB instance. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication.</strong> You can use the Snowball device to import TBs of data to AWS. Then you can load the data to the Amazon Aurora DB instance and replicate the missing data from the on-premises database server. You can cut over to the Aurora database after replication.</p><p>The option that says: <strong>Create a snapshot of the on-premises database server and use VM Import/Export service to import the snapshot to AWS. Provision a new Amazon EC2 instance from the imported snapshot. Configure replication from the on-premises database server to the EC2 instance through the VPN connection. Wait until the replication is complete then update the database DNS entry to point to the EC2 instance. Stop the database replication</strong> is incorrect. You cannot import a 25 TB snapshot using VM Import/Export. This option does not specify if it uses the company network to export the snapshot to AWS. If so, the 50Mbps won't be enough to export the entire database within the 3-week window.</p><p>The option that says: <strong>Create an AWS Database Migration Service (DMS) instance and an Amazon Aurora MySQL DB instance. Define the on-premises database server details and the Amazon Aurora MySQL instance details on the AWS DMS instance. Use the VPN connection to start the replication from the on-premises database server to AWS. Wait until the replication is complete then update the database DNS entry to point to the Aurora DB instance. Stop the database replication </strong>is incorrect. Replicating an entire 25 TB database via the 50Mbps VPN connection will take several weeks for the replication to catch up. Not to mention that there will be additional 10 GB of data to be replicated for each passing day.</p><p>The option that says: <strong>Temporarily stop the on-premises application to stop any database I/O operation. Request for an AWS Snowball device. Create a database export of the on-premises database server and load it to the Snowball device. Once the data is imported to AWS, provision an Amazon Aurora MySQL DB instance and load the data. Update the database DNS entry to point to the Aurora DB instance. Start the application again to resume normal operations</strong> is incorrect. This is possible but the downtime is too much. From the data export, loading to the Snowball device, and importing to the Amazon Aurora DB instance, the downtime will take at least a few days.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/snowball/latest/developer-guide/how-it-works.html\">https://docs.aws.amazon.com/snowball/latest/developer-guide/how-it-works.html</a></p><p><a href=\"https://aws.amazon.com/snowball/features/\">https://aws.amazon.com/snowball/features/</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.html\">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.html</a></p><p><a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/replicate-amazon-rds-mysql-on-premises/\">https://aws.amazon.com/premiumsupport/knowledge-center/replicate-amazon-rds-mysql-on-premises/</a></p><p><br></p><p><strong>Check out these AWS Snowball Edge and Amazon Aurora Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-snowball-edge/?src=udemy\">https://tutorialsdojo.com/aws-snowball-edge/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-aurora/?src=udemy\">https://tutorialsdojo.com/amazon-aurora/</a></p></div>"
	},
	{
		"question": "<p>A company has adopted cloud-native computing best practices for its infrastructure. The company started using AWS CloudFormation templates for defining its cloud resources, and the templates are hosted in its private GitHub repository. As the developers continuously update the templates, the company has encountered several downtimes caused by misconfigured templates, wrong executions, or the creation of unnecessary environments. The management wants to streamline the process of testing the CloudFormation templates to prevent these errors. The Solutions Architect has been tasked to create an automated solution.</p><p>Which of the following options should be implemented to meet the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Write an AWS Lambda function that syncs the private GitHub repository to AWS CodeCommit. Using AWS CodeDeploy, create a change set and execute the AWS CloudFormation template. Add an AWS CodeBuild stage on the deployment to build and run test scripts to verify the new stack.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set and execute the CloudFormation template. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set from the CloudFormation template and execute it using AWS CodeDeploy. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Write an AWS Lambda function that syncs the private GitHub repository to AWS CodeCommit. Using AWS CodeBuild, create a change set and execute the AWS CloudFormation template. Add a CodeBuild action to build and run test scripts to verify the new stack.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can apply continuous delivery practices to your <strong>AWS CloudFormation</strong> stacks using AWS CodePipeline. <strong>AWS CodePipeline</strong> is a continuous delivery service for fast and reliable application and infrastructure updates. CodePipeline builds, tests, and deploys your code every time there is a code change, based on the release process models you define.</p><p>With continuous delivery, you can automatically deploy CloudFormation template updates to your pipeline stages for testing and then promote them to production. For example, you could use CodePipeline to model an automated release process that provisions a test stack whenever an updated template is committed to a source repository (Git repositories managed by GitHub, AWS CodeCommit, and Atlassian Bitbucket) or uploaded to an Amazon S3 bucket. You can inspect the test stack and then approve it to the production stage, after which CodePipeline can delete the test stack and create a change set for final approval. When the change set is approved, CodePipeline can execute the change set and deploy the change to production.</p><p><strong>AWS CodeBuild</strong> is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeBuild scales continuously and processes multiple builds concurrently, so your builds are not left waiting in a queue.</p><p>Here is an example of a Quick Start from AWS for CI/CD Pipeline for AWS CloudFormation templates:</p><p><img src=\"https://media.tutorialsdojo.com/sap_codebuild_workflow.png\"></p><p>- A pipeline created by CodePipeline, which is triggered when a commit is made to the referenced branch of the Github repository used in the source stage.</p><p>- A build project in CodeBuild to run TaskCat and launch AWS CloudFormation templates for testing.</p><p>- An AWS Lambda function that merges the source branch of the Github repository with the release branch.</p><p>- AWS Identity and Access Management (IAM) roles for the Lambda function and the build project.</p><p>- An Amazon Simple Storage Service (Amazon S3) bucket to stash the build artifacts temporarily and to store the TaskCat report.</p><p>Therefore, the correct answer is: <strong>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set and execute the CloudFormation template. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack.</strong> With AWS CodePipeline, you can create change sets and automatically deploy CloudFormation template updates safely. You can have a CodeBuild stage for building artifacts and testing your new infra.</p><p>The option that says: <strong>Create a pipeline in AWS CodePipeline that is triggered automatically for commits on the private GitHub repository. Have the pipeline create a change set from the CloudFormation template and execute it using AWS CodeDeploy. Add an AWS CodeBuild stage on the pipeline to build and run test scripts to verify the new stack</strong> is incorrect. AWS CodeDeploy is unnecessary as you will not deploy the changes in your production environment. You need a pipeline to execute the CloudFormation change set and CodeBuild project to test your changes.</p><p>The option that says: <strong>Write an AWS Lambda function that syncs the private GitHub repository to AWS CodeCommit. Using AWS CodeDeploy, create a change set and execute the AWS CloudFormation template. Add an AWS CodeBuild stage on the deployment to build and run test scripts to verify the new stack</strong> is incorrect. You don't need a custom Lambda function as AWS CodePipiline supports executions from third-party Git sources.</p><p>The option that says: <strong>Write an AWS Lambda function that syncs the private GitHub repository to AWS CodeCommit. Using AWS CodeBuild, create a change set and execute the AWS CloudFormation template. Add a CodeBuild action to build and run test scripts to verify the new stack</strong> is incorrect. You don't need a custom Lambda function as AWS CodePipiline supports executions from third-party Git sources. You need AWS CodePipeline to create change set and execute the CloudFormation template.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2016/11/continuously-deliver-changes-to-aws-cloudformation-stacks-with-aws-codepipeline/\">https://aws.amazon.com/about-aws/whats-new/2016/11/continuously-deliver-changes-to-aws-cloudformation-stacks-with-aws-codepipeline/</a></p><p><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html\">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/continuous-delivery-codepipeline-basic-walkthrough.html</a></p><p><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/use-third-party-git-source-repositories-in-aws-codepipeline.html</a></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/cicd-taskcat/\">https://aws.amazon.com/quickstart/architecture/cicd-taskcat/</a></p><p><br></p><p><strong>Check out these AWS CodePipeline and AWS CloudFormation Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><br></p><p><strong>AWS CloudFormation - Templates, Stacks, Change Sets:</strong></p><p><a href=\"https://youtu.be/9Xpuprxg7aY\">https://youtu.be/9Xpuprxg7aY</a></p></div>"
	},
	{
		"question": "<p>A company manages more than 50 AWS accounts under its AWS Organization. All AWS accounts deploy resources on a single AWS region only. To enable routing across all accounts, each VPC has a Transit Gateway Attachment to a centralized AWS Transit Gateway. Each VPC also has an internet gateway and NAT gateway to provide outbound internet connectivity for its resources. As a security requirement, the company must have a centrally managed rule-based filtering solution for outbound internet traffic on all AWS accounts under its organization. It is expected that peak outbound traffic for each Availability Zone will not exceed 25 Gbps.</p><p>Which of the following options should the solutions architect implement to fulfill the company requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. On this VPC, create an Auto Scaling group of Amazon EC2 instances running with an open-source internet proxy software for rule-based filtering across all AZ in the region. Configure the route tables on each VPC to point to this Auto Scaling group.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. Configure an AWS Network Firewall firewall for the rule-based filtering. Modify all the default routes in each account to point to the Network Firewall endpoint.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Provision an Auto Scaling group of Amazon EC2 instances with network-optimized instance type on each AWS account. Install an open-source internet proxy software for rule-based filtering. Configure the route tables on each VPC to point to the Auto Scaling group.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use AWS Network Firewall service to create firewall rule groups and firewall policies for rule-based filtering. Attach the firewall policy to a new Network Firewall firewall on each account. Modify all the default routes in each account to point to their corresponding Network Firewall firewall.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Network Firewall</strong> is a stateful, managed, network firewall and intrusion detection and prevention service for your virtual private cloud (VPC) that you created in Amazon Virtual Private Cloud (Amazon VPC). With Network Firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic going to and coming from an internet gateway, NAT gateway, or over VPN or AWS Direct Connect.</p><p>Once AWS Network Firewall is deployed, you will see a firewall endpoint in each firewall subnet. Firewall endpoint is similar to interface endpoint and it shows up as vpce-id in your VPC route table target selection. You have multiple deployment models for Network Firewall.</p><p>For a centralized egress deployment model, an AWS Transit Gateway is a prerequisite. AWS Transit Gateway acts as a network hub and simplifies the connectivity between VPCs. For this model, we have a dedicated, central egress VPC which has a NAT gateway configured in a public subnet with access to IGW.</p><p><img src=\"https://media.tutorialsdojo.com/sap_network_firewall_central_egress.png\"></p><p>Traffic originating from spoke VPCs is forwarded to inspection VPC for processing. It is then forwarded to central egress VPC using a default route in the Transit Gateway firewall route table. The default route is set to target central egress VPC Attachment (pointing to the AWS Network Firewall endpoint).</p><p>Therefore, the correct answer is: <strong>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. Configure an AWS Network Firewall firewall for the rule-based filtering. Modify all the default routes in each account to point to the Network Firewall endpoint. </strong>This solution provides a dedicated VPC for rule-based inspection and controlling of egress traffic. Please check the references section for more details.</p><p>The option that says: <strong>Use AWS Network Firewall service to create firewall rule groups and firewall policies for rule-based filtering. Attach the firewall policy to a new Network Firewall firewall on each account. Modify all the default routes in each account to point to their corresponding Network Firewall firewall</strong> is incorrect. For centralized rule-based filtering with a Network Firewall, you will need an AWS Transit Gateway to act as a network hub and allow the connectivity between VPCs.</p><p>The option that says: <strong>Provision an Auto Scaling group of Amazon EC2 instances with network-optimized instance type on each AWS account. Install an open-source internet proxy software for rule-based filtering. Configure the route tables on each VPC to point to the Auto Scaling group</strong> is incorrect. This will be difficult to manage since you will have a proxy cluster essentially on each AWS account.</p><p>The option that says: <strong>Create a dedicated VPC for outbound internet traffic with a NAT gateway on it. Connect this VPC to the existing AWS Transit Gateway. On this VPC, create an Auto Scaling group of Amazon EC2 instances running with an open-source internet proxy software for rule-based filtering across all AZ in the region. Configure the route tables on each VPC to point to this Auto Scaling group</strong> is incorrect. This may be possible but is not recommended. AWS Network Firewall can offer centralized rule-based traffic filtering and inspection across your VPCs.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall/\">https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models-for-aws-network-firewall/</a></p><p><a href=\"https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html\">https://docs.aws.amazon.com/network-firewall/latest/developerguide/what-is-aws-network-firewall.html</a></p><p><a href=\"https://docs.aws.amazon.com/network-firewall/latest/developerguide/how-it-works.html\">https://docs.aws.amazon.com/network-firewall/latest/developerguide/how-it-works.html</a></p><p><br></p><p><strong>Check out this AWS Transit Gateway Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-transit-gateway/?src=udemy\">https://tutorialsdojo.com/aws-transit-gateway/</a></p></div>"
	},
	{
		"question": "<p>A company has a suite of IBM products in its on-premises data center such as IBM WebSphere, IBM MQ, and IBM DB2 servers. The solutions architect has been tasked to migrate all of the current systems to the AWS Cloud in the most cost-effective way and improve the availability of the cloud infrastructure.</p><p>Which of the following options is the MOST suitable solution that the solutions architect should implement to meet the company requirements?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, migrate, and re-architect the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Migrate and re-platform IBM MQ to Amazon MQ in a phased approach.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Server Migration Service to migrate your servers to AWS. Set up Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon MQ.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Server Migration Service to migrate your servers to AWS. Set up Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon SQS Standard Queue.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, re-architect, and migrate the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Re-host and migrate the IBM MQ service to Amazon SQS FIFO Queue.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>On Amazon EC2, you can run many of the proven IBM technologies with which you're already familiar. You may be eligible to bring many of your own IBM software and licenses (BYOSL) to run on Amazon EC2 instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ibm_byol_infra.png\"></p><p><strong>AWS Database Migration Service (DMS)</strong> and the <strong>AWS Schema Conversion Tool (SCT)</strong> can allow you to convert and migrate IBM Db2 databases on Linux, UNIX and Windows (Db2 LUW) to any DMS supported target. This can accelerate your move to the cloud by allowing you to migrate more of your legacy databases. The new Db2 LUW source adds to the existing list of relational database, NoSQL, and object store sources supported by DMS. If the database migration target is Amazon Aurora, Amazon Redshift or Amazon DynamoDB, you can use DMS free for six months.</p><p><img src=\"https://media.tutorialsdojo.com/sap_ibmmq_infra.png\"></p><p><strong>Amazon MQ</strong> is a managed message broker service from AWS that makes it easy to set up and operate message brokers in the cloud. To migrate and re-platform your on-premises IBM MQ to Amazon MQ, you can opt for a phased approach for the migration process. You can move the producers (senders) and consumers (receivers) in phases from your on-premises to the cloud. This process uses Amazon MQ as the message broker, and decommissions IBM MQ once all producers/consumers have been successfully migrated.</p><p>Hence, the correct option is: <strong>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, migrate and re-architect the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Migrate and re-platform IBM MQ to Amazon MQ in phased approach.</strong></p><p>The option that says: <strong>Use the AWS Server Migration Service to migrate your servers to AWS. Set up Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon SQS Standard Queue</strong> is incorrect because the AWS Server Migration Service simply automates the migration of your on-premises VMware vSphere or Microsoft Hyper-V/SCVMM virtual machines to the AWS Cloud. This service is limited to migrating virtual machines (VMs) only. Moreover, you can't re-host and migrate your IBM MQ service to Amazon SQS. A better solution is to re-platform IBM MQ to Amazon MQ.</p><p>The option that says: <strong>Use the AWS Server Migration Service to migrate your servers to AWS. Set up Amazon EC2 instances to re-host your IBM WebSphere and IBM DB2 servers separately. Re-host and migrate the IBM MQ service to Amazon MQ</strong> is incorrect because the AWS Server Migration Service simply automates the migration of your on-premises virtual machines to the AWS Cloud. This service is limited to migrating virtual machines (VMs) and in addition, you cannot directly re-host and migrate your IBM MQ to Amazon MQ since these are two completely different systems. Instead, you have to re-platform IBM MQ to Amazon MQ in a phased approach.</p><p>The option that says: <strong>Use the AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) to convert, re-architect and migrate the IBM Db2 database to Amazon Aurora. Set up an Auto Scaling group of EC2 instances with an ELB in front to migrate and re-host your IBM WebSphere. Re-host and migrate the IBM MQ service to Amazon SQS FIFO Queue</strong> is incorrect. Although the use of AWS Database Migration Service (DMS) and the AWS Schema Conversion Tool (SCT) is correct, the migration process for your IBM MQ is wrong. Amazon Simple Queue Service (SQS) is just a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. There are a lot of features in IBM MQ that are not available in Amazon SQS, whether it is a Standard or a FIFO Queue. You have to re-platform IBM MQ to Amazon MQ instead.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/ibm-websphere-liberty/\">https://aws.amazon.com/quickstart/architecture/ibm-websphere-liberty/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/\">https://aws.amazon.com/about-aws/whats-new/2018/04/aws-dms-supports-ibm-db2-as-a-source/</a></p><p><a href=\"https://aws.amazon.com/quickstart/architecture/ibm-mq/\">https://aws.amazon.com/quickstart/architecture/ibm-mq/</a></p><p><a href=\"https://developer.ibm.com/messaging/2018/09/26/ibm-mq-available-managed-service-aws/\">https://developer.ibm.com/messaging/2018/09/26/ibm-mq-available-managed-service-aws/</a></p><p><a href=\"https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach\">https://aws.amazon.com/blogs/compute/migrating-from-ibm-mq-to-amazon-mq-using-a-phased-approach</a></p><p><br></p><p><strong>Check out these cheat sheets on AWS Database Migration Service and AWS Server Migration Service:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-database-migration-service/?src=udemy\">https://tutorialsdojo.com/aws-database-migration-service/</a></p><p><a href=\"https://tutorialsdojo.com/aws-server-migration-service-sms/?src=udemy\">https://tutorialsdojo.com/aws-server-migration-service-sms/</a></p><p><br></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A company stores confidential financial documents as well as sensitive corporate information in an Amazon S3 bucket. There is a new security policy that prohibits any public S3 objects in the company's S3 bucket. In the event that a public object was identified, the IT Compliance team must be notified immediately and the object's permissions must be remediated automatically. The notification must be sent as soon as a public object was created in the bucket. </p><p>What is the MOST suitable solution that should be implemented by the Solutions Architect to comply with this data policy?</p>",
		"answers": [
			{
				"correct": true,
				"answer": "<p>Enable object-level logging in the S3 bucket to automatically track S3 actions using CloudTrail. Set up an Amazon CloudWatch Events rule with an SNS Topic to notify the IT Compliance team when a <code>PutObject</code> API call with public-read permission is detected in the CloudTrail logs. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Set up a Systems Manager (SSM) Automation document that changes any public object in the bucket to private. Integrate CloudWatch Events with AWS Lambda to create a scheduled process that checks the S3 bucket every hour. Configure the Lambda function to invoke the SSM Automation document when a public object is identified and to notify the IT Compliance team via email using Amazon SNS.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Integrate Amazon Lex with Amazon GuardDuty to detect public objects in the S3 bucket and to automatically update the permission of a public object to private. Associate an SNS Topic to Amazon Lex to notify the IT Compliance team via email if a public object was identified.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Automatically track S3 actions using the Trusted Advisor API and AWS Cloud Development Kit (CDK). Set up an Amazon CloudWatch Events rule with an Amazon SNS Topic to notify the IT Compliance team when the Trusted Advisor detected a <code>PutObject</code> API call with public-read permission. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon S3</strong> is integrated with <strong>AWS CloudTrail</strong>, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3. CloudTrail captures a subset of API calls for Amazon S3 as events, including calls from the Amazon S3 console and from code calls to the Amazon S3 APIs. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for Amazon S3. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in the Event history. Using the information collected by CloudTrail, you can determine the request that was made to Amazon S3, the IP address from which the request was made, who made the request, when it was made, and additional details.</p><p><img src=\"https://media.tutorialsdojo.com/sap_cloudtrail_s3_lambda_sns.jpg\"></p><p>You can also get CloudTrail logs for object-level Amazon S3 actions. To do this, specify the Amazon S3 object for your trail. When an object-level action occurs in your account, CloudTrail evaluates your trail settings. If the event matches the object that you specified in a trail, the event is logged.</p><p><strong>Amazon CloudWatch Events</strong> delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. Using simple rules that you can quickly set up, you can match events and route them to one or more target functions or streams. CloudWatch Events becomes aware of operational changes as they occur. CloudWatch Events responds to these operational changes and takes corrective action as necessary, by sending messages to respond to the environment, activating functions, making changes, and capturing state information.</p><p>Hence, the correct answer is: <strong>Enable object-level logging in the S3 bucket to automatically track S3 actions using CloudTrail. Set up an Amazon CloudWatch Events rule with an SNS Topic to notify the IT Compliance team when a </strong><code><strong>PutObject</strong></code><strong> API call with public-read permission is detected in the CloudTrail logs. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private.</strong></p><p>The option that says: <strong>Set up a Systems Manager (SSM) Automation document that changes any public object in the bucket to private. Integrate CloudWatch Events with AWS Lambda to create a scheduled process that checks the S3 bucket every hour. Configure the Lambda function to invoke the SSM Automation document when a public object was identified and to notify the IT Compliance team via email using Amazon SNS</strong> is incorrect because the requirement says that the notification must be sent as soon as a public object was created in the bucket. Running the process to check for public objects ever hour will cause delays in detecting a public object in the S3 bucket.</p><p>The option that says: <strong>Automatically track S3 actions using the Trusted Advisor API and AWS Cloud Development Kit (CDK). Set up an Amazon CloudWatch Events rule with an Amazon SNS Topic to notify the IT Compliance team when the Trusted Advisor detected a </strong><code><strong>PutObject</strong></code><strong> API call with public-read permission. Launch another CloudWatch Events rule that invokes an AWS Lambda function to turn the newly uploaded public object to private</strong> is incorrect. Using the Trusted Advisor as a web service to track all S3 actions is not enough as it only provides high-level data about your S3 bucket, excluding the object-level permissions. A better solution is to enable the object-level logging in the S3 bucket.</p><p>The option that says: <strong>Integrate Amazon Lex with Amazon GuardDuty to detect public objects in the S3 bucket and to automatically update the permission of a public object to private. Associate an SNS Topic to Amazon Lex to notify the IT Compliance team via email if a public object was identified</strong> is incorrect because Amazon Lex is simply a service for building conversational interfaces into any application using voice and text. In addition, Amazon GuardDuty is just a threat detection service that analyzes continuous streams of meta-data generated from your account and network activity found in AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs. These two services are not suitable in detecting public objects in your S3 bucket.</p><p><br></p><p><strong>References</strong>:</p><p><a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html\">https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/log-s3-data-events.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html</a></p><p><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-request-identification.html\">https://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-request-identification.html</a></p><p><br></p><p><strong>Check out this Amazon CloudWatch Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-cloudwatch/?src=udemy\">https://tutorialsdojo.com/amazon-cloudwatch/</a></p></div>"
	},
	{
		"question": "A supermarket chain is planning to launch an online shopping website to allow its loyal shoppers to buy their groceries online. Since there are a lot of online shoppers at any time of the day, the website should be highly available 24/7 and fault tolerant. \n\nWhich of the following options provides the best architecture that meets the above requirement?",
		"answers": [
			{
				"correct": false,
				"answer": "Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances each and an RDS instance deployed with Read Replicas in two separate Availability Zones."
			},
			{
				"correct": true,
				"answer": "Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and a RDS configured with Multi-AZ Deployments."
			},
			{
				"correct": false,
				"answer": "<p>Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and an Amazon RDS database running in a single Reserved EC2 Instance.</p>"
			},
			{
				"correct": false,
				"answer": "Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and one RDS instance deployed with Read Replicas in the two separate Availability Zones."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>For high availability, it is best to always choose an RDS instance configured with Multi-AZ deployments. You should also deploy your application across multiple Availability Zones to improve fault tolerance. AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to set up application scaling for multiple resources across multiple services in minutes.</p><p><img src=\"https://media.tutorialsdojo.com/sap_multiaz_multiregion_readreplica.png\"></p><p>Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database (DB) instances, making them a natural fit for production database workloads. When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/#Availability_Zones\">Availability Zone</a> (AZ). Each AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable. In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of <a href=\"https://aws.amazon.com/rds/aurora/\">Amazon Aurora</a>), so that you can resume database operations as soon as the failover is complete. Since the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.</p><p>Hence, the correct answer is the option that says: <strong>Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and an RDS configured with Multi-AZ Deployments.</strong></p><p>The option that says: <strong>Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances each and an RDS instance deployed with Read Replicas in two separate Availability Zones</strong> is incorrect because a Read Replica is primarily used to improve the scalability of the database. This architecture will neither be highly available 24/7 nor fault-tolerant in the event of an AZ outage.</p><p>The option that says: <strong>Deploy the website across 3 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and one RDS instance deployed with Read Replicas in the two separate Availability Zones</strong> is incorrect. Although the EC2 instances are highly available, the database tier is not. You have to use an Amazon RDS database with Multi-AZ configuration.</p><p>The option that says: <strong>Deploy the website across 2 Availability Zones with Auto Scaled EC2 instances behind an Application Load Balancer and an Amazon RDS database running in a single Reserved EC2 Instance</strong> is incorrect. With this architecture, the single Reserved Instance is only deployed in a single AZ. In the event of an AZ outage, the entire database will be unavailable.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/autoscaling/\">https://aws.amazon.com/autoscaling/</a></p><p><a href=\"https://aws.amazon.com/rds/features/multi-az/\">https://aws.amazon.com/rds/features/multi-az/</a></p><p><br></p><p><strong>Check out this AWS Auto Scaling Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-auto-scaling/?src=udemy\">https://tutorialsdojo.com/aws-auto-scaling/</a></p></div>"
	},
	{
		"question": "<p>A company has several applications written in TypeScript and Python hosted on the AWS cloud. The company uses an automated deployment solution for its applications using AWS CloudFormation templates and AWS CodePipeline. The company recently acquired a new business unit that uses Python scripts to deploy applications on AWS. The developers from the new business are having difficulty migrating their deployments to AWS CloudFormation because they need to learn a new domain-specific language and their old Python scripts require programming loops, which are not supported in CloudFormation.</p><p>Which of the following is the recommended solution to address the developers’ concerns and help them update their deployment procedures?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Write new CloudFormation templates for the deployments of the new business unit. Extract parts of the Python scripts to be added as EC2 user data. Deploy the CloudFormation templates using the AWS Cloud Development Kit (AWS CDK). Add a stage on AWS CodePipline to integrate AWS CDK using the templates for the application deployment.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create a standard deployment process for the company and the new business unit by leveraging a third-party resource provisioning engine on AWS CodeBuild. Add a stage on AWS CodePipeline to integrate AWS CodeBuild on the application deployment.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Write TypeScript or Python code that will define AWS resources. Convert these codes to AWS CloudFormation templates by using AWS Cloud Development Kit (AWS CDK). Create CloudFormation stacks using AWS CDK. Create an AWS CodeBuild job that includes AWS CDK and add this stage on AWS CodePipeline.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Import the Python scripts on AWS OpsWorks which can then be integrated with AWS CodePipeline. Ask the developers to write Chef recipes that run can run the Python scripts for application deployment on AWS.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Cloud Development Kit (AWS CDK)</strong> is a software development framework for defining cloud infrastructure in code and provisioning it through AWS CloudFormation.</p><p><strong>AWS CloudFormation</strong> enables you to:</p><p>- Create and provision AWS infrastructure deployments predictably and repeatedly.</p><p>- Leverage AWS products such as Amazon EC2, Amazon Elastic Block Store, Amazon SNS, Elastic Load Balancing, and Auto Scaling.</p><p>- Build highly reliable, highly scalable, cost-effective applications in the cloud without worrying about creating and configuring the underlying AWS infrastructure.</p><p>- Use a template file to create and delete a collection of resources together as a single unit (a stack).</p><p>Use the AWS CDK to define your cloud resources in a familiar programming language. The AWS CDK supports TypeScript, JavaScript, Python, Java, and C#/.Net. Developers can use one of the supported programming languages to define reusable cloud components known as Constructs.</p><p><img src=\"https://media.tutorialsdojo.com/sap_aws_cdk_appstacks.png\"></p><p>The <strong>AWS Cloud Development Kit (AWS CDK)</strong> lets you define your cloud infrastructure as code in one of five supported programming languages. It is intended for moderately to highly experienced AWS users. An AWS CDK app is an application written in TypeScript, JavaScript, Python, Java, or C# that uses the AWS CDK to define AWS infrastructure. An app defines one or more stacks. Stacks (equivalent to AWS CloudFormation stacks) contain constructs, each of which defines one or more concrete AWS resources, such as Amazon S3 buckets, Lambda functions, Amazon DynamoDB tables, and so on.</p><p>Constructs (as well as stacks and apps) are represented as types in your programming language of choice. You instantiate constructs within a stack to declare them to AWS and connect them to each other using well-defined interfaces.</p><p>The AWS CDK lets you easily define applications in the AWS Cloud using your programming language of choice. To deploy your applications, you can use <strong>AWS CodeCommit, AWS CodeBuild, AWS CodeDeploy, and AWS CodePipeline</strong>. Together, they allow you to build what's called a deployment pipeline for your application.</p><p>Therefore, the correct answer is: <strong>Write TypeScript or Python code that will define AWS resources. Convert these codes to AWS CloudFormation templates by using AWS Cloud Development Kit (AWS CDK). Create CloudFormation stacks using AWS CDK. Create an AWS CodeBuild job that includes AWS CDK and add this stage on AWS CodePipeline.</strong> With this solution, the developers no longer need to learn the AWS CloudFormation specific language as they can continue writing TypeScript or Python scripts. The AWS CDK stacks can be converted to AWS CloudFormation templates which can be integrated into the company deployment process.</p><p>The option that says: <strong>Write new CloudFormation templates for the deployments of the new business unit. Extract parts of the Python scripts to be added as EC2 user data. Deploy the CloudFormation templates using the AWS Cloud Development Kit (AWS CDK). Add a stage on AWS CodePipline to integrate AWS CDK using the templates for the application deployment</strong> is incorrect. This is possible but you don't have to write new CloudFormation templates. AWS CDK can convert the TypeScript and Python code to create AWS CDK stacks and convert them to CloudFormation templates.</p><p>The option that says: <strong>Create a standard deployment process for the company and the new business unit by leveraging a third-party resource provisioning engine on AWS CodeBuild. Add a stage on AWS CodePipeline to integrate AWS CodeBuild on the application deployment</strong> is incorrect. You don't have to rely on third-party resources to standardize the deployment process. AWS CDK can help in creating CloudFormation templates based on the TypeScript or Python code.</p><p>The option that says: <strong>Import the Python scripts on AWS OpsWorks which can then be integrated with AWS CodePipeline. Ask the developers to write Chef recipes that run can run the Python scripts for application deployment on AWS</strong> is incorrect. This is not recommended because the developers will need to learn a new domain-specific language to write Chef recipes.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html\">https://docs.aws.amazon.com/cdk/latest/guide/codepipeline_example.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/home.html\">https://docs.aws.amazon.com/cdk/latest/guide/home.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html\">https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html</a></p><p><br></p><p><strong>Check out these AWS CloudFormation and AWS CodePipeline Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-cloudformation/?src=udemy\">https://tutorialsdojo.com/aws-cloudformation/</a></p><p><a href=\"https://tutorialsdojo.com/aws-codepipeline/?src=udemy\">https://tutorialsdojo.com/aws-codepipeline/</a></p></div>"
	},
	{
		"question": "<p>A company plans to migrate its on-premises legacy application to AWS and develop a highly scalable application. Currently, all user requests are sent to the on-premises load balancer which forwards the requests to two Linux servers hosting the legacy application. The database is hosted on two servers in master-master configuration. Since this is an old application, the communication to the database servers is done through static IP addresses and not via DNS names. The license of the application is tied to the MAC address of the network adapter of the Linux server. If the application is to be installed on a new server, it will take about 15 hours for the software vendor to send the new license via email.</p><p>Which combination of actions must be done to meet the company requirements? (Select TWO.)</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files inside an Amazon EC2 instance and create a base AMI from this EC2 instance. Use bootstrap scripts to configure license keys and attach the corresponding ENI when provisioning EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create an Amazon EC2 bootstrap script that will resolve the database DNS names into IP addresses. Update the local configuration files with the resolved values.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Create an AWS Lambda function to update the database IP addresses on the Systems Manager Parameter Store. Create an Amazon EC2 bootstrap script that will retrieve the database IP address from SSM Parameter Store. Update the local configuration files with the parameters.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files on an Amazon S3 bucket and use bootstrap scripts to retrieve an unused license file and attach corresponding ENI when provisioning EC2 instances.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Install the application on an EC2 instance and configure the needed license file. Update the local configuration with the database IP addresses. Use this instance as the base AMI for all instances in the Auto Scaling group.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>AWS Systems Manager Parameter Store</strong> provides secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values. You can store values as plain text or encrypted data. You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows by using the unique name that you specified when you created the parameter.</p><p>Parameter Store offers these benefits:</p><p>You can use a secure, scalable, hosted secrets management service with no servers to manage.</p><p>Improve your security posture by separating your data from your code.</p><p>Store configuration data and encrypted strings in hierarchies and track versions.</p><p>Control and audit access at granular levels.</p><p>Parameter Store provides support for three types of parameters: <code>String</code>, <code>StringList</code>, and <code>SecureString</code>.</p><p><img src=\"https://tutorialsdojo.com/wp-content/uploads/2020/06/image-1-article-june-24.jpg\"></p><p>An <strong>elastic network interface (ENI)</strong> is a logical networking component in a VPC that represents a virtual network card. It can include the following attributes:</p><p>A primary private IPv4 address from the IPv4 address range of your VPC</p><p>One or more secondary private IPv4 addresses from the IPv4 address range of your VPC</p><p>One Elastic IP address (IPv4) per private IPv4 address</p><p>One public IPv4 address</p><p>One or more IPv6 addresses</p><p>One or more security groups</p><p>A MAC address</p><p>A source/destination check flag</p><p>A description</p><p>You can create and configure network interfaces in your account and attach them to instances in your VPC. Your account might also have requester-managed network interfaces, which are created and managed by AWS services to enable you to use other resources and services.</p><p>You can create a network interface, attach it to an instance, detach it from an instance, and attach it to another instance. The attributes of a network interface follow it as it's attached or detached from an instance and reattached to another instance. When you move a network interface from one instance to another, network traffic is redirected to the new instance.</p><p>Each instance has a default network interface, called the primary network interface. You cannot detach a primary network interface from an instance. You can create and attach additional network interfaces.</p><p>The option that says: <strong>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files on an Amazon S3 bucket and use bootstrap scripts to retrieve an unused license file and attach corresponding ENI when provisioning EC2 instances</strong> is correct. Having the license files on an Amazon S3 bucket reduces the management overhead for the EC2 instances, as you can easily add/remove more license keys if needed.</p><p>The option that says: <strong>Create an AWS Lambda function to update the database IP addresses on the Systems Manager Parameter Store. Create an Amazon EC2 bootstrap script that will retrieve the database IP address from SSM Parameter Store. Update the local configuration files with the parameters</strong> is correct. Having the database IP addresses on Parameter Store ensures that all the EC2 instances will have a central location to retrieve the IP addresses. This also reduces the need to constantly update any script from inside the EC2 instance even if you add/remove more databases in the future.</p><p>The option that says: <strong>Provision a pool of Elastic Network Interfaces (ENIs). Request a license file for each ENI from the software vendor. Store the license files inside an Amazon EC2 instance and create a base AMI from this EC2 instance. Use bootstrap scripts to configure license keys and attach the corresponding ENI when provisioning EC2 instances</strong> is incorrect. Although this is possible, this is not an ideal solution. If you need more EC2 instances and more ENIs, you will have to manually update your base AMI to include all the new license files. This can be a lot of work if you scale your cluster on a regular basis.</p><p>The option that says: <strong>Create an Amazon EC2 bootstrap script that will resolve the database DNS names into IP addresses. Update the local configuration files with the resolved values</strong> is incorrect. Although this is possible, this is not recommended. You will have to update the bootstrap script manually for the new DNS name every time you create a new database such as when you are scaling out your database instances.</p><p>The option that says: <strong>Install the application on an EC2 instance and configure the needed license file. Update the local configuration with the database IP addresses. Use this instance as the base AMI for all instances in the Auto Scaling group</strong> is incorrect. This will not work because the application license is tied to the MAC address on which it was installed. When you provision a new EC2 instance, it will have a new IP address and a newly assigned MAC address for its network adapter.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html</a></p><p><a href=\"https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html\">https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html</a></p><p><a href=\"https://docs.aws.amazon.com/cdk/latest/guide/get_ssm_value.html\">https://docs.aws.amazon.com/cdk/latest/guide/get_ssm_value.html</a></p><p><br></p><p><strong>Check out these AWS SSM Parameter Store and Amazon EC2 Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/?src=udemy\">https://tutorialsdojo.com/aws-secrets-manager-vs-systems-manager-parameter-store/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/?src=udemy\">https://tutorialsdojo.com/amazon-elastic-compute-cloud-amazon-ec2/</a></p></div>"
	},
	{
		"question": "<p>A logistics company is running its business application on Amazon EC2 instances. The web application is running on an Auto Scaling group of EC2 instances behind an Application Load Balancer. The self-managed MySQL database is also running on a large EC2 instance to handle the heavy I/O operations needed by the application. The application is able to handle the amount of traffic during normal hours. However, the performance slows down significantly during the last four days of the month as more users run their month-end reports simultaneously. The Solutions Architect was tasked to improve the performance of the application, especially during the peak days.</p><p>Which of the following should the Solutions Architect implement to improve the application performance with the LEAST impact on availability?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Take a snapshot of the EBS volumes with I/O heavy operations and replace them with Provisioned IOPS volumes during the end of the month. Revert to the old EBS volume type afterward to save on costs.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Convert all EBS volumes of the EC2 instances to GP2 volumes to improve I/O performance. Scale up the EC2 instances into bigger instance types. Pre-warm the Application Load Balancer to handle sudden spikes in traffic.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Create Amazon CloudWatch metrics based on EC2 instance CPU usage or response time on the ALB. Trigger an AWS Lambda function to change the instance size, type, and the allocated IOPS of the EBS volumes based on the breached threshold.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Migrate the Amazon EC2 database instance to Amazon RDS for MySQL. Add more read replicas to the database cluster during the end of the month to handle the spike in traffic.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p><strong>Amazon Relational Database Service (Amazon RDS)</strong> is a web service that makes it easier to set up, operate, and scale a relational database in the AWS Cloud. It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.</p><p>Amazon RDS supports the most demanding database applications. You can choose between two SSD-backed storage options: one optimized for high-performance OLTP applications, and the other for cost-effective general-purpose use. You can scale your database's compute and storage resources with only a few mouse clicks or an API call, often with no downtime.</p><p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput. Read replicas can also be promoted when needed to become standalone DB instances.</p><p><img src=\"https://media.tutorialsdojo.com/sap_RDSreadreplica_async.jpg\"></p><p>You can reduce the load on your source DB instance by routing read queries from your applications to the read replica. Read replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. Because read replicas can be promoted to master status, they are useful as part of a sharding implementation.</p><p>In this scenario, the Amazon EC2 instances are in an Auto Scaling group already which means that the database read operations is the possible bottleneck especially during the month-end wherein the reports are generated. This can be solved by creating RDS read replicas.</p><p>Therefore, the correct answer is: <strong>Migrate the Amazon EC2 database instance to Amazon RDS for MySQL. Add more read replicas to the database cluster during the end of the month to handle the spike in traffic.</strong></p><p>The option that says: <strong>Convert all EBS volumes of the EC2 instances to GP2 volumes to improve I/O performance. Scale up the EC2 instances into bigger instance types. Pre-warm the Application Load Balancer to handle sudden spikes in traffic</strong> is incorrect. The Amazon EC2 instances are in an Auto Scaling group already which means that the database read operations is the possible bottleneck so changing to bigger EC2 instances will just increase the costs unnecessarily.</p><p>The option that says: <strong>Create Amazon CloudWatch metrics based on EC2 instance CPU usage or response time on the ALB. Trigger an AWS Lambda function to change the instances size, type, and the allocated IOPS of the EBS volumes based on the breached threshold</strong> is incorrect. This will cause a lot of interruption on the application when you change the EBS volumes and the instance type for the EC2 database instance. You will have to reboot the database instances when you change the instance type.</p><p>The option that says: <strong>Take a snapshot of the EBS volumes with I/O heavy operations and replace them with Provisioned IOPS volumes during the end of the month. Revert to the old EBS volume type afterward to save on costs </strong>is incorrect. Creating snapshots will cause the disk write operations to temporarily stop in which the database will stop responding to write requests. Changing the disk types every month is not ideal as this also causes downtime on the application during the switch. A better solution for heavy read operations is to provision an Amazon RDS database with Read Replicas.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html\">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html</a></p><p><a href=\"https://aws.amazon.com/rds/features/read-replicas/\">https://aws.amazon.com/rds/features/read-replicas/</a></p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/\">https://aws.amazon.com/about-aws/whats-new/2018/01/amazon-rds-read-replicas-now-support-multi-az-deployments/</a></p><p><br></p><p><strong>Check out this Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A legal consulting firm is running a WordPress website on EC2 instances deployed across multiple Availability Zones with a Multi-AZ RDS MySQL database instance. Their website is designed to use an eventual consistency model and performs a high number of read and write operations. There is a growing number of people who are reporting that the website is slow and after checking, the root cause is due to the slow read processing in your database tier.&nbsp; &nbsp;Which of the following options would solve this issue? (Select TWO.) </p>",
		"answers": [
			{
				"correct": false,
				"answer": "Implement sharding to distribute the incoming load to multiple RDS MySQL instances. "
			},
			{
				"correct": false,
				"answer": "<p>Set up a Redis in-memory cache cluster running in an EC2 instance on each Availability Zone and enable replication on all nodes.</p>"
			},
			{
				"correct": true,
				"answer": "Add an RDS MySQL Read Replica in each Availability Zone."
			},
			{
				"correct": false,
				"answer": "Upgrade the RDS MySQL instance to use provisioned IOPS."
			},
			{
				"correct": true,
				"answer": "<p>Deploy an Amazon ElastiCache Cluster with nodes running in each Availability Zone.</p>"
			},
			{
				"correct": false,
				"answer": "Upgrade the instance type of the RDS MySQL database instance to a larger type."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>To improve the read performance of the application, you can use RDS Read Replicas and ElastiCache.</p><p><strong>Amazon ElastiCache</strong> is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud. The service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.</p><p><img src=\"https://media.tutorialsdojo.com/sap_rds_elasticache.JPG\"></p><p><strong>Amazon RDS Read Replicas</strong> provide enhanced performance and durability for database (DB) instances. This replication feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads. You can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.</p><p>The option that says: <strong>Add an RDS MySQL Read Replica in each Availability Zone. This improves the database retrieval time as there will be more nodes that can serve the database request</strong> is correct. You won't need to increase the master instance size to accommodate all the read loads.</p><p>The option that says: <strong>Deploy an Amazon ElastiCache Cluster with nodes running in each Availability Zone</strong> is correct. Adding a cache will significantly reduce the retrieval time for frequent database queries. This also reduces the load significantly on the database tier.</p><p>The option that says: <strong>Upgrade the RDS MySQL instance to use provisioned IOPS</strong> is incorrect. This provides an increase in IOPS for the database tier, however, this is also very expensive to implement since provisioned IOPS costs a lot more.</p><p>The option that says: <strong>Implement sharding to distribute the incoming load to multiple RDS MySQL instances</strong> is incorrect. Although this could provide improvement to the system in comparison with RDS Read Replica and ElastiCache, this implementation also introduces more complexity and costs on your RDS cluster.</p><p>The option that says: <strong>Set up a Redis in-memory cache cluster running in an EC2 instance on each Availability Zone and enabling replication on all nodes</strong> is incorrect. You will still have to manually set up the Redis Cache on each AZ, and configure replication on each node. That entails a lot of effort in comparison with just using ElastiCache.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/elasticache/\">https://aws.amazon.com/elasticache/</a></p><p><a href=\"https://aws.amazon.com/rds/details/read-replicas/\">https://aws.amazon.com/rds/details/read-replicas/</a></p><p><br></p><p><strong>Check out these Amazon Elasticache and Amazon RDS Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-elasticache/?src=udemy\">https://tutorialsdojo.com/amazon-elasticache/</a></p><p><a href=\"https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/?src=udemy\">https://tutorialsdojo.com/amazon-relational-database-service-amazon-rds/</a></p></div>"
	},
	{
		"question": "<p>A data analytics company has recently adopted a hybrid cloud infrastructure with AWS. They are in the business of collecting and processing vast amounts of data. Each data set generates up to several thousands of files which can range from 10 MB to 1 GB in size. The archived data is rarely restored and in case there is a request to retrieve it, the company has a maximum of 24 hours to send the files. The data sets can be searched using its file ID, set name, authors, tags, and other criteria. </p><p>Which of the following options provides the most cost-effective architecture to meet the above requirements?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>1. Store the files of the completed data sets into a single S3 bucket.</p><p>2. Store the S3 object key for the compressed files along with other search metadata in a DynamoDB table.</p><p>3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>1. For each completed data set, compress and concatenate all of the files into a single Glacier archive. </p><p>2. Store the associated archive ID for the compressed files along with other search metadata in a DynamoDB table. </p><p>3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the retrieved archive ID.</p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Store individual compressed files to an S3 bucket. Also store the search metadata and the S3 object key of the files in a separate S3 bucket.</p><p>2. Create a lifecycle rule to move the data from an S3 Standard class to Glacier after a certain a month.</p><p>3. For retrieving the data, query the S3 bucket for files matching the search criteria and then retrieve the file from the other S3 bucket. </p>"
			},
			{
				"correct": false,
				"answer": "<p>1. Store individual files in Glacier using the filename as the archive name.</p><p>2. For retrieving the data, query the Glacier vault for files matching the search criteria.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>You can further lower the cost of storing data by compressing it to a zip or tar file. In addition, searching for archives in Glacier takes a long time, which is why it is advisable to store the search criteria and archive ID to a database for faster search. You can alternatively use Glacier Select to perform filtering operations using simple Structured Query Language (SQL).</p><p><img src=\"https://media.tutorialsdojo.com/sap_dynamodb_s3_glacier.png\"></p><p>In this scenario, this option provides a more cost-effective option for the given architecture:</p><p><strong>1. For each completed data set, compress and concatenate all of the files into a single Glacier archive.</strong></p><p><strong>2. Store the associated archive ID for the compressed files along with other search metadata in a DynamoDB table.</strong></p><p><strong>3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the retrieved archive ID.</strong></p><p><br></p><p>The following option is incorrect because storing the data in an S3 Standard class is costly. It is more cost-effective to use Amazon Glacier instead.</p><p><strong>1. Store the files of the completed data sets into a single S3 bucket. </strong><br><strong> 2. Store the S3 object key for the compressed files along with other search metadata in a DynamoDB table. </strong><br><strong> 3. For retrieving the data, query the DynamoDB table for files that match the search criteria and then restore the files from the S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because initially storing the archive to an S3 Standard class for a month still entails additional cost. Since the company allows a maximum of 24 hours to retrieve the files, you can directly store the archives to Glacier instead:</p><p><strong>1. Store individual compressed files to an S3 bucket. Also store the search metadata and the S3 object key of the files in a separate S3 bucket. </strong><br><strong> 2. Create a lifecycle rule to move the data from an S3 Standard class to Glacier after a certain a month. </strong><br><strong> 3. For retrieving the data, query the S3 bucket for files matching the search criteria and then retrieve the file from the other S3 bucket.</strong></p><p><br></p><p>The following option is incorrect because Glacier doesn't have a built-in search function to help you retrieve the data. You have to store the archive ID to a database, such as DynamoDB, to help you effectively search the required data:</p><p><strong>1. Store individual files in Glacier using the filename as the archive name. </strong><br><strong>2. For retrieving the data, query the Glacier vault for files matching the search criteria.</strong></p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/glacier-select.html</a></p><p><a href=\"https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\">https://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html</a></p><p><br></p><p><strong>Amazon S3 and S3 Glacier Overview:</strong></p><p><a href=\"https://youtu.be/1ymyeN2tki4\">https://youtu.be/1ymyeN2tki4</a></p><p><strong>Check out this Amazon S3 Glacier Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-glacier/?src=udemy\">https://tutorialsdojo.com/amazon-glacier/</a></p><p><strong>Tutorials Dojo's AWS Certified Solutions Architect Professional Exam Study Guide:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-certified-solutions-architect-professional/?src=udemy\">https://tutorialsdojo.com/aws-certified-solutions-architect-professional/</a></p></div>"
	},
	{
		"question": "<p>A cryptocurrency exchange company has recently signed up for a 3rd party online auditing system, which is also using AWS, to perform regulatory compliance audits on their cloud systems. The online auditing system needs to access certain AWS resources in your network to perform the audit.</p><p>In this scenario, which of the following approach is the most secure way of providing access to the 3rd party online auditing system?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign a policy that allows full and unrestricted access to all AWS resources."
			},
			{
				"correct": false,
				"answer": "Create a new IAM user and assign a user policy to the IAM user that allows full and unrestricted access to all AWS resources. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company."
			},
			{
				"correct": false,
				"answer": "Create a new IAM user and assign a user policy to the IAM user that allows only the actions required by the online audit system. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company."
			},
			{
				"correct": true,
				"answer": "Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign it a policy that allows only the actions required for the compliance audit."
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>An IAM <em>role</em> is an IAM identity that you can create in your account that has specific permissions. An IAM role is similar to an IAM user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS. However, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. Also, a role does not have standard long-term credentials such as a password or access keys associated with it. Instead, when you assume a role, it provides you with temporary security credentials for your role session.</p><p>You can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources. For example, you might want to grant users in your AWS account access to resources they don't usually have, or grant users in one AWS account access to resources in another account. Or you might want to allow a mobile app to use AWS resources, but not want to embed AWS keys within the app (where they can be difficult to rotate and where users can potentially extract them). Sometimes you want to give AWS access to users who already have identities defined outside of AWS, such as in your corporate directory. Or, you might want to grant access to your account to third parties so that they can perform an audit on your resources.</p><p><img src=\"https://media.tutorialsdojo.com/sap_iam_assume_role_cross_account.png\"></p><p>To allow users from one AWS account to access resources in another AWS account, create a role that defines who can access it and what permissions it grants to users that switch to it. Use IAM roles to delegate access within or between AWS accounts. By setting up cross-account access, you don't need to create individual IAM users in each account in order to provide access to different AWS accounts.</p><p>The option that says: <strong>Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign it a policy that allows only the actions required for the compliance audit </strong>is correct because it uses an IAM role and only provides the needed access, which adheres to the principle of least privilege.</p><p>The option that says:<strong><em> </em>Create a new IAM role for cross-account access which allows the online auditing system account to assume the role. Assign a policy that allows full and unrestricted access to all AWS resources</strong> is incorrect. Although you are right to use an IAM role, you should provide only the access needed by the 3rd party audit system and not a full, unrestricted access to all AWS resources.</p><p>The option that says: <strong>Create a new IAM user and assign a user policy to the IAM user that allows only the actions required by the online audit system. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company</strong> is incorrect as you need to use an IAM role instead of an IAM user.</p><p>The option that says: <strong>Create a new IAM user and assign a user policy to the IAM user that allows full and unrestricted access to all AWS resources. Create a new access and secret key for the IAM user and provide these credentials to the 3rd party auditing company</strong> is incorrect as you need to use an IAM role instead of an IAM user. In addition, you should provide only the access needed by the 3rd party audit system and not a full, unrestricted access to all AWS resources.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html</a></p><p><a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\">https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html</a></p><p><br></p><p><strong>Check out this AWS IAM Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/aws-identity-and-access-management-iam/?src=udemy\">https://tutorialsdojo.com/aws-identity-and-access-management-iam/</a></p></div>"
	},
	{
		"question": "<p>A company has deployed a multi-tier web application on AWS that uses Compute Optimized Instances for server-side processing and Storage Optimized EC2 Instances to store various media files. To ensure data durability, there is a scheduled job that replicates the files to each EC2 instance. The current architecture worked for a few months but it started to fail as the number of files grew, which is why the management decided to redesign the system.</p><p>Which of the following options should the solutions architect implement in order to launch a new architecture with improved data durability and cost-efficiency?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>Migrate and host the entire web application to Amazon S3 for a more cost-effective web hosting. Enable cross-region replication to improve data durability. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate all media files to Amazon EFS then attach this new drive as a mount point to a new set of Storage Optimized EC2 Instances. For the web servers, set up an Elastic Load Balancer with an Auto Scaling of EC2 instances and use this as the origin for a new Amazon CloudFront web distribution. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>"
			},
			{
				"correct": false,
				"answer": "<p>Migrate the web application to AWS Elastic Beanstalk and move all media files to Amazon EFS for a durable and scalable storage. Set up an Amazon CloudFront distribution with EFS as the origin. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>"
			},
			{
				"correct": true,
				"answer": "<p>Migrate all media files to an Amazon S3 bucket and use this as the origin for the new CloudFront web distribution. Set up an Elastic Load Balancer with an Auto Scaling of EC2 instances to host the web servers. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>Cloud storage is a cloud computing model that stores data on the Internet through a cloud computing provider who manages and operates data storage as a service. It’s delivered on demand with just-in-time capacity and costs, and eliminates buying and managing your own data storage infrastructure. This gives you agility, global scale and durability, with 'anytime, anywhere' data access.</p><p><strong>AWS Trusted Advisor</strong> is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices. Whether establishing new workflows, developing applications, or as part of ongoing improvement, take advantage of the recommendations provided by Trusted Advisor on a regular basis to help keep your solutions provisioned optimally.</p><p><strong>AWS Cost Explorer</strong> has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. Get started quickly by creating custom reports that analyze cost and usage data, both at a high level and for highly-specific requests. Using AWS Cost Explorer, you can dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies.</p><p><img src=\"https://media.tutorialsdojo.com/sap_efs_s3_ebs_table.png\"></p><p>In this scenario, you can use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings. For data storage, you can either use S3 or EFS to store the media files. However, S3 is cheaper than EFS and is more suitable to use in storing static media files.</p><p>Therefore, the correct answer is: <strong>Migrate all media files to an Amazon S3 bucket and use this as the origin for the new CloudFront web distribution. Set up an Elastic Load Balancer with an Auto Scaling of EC2 instances to host the web servers. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings.</strong></p><p>The option that says: <strong>Migrate and host the entire web application to Amazon S3 for a more cost-effective web hosting. Enable cross-region replication to improve data durability. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings</strong> is incorrect. Amazon S3 is not capable to handle server-side processing as this can only be used for static websites. Moreover, you can only use Consolidated Billing if your account is configured to use AWS Organizations.</p><p>The option that says: <strong>Migrate all media files to Amazon EFS then attach this new drive as a mount point to a new set of Storage Optimized EC2 Instances. For the web servers, set up an Elastic Load Balancer with an Auto Scaling of EC2 instances and use this as the origin for a new Amazon CloudFront web distribution. Use a combination of Cost Explorer and AWS Trusted advisor checks to monitor the operating costs and identify potential savings</strong> is incorrect. Although this new setup may work, it entails a higher cost to maintain a new set of Storage Optimized EC2 Instances along with EFS. There is also an added cost of maintaining your web-tier which is comprised of an Elastic Load Balancer with another set of Auto Scaling of EC2 instances. It is also better to use S3 instead of EFS since you are only storing media files and not documents which requires file locking or POSIX-compliant storage.</p><p>The option that says: <strong>Migrate the web application to AWS Elastic Beanstalk and move all media files to Amazon EFS for a durable and scalable storage. Set up an Amazon CloudFront distribution with EFS as the origin. Use a combination of Consolidated Billing and AWS Trusted advisor checks to monitor the operating costs and identify potential savings</strong> is incorrect. You cannot set EFS as the origin of your CloudFront web distribution. The scenario also doesn't mention the use of AWS Organization, which is why Consolidated Billing is not applicable.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://aws.amazon.com/efs/when-to-choose-efs/\">https://aws.amazon.com/efs/when-to-choose-efs/</a></p><p><a href=\"https://aws.amazon.com/what-is-cloud-storage/\">https://aws.amazon.com/what-is-cloud-storage/</a></p><p><br></p><p><strong>Check out this Amazon S3 Cheat Sheet:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3/?src=udemy\">https://tutorialsdojo.com/amazon-s3/</a></p><p><br></p><p><strong>Check out this Amazon S3, EBS, and EFS comparison:</strong></p><p><a href=\"https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/?src=udemy\">https://tutorialsdojo.com/amazon-s3-vs-ebs-vs-efs/</a></p></div>"
	},
	{
		"question": "<p>A multinational bank has recently set up AWS Organizations to manage its several AWS accounts from their various business units. The Senior Solutions Architect attached the SCP below to an Organizational Unit (OU) to define the services that its member accounts can use:</p><div class=\"ud-component--base-components--code-block\"><div><pre class=\"prettyprint linenums prettyprinted\" role=\"presentation\" style=\"\"><ol class=\"linenums\"><li class=\"L0\"><span class=\"pun\">{</span></li><li class=\"L1\"><span class=\"str\">\"Version\"</span><span class=\"pun\">:</span><span class=\"str\">\"2012-10-17\"</span><span class=\"pun\">,</span><span class=\"pln\"> </span></li><li class=\"L2\"><span class=\"str\">\"Statement\"</span><span class=\"pun\">:[</span><span class=\"pln\"> </span></li><li class=\"L3\"><span class=\"pln\">  </span><span class=\"pun\">{</span><span class=\"pln\"> </span></li><li class=\"L4\"><span class=\"pln\">     </span><span class=\"str\">\"Effect\"</span><span class=\"pun\">:</span><span class=\"str\">\"Allow\"</span><span class=\"pun\">,</span><span class=\"pln\"> </span></li><li class=\"L5\"><span class=\"pln\">     </span><span class=\"str\">\"Action\"</span><span class=\"pun\">:[</span><span class=\"str\">\"EC2:*\"</span><span class=\"pun\">,</span><span class=\"str\">\"S3:*\"</span><span class=\"pun\">],</span><span class=\"pln\"> </span></li><li class=\"L6\"><span class=\"pln\">     </span><span class=\"str\">\"Resource\"</span><span class=\"pun\">:</span><span class=\"str\">\"*\"</span><span class=\"pln\"> </span></li><li class=\"L7\"><span class=\"pln\">   </span><span class=\"pun\">}</span><span class=\"pln\"> </span></li><li class=\"L8\"><span class=\"pln\"> </span><span class=\"pun\">]</span><span class=\"pln\"> </span></li><li class=\"L9\"><span class=\"pun\">}</span><span class=\"pln\"> </span></li></ol></pre></div></div><p>In one of the member accounts under that OU, an IAM user tried to create a new S3 bucket but was getting a permission denied error.</p><p>Which of the following options is the most likely cause of this issue?</p>",
		"answers": [
			{
				"correct": false,
				"answer": "<p>All accounts within the OU does not automatically inherit the policy attached to them. You still have to manually attach the SCP to the individual AWS accounts of the OU.</p>"
			},
			{
				"correct": false,
				"answer": "<p>You should use the root user of the account to be able to create the new S3 bucket.</p>"
			},
			{
				"correct": true,
				"answer": "<p>The IAM user in the member account does not have IAM policies that explicitly grant EC2 or S3 service actions.</p>"
			},
			{
				"correct": false,
				"answer": "<p>An IAM policy that allows the use of S3 and EC2 services should be the one attached in the OU instead of an SCP.</p>"
			}
		],
		"explanation": "<h4 class=\"udlite-heading-md\">Explanation</h4><div data-purpose=\"safely-set-inner-html:mc-quiz-question:question-explanation\"><p>A <strong>service control policy (SCP)</strong> determines what services and actions can be delegated by administrators to the users and roles in the accounts that the SCP is applied to. An SCP <strong><em>does not</em></strong> grant any permissions. Instead, SCPs are JSON policies that specify the maximum permissions for an organization or organizational unit (OU). The SCP limits permissions for entities in member accounts, including each AWS account root user.</p><p>If the SCP allows the actions for a service, the administrator of the account can grant permissions for those actions to the users and roles in that account, and the users and roles can perform the actions of the administrators grant those permissions. If the SCP denies actions for a service, the administrators in that account can't effectively grant permissions for those actions, and the users and roles in the account can't perform the actions even if granted by an administrator.</p><p><img src=\"https://media.tutorialsdojo.com/sap_scp_org_attached.png\"></p><p>Users and roles must still be granted permissions using IAM permission policies attached to them or to groups. The SCPs filter the permissions granted by such policies, and the user can't perform any actions that the applicable SCPs don't allow. Actions allowed by the SCPs can be used if they are granted to the user or role by one or more IAM permission policies. Take note that the IAM user being used by the administrator does not have IAM policies that explicitly grant EC2 or S3 service actions.</p><p>Hence, the correct answer is: <strong>The IAM user in the member account does not have IAM policies that explicitly grant EC2 or S3 service actions</strong>.</p><p>The option that says: <strong>All accounts within the OU does not automatically inherit the policy attached to them. You still have to manually attach the SCP to the individual AWS accounts of the OU</strong> is incorrect because an SCP attached to an OU is automatically inherited by all accounts within that same OU. The main cause of this issue is the missing IAM policy in the account, which explicitly grants EC2 or S3 service actions to the IAM user.</p><p>The option that says: <strong>An IAM policy that allows the use of S3 and EC2 services should be the one attached in the OU instead of an SCP</strong> is incorrect because you cannot directly assign an IAM policy to an OU. In addition, there is no attached IAM policy that allows EC2 or S3 service actions to the IAM user.</p><p><strong>Using the root user of the account to be able to create the new S3 bucket</strong> is incorrect because SCPs <strong>do</strong> affect the root user along with all IAM users and standard IAM roles in any affected account. The issue lies in the missing IAM policy of the account and not with the SCP, OU, or its AWS Organizations settings.</p><p><br></p><p><strong>References:</strong></p><p><a href=\"https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html\">https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html</a></p><p><a href=\"https://aws.amazon.com/organizations/faqs/\">https://aws.amazon.com/organizations/faqs/</a></p><p><br></p><p><strong>Check out these AWS Cheat Sheets:</strong></p><p><a href=\"https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/?src=udemy\">https://tutorialsdojo.com/service-control-policies-scp-vs-iam-policies/</a></p></div>"
	}
].slice(questionSet * 75, (questionSet + 1) * 75)
.reverse().shuffle()
.reverse().shuffle()
.reverse().shuffle()
